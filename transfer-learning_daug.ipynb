{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensorflow version: 1.12.0-rc0\n",
    "scikit-learn version: 0.17\n",
    "keras version: 2.2.4\n",
    "tensorboard version: 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "In this assignment, we will use the weights of a network pre-trained in a particular problem as starting point to train our CNN to a different problem. As training a network from scratch is time-consuming and demands a lot of data, this is a frequent strategy, specially if both datasets (the one used for pre-training and the target) shares similar structures/elements/concepts. \n",
    "\n",
    "This is specially true when working with images. Most filters learned in initial convolutional layers will detect low-level elements, such as borders, corners and color blobs, which are common to most problems in the image domain. \n",
    "\n",
    "In this notebook, we will load the SqueezeNet architecture trained in the ImageNet dataset and fine-tune it to CIFAR-10.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.5/dist-packages (2.2.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (3.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (1.11.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (1.0.6)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.17.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "from random import sample, seed\n",
    "seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.figsize'] = (15,15) # Make the figures a bit bigger\n",
    "\n",
    "# Keras imports\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, concatenate, Dropout, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "if (sklearn.__version__ == '0.20.0'):\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "else:\n",
    "    from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "#======================================\n",
    "# Global definitions\n",
    "#======================================\n",
    "n_epochs         = 100\n",
    "learning_rate    = 1e-4\n",
    "n_classes        = 10\n",
    "train_batch_size = 32\n",
    "val_batch_size   = 10\n",
    "    \n",
    "#Utility to plot\n",
    "def plotImages(imgList):\n",
    "    for i in range(len(imgList)):\n",
    "        plotImage(imgList[i])\n",
    "        \n",
    "        \n",
    "def plotImage(img):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.imshow(np.uint8(img), interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SqueezeNet definition\n",
    "These methods define our architecture and load the weights obtained using ImageNet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fire Module Definition\n",
    "sq1x1 = \"squeeze1x1\"\n",
    "exp1x1 = \"expand1x1\"\n",
    "exp3x3 = \"expand3x3\"\n",
    "relu = \"relu_\"\n",
    "\n",
    "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
    "    s_id = 'fire' + str(fire_id) + '/'\n",
    "\n",
    "    channel_axis = 3\n",
    "    \n",
    "    x = Convolution2D(squeeze, (1, 1), padding='valid', name=s_id + sq1x1)(x)\n",
    "    x = Activation('relu', name=s_id + relu + sq1x1)(x)\n",
    "\n",
    "    left = Convolution2D(expand, (1, 1), padding='valid', name=s_id + exp1x1)(x)\n",
    "    left = Activation('relu', name=s_id + relu + exp1x1)(left)\n",
    "\n",
    "    right = Convolution2D(expand, (3, 3), padding='same', name=s_id + exp3x3)(x)\n",
    "    right = Activation('relu', name=s_id + relu + exp3x3)(right)\n",
    "\n",
    "    x = concatenate([left, right], axis=channel_axis, name=s_id + 'concat')\n",
    "    return x\n",
    "\n",
    "#SqueezeNet model definition\n",
    "def SqueezeNet(input_shape):\n",
    "    img_input = Input(shape=input_shape) #placeholder\n",
    "    \n",
    "    x = Convolution2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n",
    "    x = Activation('relu', name='relu_conv1')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
    "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
    "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
    "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
    "    \n",
    "    x = Dropout(0.5, name='drop9')(x)\n",
    "\n",
    "    x = Convolution2D(1000, (1, 1), padding='valid', name='conv10')(x)\n",
    "    x = Activation('relu', name='relu_conv10')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Activation('softmax', name='loss')(x)\n",
    "\n",
    "    model = Model(img_input, x, name='squeezenet')\n",
    "\n",
    "    # Download and load ImageNet weights\n",
    "    model.load_weights('./squeezenet_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The class are **airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val data. X:  (50000, 32, 32, 3) , Y:  (50000, 1)\n",
      "Test data. X:  (10000, 32, 32, 3) , Y:  (10000, 1)\n",
      "--- Splitting data into train and val\n",
      "Train data. X: (5, 40000, 32, 32, 3) Y: (5, 40000, 1)\n",
      "Val data. X: (5, 10000, 32, 32, 3) Y: (5, 10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "(trainVal_data, trainVal_label), (X_test, y_test) = cifar10.load_data()\n",
    "print(\"Train/Val data. X: \", trainVal_data.shape, \", Y: \", trainVal_label.shape)\n",
    "print(\"Test data. X: \", X_test.shape, \", Y: \", y_test.shape)\n",
    "\n",
    "#=====================================\n",
    "# Prepare the data\n",
    "#=====================================\n",
    "\n",
    "#--- Dividing the data into training and validation\n",
    "folds = 5\n",
    "if (sklearn.__version__ == '0.20.0'):\n",
    "    sss = StratifiedShuffleSplit(folds, test_size=0.2, random_state=42)\n",
    "    sss = sss.split(trainVal_data,trainVal_label)\n",
    "else:\n",
    "    sss = StratifiedShuffleSplit(trainVal_label, folds, test_size=0.2, random_state=42)\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_val = []\n",
    "y_val = []\n",
    "for train_index, val_index in sss:\n",
    "    X_train.append(trainVal_data[train_index])\n",
    "    X_val.append(trainVal_data[val_index])\n",
    "    y_train.append(trainVal_label[train_index])\n",
    "    y_val.append(trainVal_label[val_index])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "    \n",
    "print(\"--- Splitting data into train and val\")\n",
    "print(\"Train data. X:\",X_train.shape,\"Y:\",y_train.shape)\n",
    "print(\"Val data. X:\",X_val.shape,\"Y:\",y_val.shape)\n",
    "\n",
    "#--- Data augmentation\n",
    "aug = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True,\n",
    "                         rotation_range=20, \n",
    "                         width_shift_range=0.2, height_shift_range=0.2, \n",
    "                         horizontal_flip=True, vertical_flip=True, \n",
    "                         fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000 4000 4000 4000 4000 4000 4000 4000 4000 4000]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000 4000 4000 4000 4000 4000 4000 4000 4000 4000]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000 4000 4000 4000 4000 4000 4000 4000 4000 4000]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000 4000 4000 4000 4000 4000 4000 4000 4000 4000]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000 4000 4000 4000 4000 4000 4000 4000 4000 4000]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cheking if the splits are balanced\n",
    "for i in range(folds):\n",
    "    hist = np.histogram(np.squeeze(y_train[i]))[0]\n",
    "    print(hist)    \n",
    "    plt.bar(hist,np.amax(hist),alpha=0.5)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## SqueezeNet with frozen layers\n",
    "Our initial attempt will be to remove SqueezeNet's top layers --- responsible for the classification into ImageNet classes --- and train a new set of layers to our CIFAR-10 classes. We will also freeze the layers before `drop9`. Our architecture will be like this:\n",
    "\n",
    "<img src=\"frozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/azaelmsousa/AzaEnv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 15, 15, 64)   1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv1 (Activation)         (None, 15, 15, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 7, 7, 64)     0           relu_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fire2/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     1040        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire2/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire2/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire2/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/concat (Concatenate)      (None, 7, 7, 128)    0           fire2/relu_expand1x1[0][0]       \n",
      "                                                                 fire2/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire3/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     2064        fire2/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire3/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire3/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire3/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/concat (Concatenate)      (None, 7, 7, 128)    0           fire3/relu_expand1x1[0][0]       \n",
      "                                                                 fire3/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 3, 3, 128)    0           fire3/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire4/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     4128        pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire4/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire4/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire4/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/concat (Concatenate)      (None, 3, 3, 256)    0           fire4/relu_expand1x1[0][0]       \n",
      "                                                                 fire4/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire5/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     8224        fire4/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire5/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire5/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire5/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/concat (Concatenate)      (None, 3, 3, 256)    0           fire5/relu_expand1x1[0][0]       \n",
      "                                                                 fire5/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 1, 1, 256)    0           fire5/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire6/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     12336       pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire6/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire6/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire6/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/concat (Concatenate)      (None, 1, 1, 384)    0           fire6/relu_expand1x1[0][0]       \n",
      "                                                                 fire6/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire7/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     18480       fire6/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire7/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire7/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire7/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/concat (Concatenate)      (None, 1, 1, 384)    0           fire7/relu_expand1x1[0][0]       \n",
      "                                                                 fire7/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire8/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     24640       fire7/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire8/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire8/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire8/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/concat (Concatenate)      (None, 1, 1, 512)    0           fire8/relu_expand1x1[0][0]       \n",
      "                                                                 fire8/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire9/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     32832       fire8/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire9/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire9/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire9/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/concat (Concatenate)      (None, 1, 1, 512)    0           fire9/relu_expand1x1[0][0]       \n",
      "                                                                 fire9/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "drop9 (Dropout)                 (None, 1, 1, 512)    0           fire9/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv10 (Conv2D)                 (None, 1, 1, 10)     5130        drop9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv10 (Activation)        (None, 1, 1, 10)     0           conv10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 10)           0           relu_conv10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "loss (Activation)               (None, 10)           0           global_average_pooling2d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 727,626\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 722,496\n",
      "__________________________________________________________________________________________________\n",
      "<keras.engine.input_layer.InputLayer object at 0x7fa0c72520b8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c7252668> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c72524e0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fa0c72525c0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c72525f8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c68b7588> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c68b7940> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c68cbd30> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c68cb908> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c6874438> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c68747b8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c69e4b38> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c688c518> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c688c710> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c6834828> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c68343c8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c684eac8> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c684eba8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fa0c6874390> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c684e908> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c67f4cc0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c67f4f60> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c681b198> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c681b0b8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c67b3908> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c67c5390> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c67f4cf8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c67dba58> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c67db470> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c6774cf8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c67749b0> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c679e5c0> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c679ea20> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fa0c67b39e8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c679ec18> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c729e358> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c729e898> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c72a2240> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c72a2a90> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c728b2b0> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c728bc88> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c7227198> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c72915c0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c7291470> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c7231c88> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c7231ba8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c72cef28> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c72cea20> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c728bb38> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c72e3358> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c72e37b8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c730efd0> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c730e198> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c731c080> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c731ce80> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c72cedd8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c7321278> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c7321400> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c7350358> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c7350cf8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c7347cf8> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c7347b70> False\n",
      "<keras.layers.core.Dropout object at 0x7fa0c731c4a8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0db9075c0> True\n",
      "<keras.layers.core.Activation object at 0x7fa0c45a7860> True\n",
      "<keras.layers.pooling.GlobalAveragePooling2D object at 0x7fa0c661b208> True\n",
      "<keras.layers.core.Activation object at 0x7fa0c661b080> True\n"
     ]
    }
   ],
   "source": [
    "def get_squeezenet_ft():\n",
    "    #=====================================\n",
    "    # Freezing layers\n",
    "    #=====================================\n",
    "\n",
    "    squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "    #freeze layers\n",
    "    for layer in squeezeNetModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #=====================================\n",
    "    # Add new classification layers\n",
    "    #=====================================\n",
    "\n",
    "    #--- Removing layers until drop 9\n",
    "    squeezeNetModel.layers.pop() #Convolution2D\n",
    "    squeezeNetModel.layers.pop() #Activation ReLU\n",
    "    squeezeNetModel.layers.pop() #Global Avg Pool\n",
    "    squeezeNetModel.layers.pop() #Activation Softmax\n",
    "\n",
    "    #--- Adding classification layer for 10 classes\n",
    "    out = Convolution2D(n_classes, (1, 1), padding='valid', name='conv10')(squeezeNetModel.layers[-1].output)\n",
    "    out = Activation('relu', name='relu_conv10')(out)\n",
    "\n",
    "    out = GlobalAveragePooling2D()(out)\n",
    "    out = Activation('softmax', name='loss')(out)\n",
    "\n",
    "    #=====================================\n",
    "    # New Model\n",
    "    #=====================================\n",
    "    model = Model(squeezeNetModel.inputs, out, name='squeezenet_new')\n",
    "    #=====================================\n",
    "    # Compile model\n",
    "    #=====================================\n",
    "\n",
    "    #--- Compile the model\n",
    "    # It means to configure the model for training.\n",
    "    # Other types of optimizer:\n",
    "    #    optimizers.Adam(lr=learning_rate)\n",
    "    opt = optimizer=optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)#SGD(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model  = get_squeezenet_ft()\n",
    "model.summary()\n",
    "\n",
    "#--- Check the trainable status of the individual layers\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Split: 0\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 5.1622 - acc: 0.1060 - val_loss: 2.2970 - val_acc: 0.1032\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 2.4800 - acc: 0.1094 - val_loss: 2.3019 - val_acc: 0.1003\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 2.3479 - acc: 0.1067 - val_loss: 2.3023 - val_acc: 0.1004\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.3149 - acc: 0.1066 - val_loss: 2.3018 - val_acc: 0.1008\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.3050 - acc: 0.1082 - val_loss: 2.2998 - val_acc: 0.1021\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 2.3005 - acc: 0.1096 - val_loss: 2.2995 - val_acc: 0.1027\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.3009 - acc: 0.1093 - val_loss: 2.2965 - val_acc: 0.1055\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.2921 - acc: 0.1130 - val_loss: 2.2928 - val_acc: 0.1079\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.2938 - acc: 0.1132 - val_loss: 2.2912 - val_acc: 0.1097\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.2899 - acc: 0.1147 - val_loss: 2.2908 - val_acc: 0.1100\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.2890 - acc: 0.1178 - val_loss: 2.2853 - val_acc: 0.1134\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.2880 - acc: 0.1194 - val_loss: 2.2846 - val_acc: 0.1146\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.2804 - acc: 0.1229 - val_loss: 2.2801 - val_acc: 0.1181\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.2801 - acc: 0.1255 - val_loss: 2.2745 - val_acc: 0.1230\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 2.2752 - acc: 0.1326 - val_loss: 2.2649 - val_acc: 0.1317\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.2694 - acc: 0.1398 - val_loss: 2.2656 - val_acc: 0.1320\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.2622 - acc: 0.1448 - val_loss: 2.2559 - val_acc: 0.1422\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.2619 - acc: 0.1493 - val_loss: 2.2502 - val_acc: 0.1504\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 2.2537 - acc: 0.1553 - val_loss: 2.2430 - val_acc: 0.1577\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.2529 - acc: 0.1596 - val_loss: 2.2409 - val_acc: 0.1588\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.2422 - acc: 0.1654 - val_loss: 2.2358 - val_acc: 0.1609\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 2.2413 - acc: 0.1658 - val_loss: 2.2335 - val_acc: 0.1628\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 2.2372 - acc: 0.1716 - val_loss: 2.2278 - val_acc: 0.1690\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 4s 99us/step - loss: 2.2337 - acc: 0.1747 - val_loss: 2.2202 - val_acc: 0.1711\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 2.2286 - acc: 0.1767 - val_loss: 2.2126 - val_acc: 0.1751\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.2236 - acc: 0.1807 - val_loss: 2.2084 - val_acc: 0.1783\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.2169 - acc: 0.1847 - val_loss: 2.2025 - val_acc: 0.1844\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: 2.2123 - acc: 0.1905 - val_loss: 2.1980 - val_acc: 0.1883\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.2055 - acc: 0.1944 - val_loss: 2.1843 - val_acc: 0.1948\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 2.1963 - acc: 0.1977 - val_loss: 2.1737 - val_acc: 0.1975\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.1952 - acc: 0.2004 - val_loss: 2.1673 - val_acc: 0.2023\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.1865 - acc: 0.2041 - val_loss: 2.1671 - val_acc: 0.2037\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.1806 - acc: 0.2088 - val_loss: 2.1559 - val_acc: 0.2088\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.1749 - acc: 0.2105 - val_loss: 2.1451 - val_acc: 0.2174\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.1649 - acc: 0.2163 - val_loss: 2.1258 - val_acc: 0.2208\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.1508 - acc: 0.2221 - val_loss: 2.1143 - val_acc: 0.2292\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.1378 - acc: 0.2297 - val_loss: 2.1053 - val_acc: 0.2352\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.1330 - acc: 0.2315 - val_loss: 2.0988 - val_acc: 0.2361\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.1292 - acc: 0.2337 - val_loss: 2.0884 - val_acc: 0.2430\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1226 - acc: 0.2374 - val_loss: 2.0840 - val_acc: 0.2470\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 2.1192 - acc: 0.2389 - val_loss: 2.0887 - val_acc: 0.2433\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.1152 - acc: 0.2402 - val_loss: 2.0820 - val_acc: 0.2457\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.1171 - acc: 0.2416 - val_loss: 2.0820 - val_acc: 0.2463\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.1166 - acc: 0.2405 - val_loss: 2.0811 - val_acc: 0.2503\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.1112 - acc: 0.2427 - val_loss: 2.0777 - val_acc: 0.2512\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.1095 - acc: 0.2453 - val_loss: 2.0771 - val_acc: 0.2500\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.1084 - acc: 0.2462 - val_loss: 2.0755 - val_acc: 0.2509\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1075 - acc: 0.2477 - val_loss: 2.0797 - val_acc: 0.2521\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1050 - acc: 0.2480 - val_loss: 2.0712 - val_acc: 0.2561\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 2.1021 - acc: 0.2513 - val_loss: 2.0730 - val_acc: 0.2602\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1013 - acc: 0.2506 - val_loss: 2.0698 - val_acc: 0.2599\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.0986 - acc: 0.2513 - val_loss: 2.0688 - val_acc: 0.2614\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.1003 - acc: 0.2526 - val_loss: 2.0716 - val_acc: 0.2615\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.1014 - acc: 0.2520 - val_loss: 2.0646 - val_acc: 0.2634\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0970 - acc: 0.2534 - val_loss: 2.0652 - val_acc: 0.2626\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 2.0965 - acc: 0.2571 - val_loss: 2.0684 - val_acc: 0.2598\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.0994 - acc: 0.2555 - val_loss: 2.0640 - val_acc: 0.2651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 2.0978 - acc: 0.2537 - val_loss: 2.0659 - val_acc: 0.2669\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.0943 - acc: 0.2567 - val_loss: 2.0639 - val_acc: 0.2683\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.0957 - acc: 0.2560 - val_loss: 2.0622 - val_acc: 0.2696\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.0906 - acc: 0.2576 - val_loss: 2.0615 - val_acc: 0.2693\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.0918 - acc: 0.2590 - val_loss: 2.0594 - val_acc: 0.2694\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.0944 - acc: 0.2564 - val_loss: 2.0604 - val_acc: 0.2702\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.0917 - acc: 0.2574 - val_loss: 2.0625 - val_acc: 0.2675\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.0905 - acc: 0.2599 - val_loss: 2.0650 - val_acc: 0.2714\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.0926 - acc: 0.2566 - val_loss: 2.0592 - val_acc: 0.2697\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.0912 - acc: 0.2601 - val_loss: 2.0546 - val_acc: 0.2724\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0908 - acc: 0.2606 - val_loss: 2.0566 - val_acc: 0.2725\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.0906 - acc: 0.2606 - val_loss: 2.0575 - val_acc: 0.2716\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0889 - acc: 0.2592 - val_loss: 2.0577 - val_acc: 0.2709\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.0846 - acc: 0.2607 - val_loss: 2.0543 - val_acc: 0.2757\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.0899 - acc: 0.2599 - val_loss: 2.0575 - val_acc: 0.2711\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.0871 - acc: 0.2624 - val_loss: 2.0573 - val_acc: 0.2704\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0892 - acc: 0.2588 - val_loss: 2.0600 - val_acc: 0.2705\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0879 - acc: 0.2624 - val_loss: 2.0557 - val_acc: 0.2736\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0882 - acc: 0.2600 - val_loss: 2.0571 - val_acc: 0.2737\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.0860 - acc: 0.2624 - val_loss: 2.0585 - val_acc: 0.2712\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0878 - acc: 0.2618 - val_loss: 2.0573 - val_acc: 0.2702\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0865 - acc: 0.2645 - val_loss: 2.0565 - val_acc: 0.2723\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.0875 - acc: 0.2622 - val_loss: 2.0599 - val_acc: 0.2718\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0869 - acc: 0.2622 - val_loss: 2.0574 - val_acc: 0.2735\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 2.0852 - acc: 0.2613 - val_loss: 2.0560 - val_acc: 0.2723\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.0869 - acc: 0.2636 - val_loss: 2.0549 - val_acc: 0.2727\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 2.0828 - acc: 0.2650 - val_loss: 2.0539 - val_acc: 0.2717\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 2.0835 - acc: 0.2654 - val_loss: 2.0536 - val_acc: 0.2741\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0828 - acc: 0.2642 - val_loss: 2.0571 - val_acc: 0.2722\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.0801 - acc: 0.2658 - val_loss: 2.0527 - val_acc: 0.2752\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.0782 - acc: 0.2690 - val_loss: 2.0539 - val_acc: 0.2739\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.0783 - acc: 0.2702 - val_loss: 2.0512 - val_acc: 0.2725\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.0769 - acc: 0.2707 - val_loss: 2.0518 - val_acc: 0.2789\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.0765 - acc: 0.2702 - val_loss: 2.0463 - val_acc: 0.2793\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.0742 - acc: 0.2723 - val_loss: 2.0455 - val_acc: 0.2754\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0672 - acc: 0.2772 - val_loss: 2.0363 - val_acc: 0.2857\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.0660 - acc: 0.2789 - val_loss: 2.0385 - val_acc: 0.2857\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 2.0652 - acc: 0.2788 - val_loss: 2.0382 - val_acc: 0.2865\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 2.0582 - acc: 0.2831 - val_loss: 2.0322 - val_acc: 0.2877\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 2.0564 - acc: 0.2854 - val_loss: 2.0305 - val_acc: 0.2922\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.0558 - acc: 0.2860 - val_loss: 2.0312 - val_acc: 0.2924\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0509 - acc: 0.2851 - val_loss: 2.0324 - val_acc: 0.2928\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0497 - acc: 0.2856 - val_loss: 2.0256 - val_acc: 0.2917\n",
      "10000/10000 [==============================] - 2s 171us/step\n",
      "Validation loss: 2.0255626554489137\n",
      "Validation accuracy (NORMALIZED): 0.2917000061571598\n",
      "================================================\n",
      "Split: 1\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: 5.9474 - acc: 0.1246 - val_loss: 2.3042 - val_acc: 0.1003\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 5s 129us/step - loss: 2.5015 - acc: 0.1131 - val_loss: 2.3013 - val_acc: 0.0995\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.3441 - acc: 0.1068 - val_loss: 2.3019 - val_acc: 0.1002\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.3128 - acc: 0.1059 - val_loss: 2.3022 - val_acc: 0.1001\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.3102 - acc: 0.1056 - val_loss: 2.3020 - val_acc: 0.1001\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.2972 - acc: 0.1074 - val_loss: 2.3010 - val_acc: 0.1011\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.2993 - acc: 0.1086 - val_loss: 2.3001 - val_acc: 0.1015\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.2951 - acc: 0.1102 - val_loss: 2.2964 - val_acc: 0.1034\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.2907 - acc: 0.1114 - val_loss: 2.2936 - val_acc: 0.1038\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 2.2883 - acc: 0.1160 - val_loss: 2.2918 - val_acc: 0.1051\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 2.2820 - acc: 0.1171 - val_loss: 2.2846 - val_acc: 0.1104\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 4s 100us/step - loss: 2.2811 - acc: 0.1192 - val_loss: 2.2764 - val_acc: 0.1121\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 2.2783 - acc: 0.1199 - val_loss: 2.2728 - val_acc: 0.1104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.2708 - acc: 0.1231 - val_loss: 2.2588 - val_acc: 0.1219\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.2612 - acc: 0.1299 - val_loss: 2.2472 - val_acc: 0.1244\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.2542 - acc: 0.1343 - val_loss: 2.2426 - val_acc: 0.1276\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 2.2469 - acc: 0.1408 - val_loss: 2.2236 - val_acc: 0.1384\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 2.2382 - acc: 0.1521 - val_loss: 2.2128 - val_acc: 0.1496\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.2298 - acc: 0.1565 - val_loss: 2.2110 - val_acc: 0.1548\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.2222 - acc: 0.1642 - val_loss: 2.2077 - val_acc: 0.1565\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.2172 - acc: 0.1685 - val_loss: 2.1971 - val_acc: 0.1638\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.2099 - acc: 0.1724 - val_loss: 2.1868 - val_acc: 0.1707\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.1979 - acc: 0.1807 - val_loss: 2.1659 - val_acc: 0.1801\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.1871 - acc: 0.1886 - val_loss: 2.1532 - val_acc: 0.1860\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.1736 - acc: 0.1964 - val_loss: 2.1359 - val_acc: 0.1996\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1545 - acc: 0.2074 - val_loss: 2.1257 - val_acc: 0.2116\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.1512 - acc: 0.2142 - val_loss: 2.1175 - val_acc: 0.2164\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 2.1445 - acc: 0.2212 - val_loss: 2.1150 - val_acc: 0.2199\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 6s 138us/step - loss: 2.1372 - acc: 0.2248 - val_loss: 2.1094 - val_acc: 0.2257\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 5s 128us/step - loss: 2.1310 - acc: 0.2286 - val_loss: 2.1044 - val_acc: 0.2317\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 2.1318 - acc: 0.2284 - val_loss: 2.1005 - val_acc: 0.2343\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 2.1248 - acc: 0.2355 - val_loss: 2.0914 - val_acc: 0.2423\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1197 - acc: 0.2369 - val_loss: 2.0957 - val_acc: 0.2387\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1179 - acc: 0.2399 - val_loss: 2.0894 - val_acc: 0.2426\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 2.1154 - acc: 0.2447 - val_loss: 2.0834 - val_acc: 0.2479\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1097 - acc: 0.2491 - val_loss: 2.0778 - val_acc: 0.2529\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 2.1052 - acc: 0.2512 - val_loss: 2.0772 - val_acc: 0.2556\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1056 - acc: 0.2525 - val_loss: 2.0777 - val_acc: 0.2552\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1048 - acc: 0.2565 - val_loss: 2.0709 - val_acc: 0.2618\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.0922 - acc: 0.2623 - val_loss: 2.0677 - val_acc: 0.2627\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.0888 - acc: 0.2654 - val_loss: 2.0538 - val_acc: 0.2744\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.0809 - acc: 0.2701 - val_loss: 2.0267 - val_acc: 0.2896\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.0614 - acc: 0.2802 - val_loss: 2.0085 - val_acc: 0.3006\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.0545 - acc: 0.2833 - val_loss: 2.0000 - val_acc: 0.3079\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.0461 - acc: 0.2874 - val_loss: 1.9915 - val_acc: 0.3122\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.0398 - acc: 0.2924 - val_loss: 1.9792 - val_acc: 0.3150\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.0318 - acc: 0.2964 - val_loss: 1.9724 - val_acc: 0.3205\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.0270 - acc: 0.2984 - val_loss: 1.9667 - val_acc: 0.3223\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.0241 - acc: 0.2950 - val_loss: 1.9586 - val_acc: 0.3256\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.0194 - acc: 0.2948 - val_loss: 1.9580 - val_acc: 0.3269\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.0157 - acc: 0.2980 - val_loss: 1.9555 - val_acc: 0.3301\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.0146 - acc: 0.3021 - val_loss: 1.9553 - val_acc: 0.3285\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 2.0115 - acc: 0.3010 - val_loss: 1.9511 - val_acc: 0.3294\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.0073 - acc: 0.3009 - val_loss: 1.9443 - val_acc: 0.3317\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 2.0084 - acc: 0.3026 - val_loss: 1.9486 - val_acc: 0.3316\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.0042 - acc: 0.3015 - val_loss: 1.9479 - val_acc: 0.3326\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.0053 - acc: 0.3007 - val_loss: 1.9451 - val_acc: 0.3318\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.0033 - acc: 0.3019 - val_loss: 1.9463 - val_acc: 0.3327\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.0072 - acc: 0.3017 - val_loss: 1.9438 - val_acc: 0.3346\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.0026 - acc: 0.3022 - val_loss: 1.9435 - val_acc: 0.3361\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.0024 - acc: 0.3035 - val_loss: 1.9399 - val_acc: 0.3388\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 2.0007 - acc: 0.3037 - val_loss: 1.9420 - val_acc: 0.3383\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 1.9996 - acc: 0.3061 - val_loss: 1.9412 - val_acc: 0.3361\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 1.9998 - acc: 0.3043 - val_loss: 1.9413 - val_acc: 0.3404\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 1.9975 - acc: 0.3049 - val_loss: 1.9413 - val_acc: 0.3364\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 1.9977 - acc: 0.3064 - val_loss: 1.9395 - val_acc: 0.3365\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 6s 140us/step - loss: 1.9968 - acc: 0.3045 - val_loss: 1.9385 - val_acc: 0.3384\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 1.9977 - acc: 0.3033 - val_loss: 1.9408 - val_acc: 0.3372\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 1.9989 - acc: 0.3047 - val_loss: 1.9373 - val_acc: 0.3404\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: 1.9985 - acc: 0.3053 - val_loss: 1.9358 - val_acc: 0.3384\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 1.9965 - acc: 0.3053 - val_loss: 1.9376 - val_acc: 0.3407\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 1.9961 - acc: 0.3077 - val_loss: 1.9357 - val_acc: 0.3411\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 1.9975 - acc: 0.3050 - val_loss: 1.9322 - val_acc: 0.3439\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 1.9966 - acc: 0.3056 - val_loss: 1.9326 - val_acc: 0.3388\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 1.9931 - acc: 0.3052 - val_loss: 1.9311 - val_acc: 0.3379\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 1.9942 - acc: 0.3051 - val_loss: 1.9297 - val_acc: 0.3397\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 1.9954 - acc: 0.3078 - val_loss: 1.9344 - val_acc: 0.3419\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 1.9943 - acc: 0.3070 - val_loss: 1.9308 - val_acc: 0.3416\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 1.9909 - acc: 0.3057 - val_loss: 1.9313 - val_acc: 0.3423\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 1.9913 - acc: 0.3037 - val_loss: 1.9321 - val_acc: 0.3432\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 1.9941 - acc: 0.3069 - val_loss: 1.9318 - val_acc: 0.3433\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 1.9961 - acc: 0.3062 - val_loss: 1.9290 - val_acc: 0.3417\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 1.9957 - acc: 0.3035 - val_loss: 1.9318 - val_acc: 0.3371\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 5s 129us/step - loss: 1.9921 - acc: 0.3070 - val_loss: 1.9257 - val_acc: 0.3447\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 1.9952 - acc: 0.3072 - val_loss: 1.9300 - val_acc: 0.3432\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 1.9902 - acc: 0.3094 - val_loss: 1.9283 - val_acc: 0.3404\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 1.9907 - acc: 0.3073 - val_loss: 1.9331 - val_acc: 0.3400\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 1.9879 - acc: 0.3108 - val_loss: 1.9306 - val_acc: 0.3420\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 1.9902 - acc: 0.3061 - val_loss: 1.9286 - val_acc: 0.3428\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 1.9938 - acc: 0.3066 - val_loss: 1.9321 - val_acc: 0.3410\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 1.9893 - acc: 0.3089 - val_loss: 1.9249 - val_acc: 0.3458\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 1.9907 - acc: 0.3062 - val_loss: 1.9304 - val_acc: 0.3368\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 1.9906 - acc: 0.3078 - val_loss: 1.9275 - val_acc: 0.3420\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 1.9908 - acc: 0.3085 - val_loss: 1.9333 - val_acc: 0.3399\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 1.9916 - acc: 0.3070 - val_loss: 1.9284 - val_acc: 0.3407\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 4s 101us/step - loss: 1.9940 - acc: 0.3054 - val_loss: 1.9288 - val_acc: 0.3429\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 1.9894 - acc: 0.3034 - val_loss: 1.9279 - val_acc: 0.3440\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 1.9919 - acc: 0.3060 - val_loss: 1.9342 - val_acc: 0.3379\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 1.9926 - acc: 0.3057 - val_loss: 1.9299 - val_acc: 0.3385\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 1.9903 - acc: 0.3085 - val_loss: 1.9279 - val_acc: 0.3397\n",
      "10000/10000 [==============================] - 2s 185us/step\n",
      "Validation loss: 1.9278625366687774\n",
      "Validation accuracy (NORMALIZED): 0.33970000661909583\n",
      "================================================\n",
      "Split: 2\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 4.6378 - acc: 0.1078 - val_loss: 2.3108 - val_acc: 0.1035\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.4657 - acc: 0.1041 - val_loss: 2.3037 - val_acc: 0.1010\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.3476 - acc: 0.1015 - val_loss: 2.3043 - val_acc: 0.1001\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.3193 - acc: 0.1010 - val_loss: 2.3034 - val_acc: 0.1001\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 2.3121 - acc: 0.1009 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.3063 - acc: 0.1011 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 2.3055 - acc: 0.1016 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.3041 - acc: 0.1010 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.3040 - acc: 0.1012 - val_loss: 2.3031 - val_acc: 0.1001\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.3034 - acc: 0.1022 - val_loss: 2.3035 - val_acc: 0.1002\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.3023 - acc: 0.1026 - val_loss: 2.3030 - val_acc: 0.1003\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.3009 - acc: 0.1033 - val_loss: 2.3030 - val_acc: 0.1005\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.3013 - acc: 0.1027 - val_loss: 2.3027 - val_acc: 0.1002\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.3005 - acc: 0.1025 - val_loss: 2.3026 - val_acc: 0.1004\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.2997 - acc: 0.1036 - val_loss: 2.3017 - val_acc: 0.1016\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.2992 - acc: 0.1046 - val_loss: 2.3012 - val_acc: 0.1019\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.2965 - acc: 0.1060 - val_loss: 2.2980 - val_acc: 0.1047\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.2965 - acc: 0.1067 - val_loss: 2.2981 - val_acc: 0.1045\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 2.2960 - acc: 0.1081 - val_loss: 2.2926 - val_acc: 0.1084\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 2.2918 - acc: 0.1104 - val_loss: 2.2899 - val_acc: 0.1097\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 2.2891 - acc: 0.1148 - val_loss: 2.2855 - val_acc: 0.1130\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 2.2895 - acc: 0.1150 - val_loss: 2.2810 - val_acc: 0.1168\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.2806 - acc: 0.1188 - val_loss: 2.2754 - val_acc: 0.1198\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.2813 - acc: 0.1188 - val_loss: 2.2773 - val_acc: 0.1183\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.2788 - acc: 0.1211 - val_loss: 2.2692 - val_acc: 0.1229\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.2724 - acc: 0.1278 - val_loss: 2.2622 - val_acc: 0.1262\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 2.2673 - acc: 0.1283 - val_loss: 2.2540 - val_acc: 0.1296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 2.2619 - acc: 0.1307 - val_loss: 2.2479 - val_acc: 0.1334\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 2.2507 - acc: 0.1351 - val_loss: 2.2300 - val_acc: 0.1330\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.2432 - acc: 0.1370 - val_loss: 2.2181 - val_acc: 0.1386\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 2.2313 - acc: 0.1433 - val_loss: 2.1969 - val_acc: 0.1470\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: 2.2172 - acc: 0.1511 - val_loss: 2.1809 - val_acc: 0.1537\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 2.2055 - acc: 0.1550 - val_loss: 2.1721 - val_acc: 0.1643\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.1998 - acc: 0.1610 - val_loss: 2.1656 - val_acc: 0.1666\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1918 - acc: 0.1662 - val_loss: 2.1606 - val_acc: 0.1726\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 5s 131us/step - loss: 2.1883 - acc: 0.1661 - val_loss: 2.1578 - val_acc: 0.1735\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 2.1867 - acc: 0.1690 - val_loss: 2.1533 - val_acc: 0.1759\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.1838 - acc: 0.1709 - val_loss: 2.1519 - val_acc: 0.1770\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.1836 - acc: 0.1695 - val_loss: 2.1505 - val_acc: 0.1790\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.1810 - acc: 0.1716 - val_loss: 2.1493 - val_acc: 0.1804\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.1795 - acc: 0.1731 - val_loss: 2.1482 - val_acc: 0.1795\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1784 - acc: 0.1732 - val_loss: 2.1456 - val_acc: 0.1833\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1769 - acc: 0.1770 - val_loss: 2.1483 - val_acc: 0.1813\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1763 - acc: 0.1746 - val_loss: 2.1462 - val_acc: 0.1814\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1755 - acc: 0.1783 - val_loss: 2.1463 - val_acc: 0.1815\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1744 - acc: 0.1769 - val_loss: 2.1437 - val_acc: 0.1853\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.1733 - acc: 0.1783 - val_loss: 2.1404 - val_acc: 0.1840\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 2.1740 - acc: 0.1760 - val_loss: 2.1421 - val_acc: 0.1867\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1727 - acc: 0.1779 - val_loss: 2.1420 - val_acc: 0.1872\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1747 - acc: 0.1788 - val_loss: 2.1425 - val_acc: 0.1865\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 2.1730 - acc: 0.1783 - val_loss: 2.1407 - val_acc: 0.1870\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1698 - acc: 0.1798 - val_loss: 2.1414 - val_acc: 0.1855\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.1696 - acc: 0.1797 - val_loss: 2.1406 - val_acc: 0.1869\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 2.1689 - acc: 0.1847 - val_loss: 2.1407 - val_acc: 0.1866\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 5s 134us/step - loss: 2.1693 - acc: 0.1823 - val_loss: 2.1411 - val_acc: 0.1876\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.1681 - acc: 0.1818 - val_loss: 2.1395 - val_acc: 0.1886\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.1694 - acc: 0.1807 - val_loss: 2.1390 - val_acc: 0.1883\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 2.1698 - acc: 0.1805 - val_loss: 2.1387 - val_acc: 0.1906\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1683 - acc: 0.1840 - val_loss: 2.1395 - val_acc: 0.1900\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 2.1694 - acc: 0.1830 - val_loss: 2.1399 - val_acc: 0.1918\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.1688 - acc: 0.1822 - val_loss: 2.1359 - val_acc: 0.1910\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1689 - acc: 0.1834 - val_loss: 2.1425 - val_acc: 0.1859\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.1651 - acc: 0.1857 - val_loss: 2.1345 - val_acc: 0.1920\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 2.1679 - acc: 0.1851 - val_loss: 2.1404 - val_acc: 0.1884\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 2.1633 - acc: 0.1853 - val_loss: 2.1356 - val_acc: 0.1922\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1636 - acc: 0.1837 - val_loss: 2.1380 - val_acc: 0.1902\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1633 - acc: 0.1870 - val_loss: 2.1412 - val_acc: 0.1891\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 2.1631 - acc: 0.1872 - val_loss: 2.1373 - val_acc: 0.1892\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.1629 - acc: 0.1858 - val_loss: 2.1361 - val_acc: 0.1905\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1614 - acc: 0.1873 - val_loss: 2.1374 - val_acc: 0.1915\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1624 - acc: 0.1846 - val_loss: 2.1336 - val_acc: 0.1939\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1631 - acc: 0.1864 - val_loss: 2.1379 - val_acc: 0.1831\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1621 - acc: 0.1867 - val_loss: 2.1354 - val_acc: 0.1935\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1626 - acc: 0.1875 - val_loss: 2.1306 - val_acc: 0.1934\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 2.1568 - acc: 0.1918 - val_loss: 2.1278 - val_acc: 0.1992\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 2.1570 - acc: 0.1932 - val_loss: 2.1236 - val_acc: 0.1991\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 2.1512 - acc: 0.1955 - val_loss: 2.1212 - val_acc: 0.2041\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 2.1553 - acc: 0.1966 - val_loss: 2.1224 - val_acc: 0.2046\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 2.1500 - acc: 0.1989 - val_loss: 2.1186 - val_acc: 0.2067\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.1483 - acc: 0.2000 - val_loss: 2.1142 - val_acc: 0.2099\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.1484 - acc: 0.1988 - val_loss: 2.1136 - val_acc: 0.2098\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.1457 - acc: 0.2034 - val_loss: 2.1102 - val_acc: 0.2103\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1394 - acc: 0.2045 - val_loss: 2.1033 - val_acc: 0.2142\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1411 - acc: 0.2028 - val_loss: 2.1057 - val_acc: 0.2132\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.1413 - acc: 0.2052 - val_loss: 2.1074 - val_acc: 0.2125\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.1396 - acc: 0.2054 - val_loss: 2.1057 - val_acc: 0.2146\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1363 - acc: 0.2056 - val_loss: 2.1018 - val_acc: 0.2163\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1338 - acc: 0.2080 - val_loss: 2.1027 - val_acc: 0.2176\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1385 - acc: 0.2072 - val_loss: 2.0992 - val_acc: 0.2176\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.1309 - acc: 0.2095 - val_loss: 2.0931 - val_acc: 0.2207\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1338 - acc: 0.2123 - val_loss: 2.0940 - val_acc: 0.2199\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.1299 - acc: 0.2133 - val_loss: 2.0926 - val_acc: 0.2231\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 2.1287 - acc: 0.2128 - val_loss: 2.0869 - val_acc: 0.2250\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1266 - acc: 0.2146 - val_loss: 2.0842 - val_acc: 0.2266\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 5s 129us/step - loss: 2.1243 - acc: 0.2175 - val_loss: 2.0853 - val_acc: 0.2262\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.1245 - acc: 0.2164 - val_loss: 2.0799 - val_acc: 0.2289\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.1215 - acc: 0.2173 - val_loss: 2.0845 - val_acc: 0.2280\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.1214 - acc: 0.2179 - val_loss: 2.0772 - val_acc: 0.2324\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1207 - acc: 0.2175 - val_loss: 2.0746 - val_acc: 0.2341\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1169 - acc: 0.2202 - val_loss: 2.0777 - val_acc: 0.2311\n",
      "10000/10000 [==============================] - 2s 194us/step\n",
      "Validation loss: 2.077702976822853\n",
      "Validation accuracy (NORMALIZED): 0.2311000048443675\n",
      "================================================\n",
      "Split: 3\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: 4.6170 - acc: 0.1020 - val_loss: 2.3130 - val_acc: 0.1137\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.4982 - acc: 0.1074 - val_loss: 2.2999 - val_acc: 0.1024\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 4s 102us/step - loss: 2.3435 - acc: 0.1036 - val_loss: 2.3029 - val_acc: 0.1003\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.3171 - acc: 0.1027 - val_loss: 2.3028 - val_acc: 0.1000\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 2.3088 - acc: 0.1026 - val_loss: 2.3028 - val_acc: 0.1000\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.3060 - acc: 0.1025 - val_loss: 2.3024 - val_acc: 0.1003\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 2.3037 - acc: 0.1028 - val_loss: 2.3018 - val_acc: 0.1005\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 2.2997 - acc: 0.1042 - val_loss: 2.3017 - val_acc: 0.1005\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.3015 - acc: 0.1035 - val_loss: 2.3016 - val_acc: 0.1005\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.2990 - acc: 0.1036 - val_loss: 2.3013 - val_acc: 0.1011\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.2954 - acc: 0.1049 - val_loss: 2.2990 - val_acc: 0.1018\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.2940 - acc: 0.1056 - val_loss: 2.2948 - val_acc: 0.1034\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.2896 - acc: 0.1080 - val_loss: 2.2890 - val_acc: 0.1061\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.2873 - acc: 0.1085 - val_loss: 2.2863 - val_acc: 0.1057\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.2833 - acc: 0.1068 - val_loss: 2.2835 - val_acc: 0.1060\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.2849 - acc: 0.1094 - val_loss: 2.2778 - val_acc: 0.1080\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 2.2786 - acc: 0.1120 - val_loss: 2.2696 - val_acc: 0.1109\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: 2.2748 - acc: 0.1127 - val_loss: 2.2656 - val_acc: 0.1098\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 2.2713 - acc: 0.1138 - val_loss: 2.2622 - val_acc: 0.1098\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.2666 - acc: 0.1160 - val_loss: 2.2614 - val_acc: 0.1113\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.2635 - acc: 0.1186 - val_loss: 2.2510 - val_acc: 0.1185\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.2585 - acc: 0.1220 - val_loss: 2.2445 - val_acc: 0.1232\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 5s 125us/step - loss: 2.2504 - acc: 0.1271 - val_loss: 2.2348 - val_acc: 0.1288\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.2456 - acc: 0.1311 - val_loss: 2.2292 - val_acc: 0.1334\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.2387 - acc: 0.1363 - val_loss: 2.2235 - val_acc: 0.1366\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.2386 - acc: 0.1379 - val_loss: 2.2225 - val_acc: 0.1370\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 2.2329 - acc: 0.1415 - val_loss: 2.2150 - val_acc: 0.1417\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 2.2247 - acc: 0.1458 - val_loss: 2.2041 - val_acc: 0.1442\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 6s 139us/step - loss: 2.2150 - acc: 0.1508 - val_loss: 2.1957 - val_acc: 0.1502\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.2113 - acc: 0.1553 - val_loss: 2.1861 - val_acc: 0.1553\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.2009 - acc: 0.1607 - val_loss: 2.1817 - val_acc: 0.1593\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.1954 - acc: 0.1609 - val_loss: 2.1725 - val_acc: 0.1654\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1851 - acc: 0.1692 - val_loss: 2.1643 - val_acc: 0.1736\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1786 - acc: 0.1719 - val_loss: 2.1605 - val_acc: 0.1766\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.1794 - acc: 0.1748 - val_loss: 2.1594 - val_acc: 0.1780\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.1763 - acc: 0.1770 - val_loss: 2.1597 - val_acc: 0.1775\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.1729 - acc: 0.1787 - val_loss: 2.1559 - val_acc: 0.1783\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.1725 - acc: 0.1808 - val_loss: 2.1545 - val_acc: 0.1823\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1737 - acc: 0.1804 - val_loss: 2.1538 - val_acc: 0.1801\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 2.1684 - acc: 0.1811 - val_loss: 2.1528 - val_acc: 0.1820\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1693 - acc: 0.1825 - val_loss: 2.1496 - val_acc: 0.1852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1691 - acc: 0.1837 - val_loss: 2.1532 - val_acc: 0.1813\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.1699 - acc: 0.1810 - val_loss: 2.1569 - val_acc: 0.1816\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.1689 - acc: 0.1859 - val_loss: 2.1508 - val_acc: 0.1831\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.1630 - acc: 0.1863 - val_loss: 2.1526 - val_acc: 0.1848\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.1619 - acc: 0.1895 - val_loss: 2.1519 - val_acc: 0.1859\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.1637 - acc: 0.1878 - val_loss: 2.1482 - val_acc: 0.1876\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.1621 - acc: 0.1881 - val_loss: 2.1485 - val_acc: 0.1888\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.1582 - acc: 0.1914 - val_loss: 2.1506 - val_acc: 0.1840\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 2.1569 - acc: 0.1979 - val_loss: 2.1451 - val_acc: 0.1883\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1518 - acc: 0.1991 - val_loss: 2.1450 - val_acc: 0.1872\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 2.1491 - acc: 0.2012 - val_loss: 2.1517 - val_acc: 0.1853\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.1467 - acc: 0.2016 - val_loss: 2.1453 - val_acc: 0.1869\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 2.1495 - acc: 0.2001 - val_loss: 2.1421 - val_acc: 0.1885\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 2.1459 - acc: 0.2026 - val_loss: 2.1413 - val_acc: 0.1886\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 2.1454 - acc: 0.2025 - val_loss: 2.1420 - val_acc: 0.1873\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.1426 - acc: 0.2047 - val_loss: 2.1401 - val_acc: 0.1882\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 5s 124us/step - loss: 2.1400 - acc: 0.2070 - val_loss: 2.1401 - val_acc: 0.1853\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1410 - acc: 0.2059 - val_loss: 2.1371 - val_acc: 0.1876\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 2.1388 - acc: 0.2060 - val_loss: 2.1391 - val_acc: 0.1867\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1407 - acc: 0.2055 - val_loss: 2.1356 - val_acc: 0.1890\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.1362 - acc: 0.2096 - val_loss: 2.1375 - val_acc: 0.1863\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.1358 - acc: 0.2103 - val_loss: 2.1340 - val_acc: 0.1899\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.1357 - acc: 0.2089 - val_loss: 2.1323 - val_acc: 0.1905\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.1334 - acc: 0.2129 - val_loss: 2.1314 - val_acc: 0.1910\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.1328 - acc: 0.2131 - val_loss: 2.1297 - val_acc: 0.1954\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1304 - acc: 0.2115 - val_loss: 2.1322 - val_acc: 0.1925\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.1343 - acc: 0.2129 - val_loss: 2.1284 - val_acc: 0.1931\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.1276 - acc: 0.2174 - val_loss: 2.1262 - val_acc: 0.1941\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.1285 - acc: 0.2171 - val_loss: 2.1287 - val_acc: 0.1932\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1302 - acc: 0.2192 - val_loss: 2.1265 - val_acc: 0.1957\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1264 - acc: 0.2195 - val_loss: 2.1267 - val_acc: 0.1984\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.1237 - acc: 0.2228 - val_loss: 2.1194 - val_acc: 0.2073\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1244 - acc: 0.2220 - val_loss: 2.1242 - val_acc: 0.2013\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1228 - acc: 0.2233 - val_loss: 2.1201 - val_acc: 0.2043\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.1223 - acc: 0.2261 - val_loss: 2.1238 - val_acc: 0.2010\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1223 - acc: 0.2258 - val_loss: 2.1208 - val_acc: 0.2053\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1187 - acc: 0.2259 - val_loss: 2.1219 - val_acc: 0.2036\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 2.1223 - acc: 0.2274 - val_loss: 2.1208 - val_acc: 0.2093\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.1213 - acc: 0.2284 - val_loss: 2.1190 - val_acc: 0.2062\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.1198 - acc: 0.2275 - val_loss: 2.1157 - val_acc: 0.2105\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1162 - acc: 0.2296 - val_loss: 2.1162 - val_acc: 0.2092\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 2.1213 - acc: 0.2273 - val_loss: 2.1196 - val_acc: 0.2067\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 2.1157 - acc: 0.2282 - val_loss: 2.1167 - val_acc: 0.2114\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.1186 - acc: 0.2285 - val_loss: 2.1154 - val_acc: 0.2135\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.1195 - acc: 0.2294 - val_loss: 2.1153 - val_acc: 0.2116\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.1172 - acc: 0.2297 - val_loss: 2.1162 - val_acc: 0.2118\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.1175 - acc: 0.2313 - val_loss: 2.1152 - val_acc: 0.2150\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: 2.1167 - acc: 0.2286 - val_loss: 2.1126 - val_acc: 0.2151\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.1157 - acc: 0.2320 - val_loss: 2.1143 - val_acc: 0.2148\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1139 - acc: 0.2312 - val_loss: 2.1123 - val_acc: 0.2189\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: 2.1162 - acc: 0.2341 - val_loss: 2.1147 - val_acc: 0.2168\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.1133 - acc: 0.2344 - val_loss: 2.1138 - val_acc: 0.2173\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.1144 - acc: 0.2301 - val_loss: 2.1094 - val_acc: 0.2221\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 2.1122 - acc: 0.2329 - val_loss: 2.1106 - val_acc: 0.2201\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.1124 - acc: 0.2345 - val_loss: 2.0912 - val_acc: 0.2460\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.0952 - acc: 0.2536 - val_loss: 2.0570 - val_acc: 0.2722\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.0892 - acc: 0.2576 - val_loss: 2.0571 - val_acc: 0.2721\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 4s 103us/step - loss: 2.0891 - acc: 0.2575 - val_loss: 2.0608 - val_acc: 0.2687\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 4s 104us/step - loss: 2.0895 - acc: 0.2590 - val_loss: 2.0545 - val_acc: 0.2724\n",
      "10000/10000 [==============================] - 2s 185us/step\n",
      "Validation loss: 2.05449698638916\n",
      "Validation accuracy (NORMALIZED): 0.27240000557154415\n",
      "================================================\n",
      "Split: 4\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 5s 134us/step - loss: 5.0392 - acc: 0.1127 - val_loss: 2.3286 - val_acc: 0.1155\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.5362 - acc: 0.1137 - val_loss: 2.3008 - val_acc: 0.1022\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.3559 - acc: 0.1072 - val_loss: 2.3016 - val_acc: 0.1004\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.3214 - acc: 0.1059 - val_loss: 2.3017 - val_acc: 0.1001\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.3096 - acc: 0.1053 - val_loss: 2.3016 - val_acc: 0.1003\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.3062 - acc: 0.1045 - val_loss: 2.3017 - val_acc: 0.1003\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.3000 - acc: 0.1045 - val_loss: 2.3013 - val_acc: 0.1005\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.2993 - acc: 0.1054 - val_loss: 2.3014 - val_acc: 0.1005\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.2958 - acc: 0.1076 - val_loss: 2.2991 - val_acc: 0.1022\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.2901 - acc: 0.1122 - val_loss: 2.2928 - val_acc: 0.1085\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: 2.2898 - acc: 0.1151 - val_loss: 2.2934 - val_acc: 0.1075\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.2866 - acc: 0.1157 - val_loss: 2.2896 - val_acc: 0.1103\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.2826 - acc: 0.1186 - val_loss: 2.2787 - val_acc: 0.1174\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.2774 - acc: 0.1227 - val_loss: 2.2732 - val_acc: 0.1192\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.2695 - acc: 0.1276 - val_loss: 2.2606 - val_acc: 0.1251\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.2610 - acc: 0.1294 - val_loss: 2.2429 - val_acc: 0.1327\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.2465 - acc: 0.1449 - val_loss: 2.2206 - val_acc: 0.1434\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 4s 110us/step - loss: 2.2322 - acc: 0.1533 - val_loss: 2.2017 - val_acc: 0.1513\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.2178 - acc: 0.1601 - val_loss: 2.1847 - val_acc: 0.1628\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.2011 - acc: 0.1692 - val_loss: 2.1695 - val_acc: 0.1741\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1957 - acc: 0.1816 - val_loss: 2.1596 - val_acc: 0.1821\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.1833 - acc: 0.1901 - val_loss: 2.1506 - val_acc: 0.1907\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.1679 - acc: 0.1979 - val_loss: 2.1360 - val_acc: 0.2023\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.1574 - acc: 0.2074 - val_loss: 2.1361 - val_acc: 0.2082\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.1538 - acc: 0.2114 - val_loss: 2.1264 - val_acc: 0.2142\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 5s 134us/step - loss: 2.1491 - acc: 0.2171 - val_loss: 2.1204 - val_acc: 0.2191\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: 2.1424 - acc: 0.2221 - val_loss: 2.1175 - val_acc: 0.2272\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 2.1434 - acc: 0.2233 - val_loss: 2.1133 - val_acc: 0.2316\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: 2.1358 - acc: 0.2256 - val_loss: 2.1120 - val_acc: 0.2298\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 2.1311 - acc: 0.2321 - val_loss: 2.1076 - val_acc: 0.2361\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 5s 130us/step - loss: 2.1328 - acc: 0.2317 - val_loss: 2.1047 - val_acc: 0.2388\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 5s 130us/step - loss: 2.1287 - acc: 0.2354 - val_loss: 2.1015 - val_acc: 0.2426\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 2.1256 - acc: 0.2367 - val_loss: 2.0990 - val_acc: 0.2444\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 2.1249 - acc: 0.2387 - val_loss: 2.0958 - val_acc: 0.2475\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.1195 - acc: 0.2444 - val_loss: 2.0941 - val_acc: 0.2527\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.1179 - acc: 0.2421 - val_loss: 2.0945 - val_acc: 0.2489\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.1123 - acc: 0.2477 - val_loss: 2.0860 - val_acc: 0.2575\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.1138 - acc: 0.2473 - val_loss: 2.0888 - val_acc: 0.2563\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.1112 - acc: 0.2491 - val_loss: 2.0811 - val_acc: 0.2612\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.1132 - acc: 0.2497 - val_loss: 2.0804 - val_acc: 0.2634\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.1090 - acc: 0.2501 - val_loss: 2.0794 - val_acc: 0.2638\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.1027 - acc: 0.2532 - val_loss: 2.0713 - val_acc: 0.2698\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.1029 - acc: 0.2557 - val_loss: 2.0697 - val_acc: 0.2718\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.1008 - acc: 0.2559 - val_loss: 2.0683 - val_acc: 0.2725\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0999 - acc: 0.2576 - val_loss: 2.0648 - val_acc: 0.2751\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0997 - acc: 0.2577 - val_loss: 2.0628 - val_acc: 0.2755\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0954 - acc: 0.2596 - val_loss: 2.0648 - val_acc: 0.2740\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0927 - acc: 0.2635 - val_loss: 2.0573 - val_acc: 0.2768\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0939 - acc: 0.2617 - val_loss: 2.0574 - val_acc: 0.2749\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0942 - acc: 0.2626 - val_loss: 2.0626 - val_acc: 0.2764\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0928 - acc: 0.2619 - val_loss: 2.0554 - val_acc: 0.2793\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0922 - acc: 0.2650 - val_loss: 2.0578 - val_acc: 0.2774\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0892 - acc: 0.2656 - val_loss: 2.0534 - val_acc: 0.2807\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0909 - acc: 0.2645 - val_loss: 2.0551 - val_acc: 0.2798\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0890 - acc: 0.2645 - val_loss: 2.0547 - val_acc: 0.2803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.0909 - acc: 0.2637 - val_loss: 2.0509 - val_acc: 0.2845\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.0906 - acc: 0.2650 - val_loss: 2.0500 - val_acc: 0.2858\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0894 - acc: 0.2652 - val_loss: 2.0498 - val_acc: 0.2818\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0870 - acc: 0.2651 - val_loss: 2.0486 - val_acc: 0.2846\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0858 - acc: 0.2668 - val_loss: 2.0497 - val_acc: 0.2837\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0904 - acc: 0.2645 - val_loss: 2.0502 - val_acc: 0.2868\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0865 - acc: 0.2647 - val_loss: 2.0475 - val_acc: 0.2851\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0877 - acc: 0.2668 - val_loss: 2.0462 - val_acc: 0.2842\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0870 - acc: 0.2651 - val_loss: 2.0479 - val_acc: 0.2861\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0863 - acc: 0.2659 - val_loss: 2.0470 - val_acc: 0.2853\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0853 - acc: 0.2669 - val_loss: 2.0441 - val_acc: 0.2855\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0857 - acc: 0.2668 - val_loss: 2.0446 - val_acc: 0.2843\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0871 - acc: 0.2662 - val_loss: 2.0485 - val_acc: 0.2845\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0844 - acc: 0.2693 - val_loss: 2.0480 - val_acc: 0.2832\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0850 - acc: 0.2687 - val_loss: 2.0462 - val_acc: 0.2869\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: 2.0850 - acc: 0.2675 - val_loss: 2.0479 - val_acc: 0.2826\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: 2.0833 - acc: 0.2689 - val_loss: 2.0439 - val_acc: 0.2869\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 5s 119us/step - loss: 2.0847 - acc: 0.2659 - val_loss: 2.0456 - val_acc: 0.2837\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: 2.0814 - acc: 0.2705 - val_loss: 2.0418 - val_acc: 0.2875\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0832 - acc: 0.2690 - val_loss: 2.0470 - val_acc: 0.2868\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.0835 - acc: 0.2696 - val_loss: 2.0440 - val_acc: 0.2873\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.0824 - acc: 0.2677 - val_loss: 2.0434 - val_acc: 0.2892\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.0825 - acc: 0.2679 - val_loss: 2.0420 - val_acc: 0.2920\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.0818 - acc: 0.2674 - val_loss: 2.0414 - val_acc: 0.2877\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: 2.0804 - acc: 0.2683 - val_loss: 2.0422 - val_acc: 0.2857\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0799 - acc: 0.2701 - val_loss: 2.0395 - val_acc: 0.2874\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.0787 - acc: 0.2699 - val_loss: 2.0400 - val_acc: 0.2897\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.0775 - acc: 0.2703 - val_loss: 2.0353 - val_acc: 0.2906\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.0767 - acc: 0.2704 - val_loss: 2.0366 - val_acc: 0.2900\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: 2.0749 - acc: 0.2715 - val_loss: 2.0352 - val_acc: 0.2886\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.0767 - acc: 0.2713 - val_loss: 2.0379 - val_acc: 0.2871\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.0748 - acc: 0.2702 - val_loss: 2.0336 - val_acc: 0.2903\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 4s 107us/step - loss: 2.0747 - acc: 0.2715 - val_loss: 2.0344 - val_acc: 0.2906\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0737 - acc: 0.2727 - val_loss: 2.0325 - val_acc: 0.2920\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0737 - acc: 0.2744 - val_loss: 2.0332 - val_acc: 0.2959\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0702 - acc: 0.2714 - val_loss: 2.0288 - val_acc: 0.2946\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 2.0680 - acc: 0.2742 - val_loss: 2.0315 - val_acc: 0.2941\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: 2.0704 - acc: 0.2736 - val_loss: 2.0325 - val_acc: 0.2915\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.0688 - acc: 0.2726 - val_loss: 2.0271 - val_acc: 0.2948\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 4s 106us/step - loss: 2.0688 - acc: 0.2724 - val_loss: 2.0270 - val_acc: 0.2932\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 4s 109us/step - loss: 2.0690 - acc: 0.2757 - val_loss: 2.0270 - val_acc: 0.2974\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.0684 - acc: 0.2752 - val_loss: 2.0296 - val_acc: 0.2940\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: 2.0688 - acc: 0.2730 - val_loss: 2.0288 - val_acc: 0.2953\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: 2.0649 - acc: 0.2770 - val_loss: 2.0249 - val_acc: 0.2948\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: 2.0652 - acc: 0.2757 - val_loss: 2.0236 - val_acc: 0.3008\n",
      "10000/10000 [==============================] - 2s 195us/step\n",
      "Validation loss: 2.023556424736977\n",
      "Validation accuracy (NORMALIZED): 0.3008000060021877\n",
      "[[2.0255626554489137, 0.2917000061571598], [1.9278625366687774, 0.33970000661909583], [2.077702976822853, 0.2311000048443675], [2.05449698638916, 0.27240000557154415], [2.023556424736977, 0.3008000060021877]]\n"
     ]
    }
   ],
   "source": [
    "#=====================================\n",
    "# Training model and Evaluation\n",
    "#=====================================\n",
    "\n",
    "y_train_categorical = to_categorical(y_train, num_classes=n_classes)\n",
    "y_val_categorical = to_categorical(y_val, num_classes=n_classes)\n",
    "\n",
    "scores = []\n",
    "for i in range(folds):\n",
    "    #--- Loading model\n",
    "    model  = get_squeezenet_ft()\n",
    "    \n",
    "    #--- Evaluating the model for split i\n",
    "    print(\"================================================\")\n",
    "    print(\"Split: \"+str(i))\n",
    "    print(\"Number of Epochs: \"+str(n_epochs))\n",
    "    print(\"Train shape:\",X_train[i].shape)\n",
    "    print(\"Train batch size: \"+str(train_batch_size))\n",
    "    print(\"Val batch size: \"+str(val_batch_size))\n",
    "    print(\"================================================\")\n",
    "    \n",
    "    # Training with data augmentation\n",
    "    aug.fit(X_train[i])\n",
    "    model.fit_generator(aug.flow(X_train[i],y_train_categorical[i], batch_size=train_batch_size),\n",
    "                        steps_per_epoch=X_train.shape[1]//train_batch_size,\n",
    "                        epochs=n_epochs, \n",
    "                        verbose=1)\n",
    "    \n",
    "    #--- Evaluating the model for split i\n",
    "    score = model.evaluate(x=X_val[i], y=y_val_categorical[i], batch_size=val_batch_size, verbose=1)\n",
    "    scores.append(score)\n",
    "    print('Validation loss:', score[0])\n",
    "    print('Validation accuracy (NORMALIZED):', score[1])\n",
    "    \n",
    "#--- Showing scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Validation loss: 2.0218363160133364\n",
      "Mean Validation accuracy (NORMALIZED): 0.28714000583887095\n"
     ]
    }
   ],
   "source": [
    "#=====================================\n",
    "# Evaluate on validation\n",
    "#=====================================\n",
    "\n",
    "#--- The evaluation of the model \n",
    "np_aux = np.array(scores).mean(axis=0)\n",
    "\n",
    "print('Mean Validation loss:', np_aux[0])\n",
    "print('Mean Validation accuracy (NORMALIZED):', np_aux[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "-----------------\n",
    "\n",
    "# Training last 2 Fire Modules + classification layers\n",
    "As we could see, the frozen network performed very poorly. By freezing most layers, we do not allow SqueezeNet to adapt its weights to features present in CIFAR-10.\n",
    "\n",
    "Let's try to unfreeze the last two fire modules and train once more. The architecture will be:\n",
    "<img src=\"partFrozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 15, 15, 64)   1792        input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv1 (Activation)         (None, 15, 15, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 7, 7, 64)     0           relu_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fire2/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     1040        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire2/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire2/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire2/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/concat (Concatenate)      (None, 7, 7, 128)    0           fire2/relu_expand1x1[0][0]       \n",
      "                                                                 fire2/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire3/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     2064        fire2/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire3/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire3/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire3/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/concat (Concatenate)      (None, 7, 7, 128)    0           fire3/relu_expand1x1[0][0]       \n",
      "                                                                 fire3/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 3, 3, 128)    0           fire3/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire4/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     4128        pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire4/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire4/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire4/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/concat (Concatenate)      (None, 3, 3, 256)    0           fire4/relu_expand1x1[0][0]       \n",
      "                                                                 fire4/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire5/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     8224        fire4/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire5/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire5/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire5/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/concat (Concatenate)      (None, 3, 3, 256)    0           fire5/relu_expand1x1[0][0]       \n",
      "                                                                 fire5/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 1, 1, 256)    0           fire5/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire6/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     12336       pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire6/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire6/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire6/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/concat (Concatenate)      (None, 1, 1, 384)    0           fire6/relu_expand1x1[0][0]       \n",
      "                                                                 fire6/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire7/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     18480       fire6/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire7/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire7/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire7/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/concat (Concatenate)      (None, 1, 1, 384)    0           fire7/relu_expand1x1[0][0]       \n",
      "                                                                 fire7/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire8/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     24640       fire7/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire8/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire8/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire8/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/concat (Concatenate)      (None, 1, 1, 512)    0           fire8/relu_expand1x1[0][0]       \n",
      "                                                                 fire8/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire9/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     32832       fire8/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire9/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire9/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire9/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/concat (Concatenate)      (None, 1, 1, 512)    0           fire9/relu_expand1x1[0][0]       \n",
      "                                                                 fire9/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "drop9 (Dropout)                 (None, 1, 1, 512)    0           fire9/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv10 (Conv2D)                 (None, 1, 1, 10)     5130        drop9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv10 (Activation)        (None, 1, 1, 10)     0           conv10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_18 (Gl (None, 10)           0           relu_conv10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "loss (Activation)               (None, 10)           0           global_average_pooling2d_18[0][0]\n",
      "==================================================================================================\n",
      "Total params: 727,626\n",
      "Trainable params: 391,306\n",
      "Non-trainable params: 336,320\n",
      "__________________________________________________________________________________________________\n",
      "input_9 False\n",
      "conv1 False\n",
      "relu_conv1 False\n",
      "pool1 False\n",
      "fire2/squeeze1x1 False\n",
      "fire2/relu_squeeze1x1 False\n",
      "fire2/expand1x1 False\n",
      "fire2/expand3x3 False\n",
      "fire2/relu_expand1x1 False\n",
      "fire2/relu_expand3x3 False\n",
      "fire2/concat False\n",
      "fire3/squeeze1x1 False\n",
      "fire3/relu_squeeze1x1 False\n",
      "fire3/expand1x1 False\n",
      "fire3/expand3x3 False\n",
      "fire3/relu_expand1x1 False\n",
      "fire3/relu_expand3x3 False\n",
      "fire3/concat False\n",
      "pool3 False\n",
      "fire4/squeeze1x1 False\n",
      "fire4/relu_squeeze1x1 False\n",
      "fire4/expand1x1 False\n",
      "fire4/expand3x3 False\n",
      "fire4/relu_expand1x1 False\n",
      "fire4/relu_expand3x3 False\n",
      "fire4/concat False\n",
      "fire5/squeeze1x1 False\n",
      "fire5/relu_squeeze1x1 False\n",
      "fire5/expand1x1 False\n",
      "fire5/expand3x3 False\n",
      "fire5/relu_expand1x1 False\n",
      "fire5/relu_expand3x3 False\n",
      "fire5/concat False\n",
      "pool5 False\n",
      "fire6/squeeze1x1 False\n",
      "fire6/relu_squeeze1x1 False\n",
      "fire6/expand1x1 False\n",
      "fire6/expand3x3 False\n",
      "fire6/relu_expand1x1 False\n",
      "fire6/relu_expand3x3 False\n",
      "fire6/concat False\n",
      "fire7/squeeze1x1 False\n",
      "fire7/relu_squeeze1x1 False\n",
      "fire7/expand1x1 False\n",
      "fire7/expand3x3 False\n",
      "fire7/relu_expand1x1 False\n",
      "fire7/relu_expand3x3 False\n",
      "fire7/concat False\n",
      "fire8/squeeze1x1 True\n",
      "fire8/relu_squeeze1x1 True\n",
      "fire8/expand1x1 True\n",
      "fire8/expand3x3 True\n",
      "fire8/relu_expand1x1 True\n",
      "fire8/relu_expand3x3 True\n",
      "fire8/concat True\n",
      "fire9/squeeze1x1 True\n",
      "fire9/relu_squeeze1x1 True\n",
      "fire9/expand1x1 True\n",
      "fire9/expand3x3 True\n",
      "fire9/relu_expand1x1 True\n",
      "fire9/relu_expand3x3 True\n",
      "fire9/concat True\n",
      "drop9 True\n",
      "conv10 True\n",
      "relu_conv10 True\n",
      "global_average_pooling2d_18 True\n",
      "loss True\n"
     ]
    }
   ],
   "source": [
    "def get_squeezenet_ft2():\n",
    "    squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "    #=====================================\n",
    "    # Freezing mentioned layers\n",
    "    #=====================================\n",
    "\n",
    "    trainable_layer_index = 19\n",
    "    for i in range(len(squeezeNetModel.layers)-trainable_layer_index):\n",
    "        squeezeNetModel.layers[i].trainable = False\n",
    "\n",
    "    #=====================================\n",
    "    # Add new classification layers\n",
    "    #=====================================\n",
    "\n",
    "    #--- Removing layers until drop 9\n",
    "    squeezeNetModel.layers.pop() #Convolution2D\n",
    "    squeezeNetModel.layers.pop() #Activation ReLU\n",
    "    squeezeNetModel.layers.pop() #Global Avg Pool\n",
    "    squeezeNetModel.layers.pop() #Activation Softmax\n",
    "\n",
    "    #--- Adding classification layer for 10 classes\n",
    "    out = Convolution2D(n_classes, (1, 1), padding='valid', name='conv10')(squeezeNetModel.layers[-1].output)\n",
    "    out = Activation('relu', name='relu_conv10')(out)\n",
    "\n",
    "    out = GlobalAveragePooling2D()(out)\n",
    "    out = Activation('softmax', name='loss')(out)\n",
    "\n",
    "    #=====================================\n",
    "    # New Model\n",
    "    #=====================================\n",
    "    model = Model(squeezeNetModel.inputs, out, name='squeezenet_new')\n",
    "    opt = optimizer=optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)#SGD(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_squeezenet_ft2()\n",
    "model.summary()\n",
    "\n",
    "#--- Check the trainable status of the individual layers\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Split: 0\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 8s 199us/step - loss: 2.3657 - acc: 0.1563 - val_loss: 2.1477 - val_acc: 0.2350\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 7s 184us/step - loss: 2.1195 - acc: 0.2282 - val_loss: 1.9936 - val_acc: 0.3222\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 2.0531 - acc: 0.2727 - val_loss: 1.8918 - val_acc: 0.3797\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.9549 - acc: 0.3215 - val_loss: 1.7618 - val_acc: 0.4315\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.8505 - acc: 0.3544 - val_loss: 1.6496 - val_acc: 0.4389\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.7884 - acc: 0.3707 - val_loss: 1.6109 - val_acc: 0.4553\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 1.7457 - acc: 0.3842 - val_loss: 1.5789 - val_acc: 0.4615\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.7069 - acc: 0.3946 - val_loss: 1.5430 - val_acc: 0.4700\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.6764 - acc: 0.4077 - val_loss: 1.5324 - val_acc: 0.4717\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.6468 - acc: 0.4233 - val_loss: 1.5133 - val_acc: 0.4796\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 1.6233 - acc: 0.4325 - val_loss: 1.5036 - val_acc: 0.4737\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.5975 - acc: 0.4448 - val_loss: 1.4741 - val_acc: 0.4925\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.5766 - acc: 0.4521 - val_loss: 1.4810 - val_acc: 0.4836\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 1.5618 - acc: 0.4583 - val_loss: 1.4664 - val_acc: 0.4913\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.5399 - acc: 0.4651 - val_loss: 1.4561 - val_acc: 0.4921\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 1.5177 - acc: 0.4746 - val_loss: 1.4594 - val_acc: 0.4930\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.5078 - acc: 0.4806 - val_loss: 1.4490 - val_acc: 0.4982\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.4913 - acc: 0.4841 - val_loss: 1.4419 - val_acc: 0.5008\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4770 - acc: 0.4905 - val_loss: 1.4317 - val_acc: 0.5038\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.4647 - acc: 0.4951 - val_loss: 1.4256 - val_acc: 0.5092\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.4483 - acc: 0.5015 - val_loss: 1.4229 - val_acc: 0.5106\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4352 - acc: 0.5074 - val_loss: 1.4165 - val_acc: 0.5098\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.4247 - acc: 0.5085 - val_loss: 1.4190 - val_acc: 0.5068\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 7s 181us/step - loss: 1.4095 - acc: 0.5142 - val_loss: 1.4250 - val_acc: 0.5110\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 1.3997 - acc: 0.5188 - val_loss: 1.4101 - val_acc: 0.5113\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 7s 179us/step - loss: 1.3855 - acc: 0.5225 - val_loss: 1.4150 - val_acc: 0.5084\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.3766 - acc: 0.5252 - val_loss: 1.4073 - val_acc: 0.5135\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.3651 - acc: 0.5296 - val_loss: 1.4240 - val_acc: 0.5088\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.3485 - acc: 0.5326 - val_loss: 1.4127 - val_acc: 0.5092\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.3418 - acc: 0.5374 - val_loss: 1.4159 - val_acc: 0.5104\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.3300 - acc: 0.5402 - val_loss: 1.4079 - val_acc: 0.5193\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3178 - acc: 0.5427 - val_loss: 1.4123 - val_acc: 0.5168\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3084 - acc: 0.5448 - val_loss: 1.4110 - val_acc: 0.5150\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2996 - acc: 0.5492 - val_loss: 1.4403 - val_acc: 0.5155\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2889 - acc: 0.5514 - val_loss: 1.4213 - val_acc: 0.5150\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.2756 - acc: 0.5530 - val_loss: 1.4218 - val_acc: 0.5134\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2683 - acc: 0.5584 - val_loss: 1.4272 - val_acc: 0.5161\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2607 - acc: 0.5591 - val_loss: 1.4391 - val_acc: 0.5105\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2482 - acc: 0.5612 - val_loss: 1.4264 - val_acc: 0.5142\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 1.2379 - acc: 0.5648 - val_loss: 1.4399 - val_acc: 0.5127\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 7s 183us/step - loss: 1.2276 - acc: 0.5702 - val_loss: 1.4386 - val_acc: 0.5173\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 7s 177us/step - loss: 1.2206 - acc: 0.5707 - val_loss: 1.4536 - val_acc: 0.5181\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.2083 - acc: 0.5739 - val_loss: 1.4524 - val_acc: 0.5168\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.1957 - acc: 0.5801 - val_loss: 1.4432 - val_acc: 0.5199\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.1910 - acc: 0.5781 - val_loss: 1.4571 - val_acc: 0.5147\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 6s 159us/step - loss: 1.1808 - acc: 0.5851 - val_loss: 1.4563 - val_acc: 0.5155\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1739 - acc: 0.5860 - val_loss: 1.4837 - val_acc: 0.5155\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.1633 - acc: 0.5898 - val_loss: 1.4881 - val_acc: 0.5157\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 1.1512 - acc: 0.5918 - val_loss: 1.4857 - val_acc: 0.5187\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 6s 162us/step - loss: 1.1390 - acc: 0.5927 - val_loss: 1.4668 - val_acc: 0.5140\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 6s 160us/step - loss: 1.1348 - acc: 0.5956 - val_loss: 1.5095 - val_acc: 0.5073\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 6s 161us/step - loss: 1.1242 - acc: 0.5995 - val_loss: 1.5080 - val_acc: 0.5165\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.1151 - acc: 0.6019 - val_loss: 1.5260 - val_acc: 0.5181\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1079 - acc: 0.6056 - val_loss: 1.4969 - val_acc: 0.5128\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 1.1000 - acc: 0.6099 - val_loss: 1.5495 - val_acc: 0.5159\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.0875 - acc: 0.6105 - val_loss: 1.5194 - val_acc: 0.5102\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.0788 - acc: 0.6160 - val_loss: 1.5150 - val_acc: 0.5127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0673 - acc: 0.6181 - val_loss: 1.5291 - val_acc: 0.5124\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.0608 - acc: 0.6196 - val_loss: 1.5571 - val_acc: 0.5182\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0516 - acc: 0.6214 - val_loss: 1.5548 - val_acc: 0.5149\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 7s 177us/step - loss: 1.0409 - acc: 0.6244 - val_loss: 1.5887 - val_acc: 0.5093\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.0350 - acc: 0.6271 - val_loss: 1.5692 - val_acc: 0.5141\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0254 - acc: 0.6320 - val_loss: 1.5882 - val_acc: 0.5154\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 1.0142 - acc: 0.6336 - val_loss: 1.6141 - val_acc: 0.5137\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.0131 - acc: 0.6337 - val_loss: 1.6335 - val_acc: 0.5113\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.0063 - acc: 0.6365 - val_loss: 1.6437 - val_acc: 0.5159\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.9941 - acc: 0.6394 - val_loss: 1.6623 - val_acc: 0.5118\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.9852 - acc: 0.6439 - val_loss: 1.6613 - val_acc: 0.5019\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 6s 162us/step - loss: 0.9776 - acc: 0.6468 - val_loss: 1.6756 - val_acc: 0.5095\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 0.9690 - acc: 0.6472 - val_loss: 1.6613 - val_acc: 0.5091\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.9610 - acc: 0.6531 - val_loss: 1.6725 - val_acc: 0.5050\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 6s 162us/step - loss: 0.9511 - acc: 0.6540 - val_loss: 1.6733 - val_acc: 0.5083\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 6s 162us/step - loss: 0.9470 - acc: 0.6554 - val_loss: 1.7078 - val_acc: 0.5062\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 0.9380 - acc: 0.6568 - val_loss: 1.6887 - val_acc: 0.5008\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 6s 160us/step - loss: 0.9260 - acc: 0.6631 - val_loss: 1.7164 - val_acc: 0.5094\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 6s 161us/step - loss: 0.9248 - acc: 0.6631 - val_loss: 1.7026 - val_acc: 0.4991\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 6s 161us/step - loss: 0.9146 - acc: 0.6690 - val_loss: 1.7386 - val_acc: 0.5030\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 0.9061 - acc: 0.6680 - val_loss: 1.7790 - val_acc: 0.5020\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 0.9021 - acc: 0.6720 - val_loss: 1.7948 - val_acc: 0.5045\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.8918 - acc: 0.6740 - val_loss: 1.7900 - val_acc: 0.5050\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 0.8819 - acc: 0.6774 - val_loss: 1.7490 - val_acc: 0.4968\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 7s 179us/step - loss: 0.8767 - acc: 0.6800 - val_loss: 1.8720 - val_acc: 0.4995\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 0.8716 - acc: 0.6822 - val_loss: 1.8208 - val_acc: 0.5011\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 0.8599 - acc: 0.6854 - val_loss: 1.8939 - val_acc: 0.5060\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.8589 - acc: 0.6852 - val_loss: 1.8528 - val_acc: 0.5047\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.8471 - acc: 0.6898 - val_loss: 1.8408 - val_acc: 0.4963\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 0.8429 - acc: 0.6897 - val_loss: 1.8636 - val_acc: 0.5043\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 0.8389 - acc: 0.6915 - val_loss: 1.8759 - val_acc: 0.4974\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8299 - acc: 0.6933 - val_loss: 1.9649 - val_acc: 0.4969\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 0.8254 - acc: 0.6951 - val_loss: 1.9618 - val_acc: 0.4980\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.8199 - acc: 0.6973 - val_loss: 1.9121 - val_acc: 0.4941\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.8100 - acc: 0.7018 - val_loss: 1.9602 - val_acc: 0.4970\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.8068 - acc: 0.7005 - val_loss: 1.9308 - val_acc: 0.4966\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.7983 - acc: 0.7065 - val_loss: 1.9999 - val_acc: 0.4962\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.7902 - acc: 0.7063 - val_loss: 1.9608 - val_acc: 0.4957\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.7858 - acc: 0.7069 - val_loss: 2.0397 - val_acc: 0.4951\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 0.7797 - acc: 0.7104 - val_loss: 2.0647 - val_acc: 0.4974\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 0.7773 - acc: 0.7152 - val_loss: 1.9809 - val_acc: 0.4965\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.7690 - acc: 0.7144 - val_loss: 2.1065 - val_acc: 0.4950\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.7582 - acc: 0.7200 - val_loss: 2.0585 - val_acc: 0.4937\n",
      "10000/10000 [==============================] - 2s 198us/step\n",
      "Validation loss: 2.0585452838242055\n",
      "Validation accuracy (NORMALIZED): 0.49370000678300857\n",
      "================================================\n",
      "Split: 1\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 8s 204us/step - loss: 2.3974 - acc: 0.1593 - val_loss: 1.9914 - val_acc: 0.3206\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 2.0475 - acc: 0.2529 - val_loss: 1.8458 - val_acc: 0.3751\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.9572 - acc: 0.2942 - val_loss: 1.7959 - val_acc: 0.4070\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.8674 - acc: 0.3378 - val_loss: 1.7013 - val_acc: 0.4270\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.7886 - acc: 0.3655 - val_loss: 1.6304 - val_acc: 0.4419\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.7309 - acc: 0.3854 - val_loss: 1.5825 - val_acc: 0.4483\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.6904 - acc: 0.4060 - val_loss: 1.5581 - val_acc: 0.4589\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.6467 - acc: 0.4203 - val_loss: 1.5412 - val_acc: 0.4646\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.6287 - acc: 0.4315 - val_loss: 1.5277 - val_acc: 0.4654\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.6087 - acc: 0.4393 - val_loss: 1.5148 - val_acc: 0.4690\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.5900 - acc: 0.4479 - val_loss: 1.5192 - val_acc: 0.4733\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.5683 - acc: 0.4565 - val_loss: 1.4931 - val_acc: 0.4793\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.5508 - acc: 0.4636 - val_loss: 1.4805 - val_acc: 0.4894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.5318 - acc: 0.4700 - val_loss: 1.4710 - val_acc: 0.4922\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.5210 - acc: 0.4749 - val_loss: 1.4678 - val_acc: 0.4900\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.5017 - acc: 0.4800 - val_loss: 1.4590 - val_acc: 0.4970\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4825 - acc: 0.4881 - val_loss: 1.4547 - val_acc: 0.4987\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4667 - acc: 0.4936 - val_loss: 1.4573 - val_acc: 0.4913\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4584 - acc: 0.4994 - val_loss: 1.4581 - val_acc: 0.4953\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4440 - acc: 0.5003 - val_loss: 1.4391 - val_acc: 0.4997\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4332 - acc: 0.5039 - val_loss: 1.4407 - val_acc: 0.5001\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4157 - acc: 0.5138 - val_loss: 1.4502 - val_acc: 0.4982\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4047 - acc: 0.5146 - val_loss: 1.4374 - val_acc: 0.5037\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3949 - acc: 0.5191 - val_loss: 1.4310 - val_acc: 0.4998\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3804 - acc: 0.5247 - val_loss: 1.4402 - val_acc: 0.5066\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3685 - acc: 0.5249 - val_loss: 1.4317 - val_acc: 0.5061\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3633 - acc: 0.5297 - val_loss: 1.4268 - val_acc: 0.5054\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3508 - acc: 0.5313 - val_loss: 1.4409 - val_acc: 0.5080\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3399 - acc: 0.5345 - val_loss: 1.4266 - val_acc: 0.5049\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3271 - acc: 0.5397 - val_loss: 1.4370 - val_acc: 0.5077\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3155 - acc: 0.5421 - val_loss: 1.4348 - val_acc: 0.5114\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3050 - acc: 0.5455 - val_loss: 1.4487 - val_acc: 0.5098\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2950 - acc: 0.5492 - val_loss: 1.4473 - val_acc: 0.5075\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2908 - acc: 0.5467 - val_loss: 1.4572 - val_acc: 0.5005\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2812 - acc: 0.5515 - val_loss: 1.4336 - val_acc: 0.5095\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.2671 - acc: 0.5554 - val_loss: 1.4452 - val_acc: 0.5067\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2591 - acc: 0.5573 - val_loss: 1.4430 - val_acc: 0.5089\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.2476 - acc: 0.5588 - val_loss: 1.4483 - val_acc: 0.5120\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2373 - acc: 0.5658 - val_loss: 1.4595 - val_acc: 0.5073\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.2274 - acc: 0.5661 - val_loss: 1.4687 - val_acc: 0.5051\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2211 - acc: 0.5706 - val_loss: 1.4526 - val_acc: 0.5090\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.2090 - acc: 0.5728 - val_loss: 1.4592 - val_acc: 0.5043\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2006 - acc: 0.5774 - val_loss: 1.4771 - val_acc: 0.5054\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1907 - acc: 0.5781 - val_loss: 1.4895 - val_acc: 0.5072\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1820 - acc: 0.5806 - val_loss: 1.4903 - val_acc: 0.5125\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1733 - acc: 0.5824 - val_loss: 1.4828 - val_acc: 0.5069\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1623 - acc: 0.5862 - val_loss: 1.5053 - val_acc: 0.5057\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1595 - acc: 0.5869 - val_loss: 1.4863 - val_acc: 0.5076\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1437 - acc: 0.5907 - val_loss: 1.5032 - val_acc: 0.5114\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1356 - acc: 0.5945 - val_loss: 1.5502 - val_acc: 0.5099\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1249 - acc: 0.5981 - val_loss: 1.5151 - val_acc: 0.5092\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1178 - acc: 0.6008 - val_loss: 1.5196 - val_acc: 0.5049\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1107 - acc: 0.6039 - val_loss: 1.5692 - val_acc: 0.5055\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1006 - acc: 0.6048 - val_loss: 1.5714 - val_acc: 0.5062\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0856 - acc: 0.6063 - val_loss: 1.6059 - val_acc: 0.5047\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0822 - acc: 0.6116 - val_loss: 1.5975 - val_acc: 0.5040\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0767 - acc: 0.6122 - val_loss: 1.5969 - val_acc: 0.5012\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0638 - acc: 0.6139 - val_loss: 1.5930 - val_acc: 0.5060\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0568 - acc: 0.6174 - val_loss: 1.5921 - val_acc: 0.5014\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0491 - acc: 0.6218 - val_loss: 1.6179 - val_acc: 0.5030\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0381 - acc: 0.6237 - val_loss: 1.6303 - val_acc: 0.5089\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0271 - acc: 0.6253 - val_loss: 1.7057 - val_acc: 0.4955\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0233 - acc: 0.6292 - val_loss: 1.6373 - val_acc: 0.4944\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0141 - acc: 0.6311 - val_loss: 1.6472 - val_acc: 0.4983\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0071 - acc: 0.6348 - val_loss: 1.6614 - val_acc: 0.5055\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9969 - acc: 0.6385 - val_loss: 1.7192 - val_acc: 0.5016\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9944 - acc: 0.6403 - val_loss: 1.7035 - val_acc: 0.4993\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9843 - acc: 0.6433 - val_loss: 1.6487 - val_acc: 0.5028\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9725 - acc: 0.6455 - val_loss: 1.7158 - val_acc: 0.4989\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9663 - acc: 0.6458 - val_loss: 1.7733 - val_acc: 0.5018\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9614 - acc: 0.6502 - val_loss: 1.7703 - val_acc: 0.5006\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9527 - acc: 0.6529 - val_loss: 1.7852 - val_acc: 0.5007\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9448 - acc: 0.6534 - val_loss: 1.7584 - val_acc: 0.4983\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9382 - acc: 0.6572 - val_loss: 1.7767 - val_acc: 0.4936\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9283 - acc: 0.6595 - val_loss: 1.7854 - val_acc: 0.4966\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9251 - acc: 0.6585 - val_loss: 1.7837 - val_acc: 0.4976\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9147 - acc: 0.6634 - val_loss: 1.7882 - val_acc: 0.4931\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9104 - acc: 0.6655 - val_loss: 1.8771 - val_acc: 0.4969\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9040 - acc: 0.6673 - val_loss: 1.8823 - val_acc: 0.4874\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8936 - acc: 0.6690 - val_loss: 1.8882 - val_acc: 0.4905\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8844 - acc: 0.6718 - val_loss: 1.8590 - val_acc: 0.4918\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8763 - acc: 0.6728 - val_loss: 1.9247 - val_acc: 0.4897\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8729 - acc: 0.6790 - val_loss: 1.9635 - val_acc: 0.4953\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8668 - acc: 0.6794 - val_loss: 1.9380 - val_acc: 0.4903\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8595 - acc: 0.6808 - val_loss: 1.9070 - val_acc: 0.4931\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8501 - acc: 0.6842 - val_loss: 1.8537 - val_acc: 0.4893\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8490 - acc: 0.6853 - val_loss: 1.9412 - val_acc: 0.4916\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8432 - acc: 0.6873 - val_loss: 1.9661 - val_acc: 0.4906\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8347 - acc: 0.6913 - val_loss: 2.0146 - val_acc: 0.4876\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8320 - acc: 0.6889 - val_loss: 2.0077 - val_acc: 0.4888\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8242 - acc: 0.6912 - val_loss: 2.0343 - val_acc: 0.4832\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8152 - acc: 0.6976 - val_loss: 1.9977 - val_acc: 0.4872\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8082 - acc: 0.6970 - val_loss: 2.0844 - val_acc: 0.4879\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8017 - acc: 0.7000 - val_loss: 2.2221 - val_acc: 0.4865\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.7973 - acc: 0.7007 - val_loss: 2.1016 - val_acc: 0.4928\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7948 - acc: 0.7044 - val_loss: 2.1564 - val_acc: 0.4886\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7824 - acc: 0.7071 - val_loss: 2.1635 - val_acc: 0.4904\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7823 - acc: 0.7091 - val_loss: 2.1534 - val_acc: 0.4920\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7756 - acc: 0.7096 - val_loss: 2.1199 - val_acc: 0.4845\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7706 - acc: 0.7121 - val_loss: 2.1327 - val_acc: 0.4882\n",
      "10000/10000 [==============================] - 2s 198us/step\n",
      "Validation loss: 2.1327066971957684\n",
      "Validation accuracy (NORMALIZED): 0.4882000065147877\n",
      "================================================\n",
      "Split: 2\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 8s 204us/step - loss: 2.3866 - acc: 0.1740 - val_loss: 2.0288 - val_acc: 0.2936\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 2.0886 - acc: 0.2505 - val_loss: 1.9260 - val_acc: 0.3513\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.9841 - acc: 0.3026 - val_loss: 1.7207 - val_acc: 0.4109\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.8368 - acc: 0.3482 - val_loss: 1.6068 - val_acc: 0.4506\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.7701 - acc: 0.3746 - val_loss: 1.5767 - val_acc: 0.4489\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.7224 - acc: 0.3895 - val_loss: 1.5402 - val_acc: 0.4668\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.6917 - acc: 0.4019 - val_loss: 1.5334 - val_acc: 0.4679\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.6630 - acc: 0.4128 - val_loss: 1.5217 - val_acc: 0.4680\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.6373 - acc: 0.4217 - val_loss: 1.5014 - val_acc: 0.4797\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.6123 - acc: 0.4382 - val_loss: 1.5011 - val_acc: 0.4827\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.5864 - acc: 0.4494 - val_loss: 1.4781 - val_acc: 0.4895\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.5678 - acc: 0.4534 - val_loss: 1.4760 - val_acc: 0.4868\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.5435 - acc: 0.4640 - val_loss: 1.4614 - val_acc: 0.4887\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.5267 - acc: 0.4694 - val_loss: 1.4567 - val_acc: 0.4875\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.5115 - acc: 0.4783 - val_loss: 1.4399 - val_acc: 0.5010\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4958 - acc: 0.4826 - val_loss: 1.4347 - val_acc: 0.5022\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4850 - acc: 0.4870 - val_loss: 1.4482 - val_acc: 0.4969\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4677 - acc: 0.4910 - val_loss: 1.4258 - val_acc: 0.5021\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4452 - acc: 0.4988 - val_loss: 1.4289 - val_acc: 0.5050\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4372 - acc: 0.5031 - val_loss: 1.4289 - val_acc: 0.5068\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4243 - acc: 0.5087 - val_loss: 1.4175 - val_acc: 0.5076\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4139 - acc: 0.5104 - val_loss: 1.4095 - val_acc: 0.5100\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3960 - acc: 0.5176 - val_loss: 1.4142 - val_acc: 0.5064\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3890 - acc: 0.5196 - val_loss: 1.4098 - val_acc: 0.5111\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3717 - acc: 0.5262 - val_loss: 1.4154 - val_acc: 0.5131\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3674 - acc: 0.5259 - val_loss: 1.4143 - val_acc: 0.5126\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3500 - acc: 0.5330 - val_loss: 1.4190 - val_acc: 0.5128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3389 - acc: 0.5340 - val_loss: 1.4270 - val_acc: 0.5124\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3254 - acc: 0.5378 - val_loss: 1.4135 - val_acc: 0.5121\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3155 - acc: 0.5413 - val_loss: 1.4140 - val_acc: 0.5165\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3039 - acc: 0.5456 - val_loss: 1.4176 - val_acc: 0.5174\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2974 - acc: 0.5466 - val_loss: 1.4132 - val_acc: 0.5151\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2859 - acc: 0.5520 - val_loss: 1.4390 - val_acc: 0.5125\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2729 - acc: 0.5549 - val_loss: 1.4183 - val_acc: 0.5155\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2640 - acc: 0.5581 - val_loss: 1.4229 - val_acc: 0.5127\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2533 - acc: 0.5615 - val_loss: 1.4416 - val_acc: 0.5130\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.2458 - acc: 0.5609 - val_loss: 1.4296 - val_acc: 0.5151\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.2300 - acc: 0.5678 - val_loss: 1.4276 - val_acc: 0.5137\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2215 - acc: 0.5704 - val_loss: 1.4417 - val_acc: 0.5144\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2113 - acc: 0.5722 - val_loss: 1.4501 - val_acc: 0.5141\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2087 - acc: 0.5753 - val_loss: 1.4652 - val_acc: 0.5069\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1986 - acc: 0.5758 - val_loss: 1.4490 - val_acc: 0.5150\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1811 - acc: 0.5806 - val_loss: 1.4592 - val_acc: 0.5122\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1776 - acc: 0.5837 - val_loss: 1.4567 - val_acc: 0.5072\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1614 - acc: 0.5874 - val_loss: 1.4732 - val_acc: 0.5071\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1541 - acc: 0.5924 - val_loss: 1.4860 - val_acc: 0.5144\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1435 - acc: 0.5956 - val_loss: 1.4806 - val_acc: 0.5115\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1404 - acc: 0.5959 - val_loss: 1.4824 - val_acc: 0.5145\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1231 - acc: 0.5972 - val_loss: 1.5050 - val_acc: 0.5110\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1150 - acc: 0.6009 - val_loss: 1.5597 - val_acc: 0.5057\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1119 - acc: 0.6025 - val_loss: 1.5081 - val_acc: 0.5073\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1006 - acc: 0.6048 - val_loss: 1.5367 - val_acc: 0.5115\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0901 - acc: 0.6074 - val_loss: 1.5546 - val_acc: 0.5057\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0818 - acc: 0.6088 - val_loss: 1.5360 - val_acc: 0.5090\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0697 - acc: 0.6132 - val_loss: 1.5653 - val_acc: 0.5078\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0623 - acc: 0.6166 - val_loss: 1.5699 - val_acc: 0.5088\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0549 - acc: 0.6200 - val_loss: 1.5650 - val_acc: 0.5075\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0442 - acc: 0.6230 - val_loss: 1.5903 - val_acc: 0.5040\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0367 - acc: 0.6245 - val_loss: 1.6057 - val_acc: 0.5086\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0269 - acc: 0.6274 - val_loss: 1.6239 - val_acc: 0.5013\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0167 - acc: 0.6300 - val_loss: 1.6207 - val_acc: 0.5048\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0070 - acc: 0.6350 - val_loss: 1.6414 - val_acc: 0.5020\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0027 - acc: 0.6358 - val_loss: 1.6594 - val_acc: 0.5047\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9914 - acc: 0.6409 - val_loss: 1.6389 - val_acc: 0.5070\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9854 - acc: 0.6411 - val_loss: 1.6740 - val_acc: 0.5060\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9748 - acc: 0.6432 - val_loss: 1.6720 - val_acc: 0.5016\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9660 - acc: 0.6454 - val_loss: 1.6908 - val_acc: 0.5054\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9555 - acc: 0.6514 - val_loss: 1.7283 - val_acc: 0.5023\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9461 - acc: 0.6530 - val_loss: 1.7202 - val_acc: 0.5030\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9356 - acc: 0.6585 - val_loss: 1.7713 - val_acc: 0.5040\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9325 - acc: 0.6592 - val_loss: 1.7717 - val_acc: 0.4998\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9241 - acc: 0.6626 - val_loss: 1.7763 - val_acc: 0.5005\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9180 - acc: 0.6626 - val_loss: 1.8037 - val_acc: 0.5048\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9084 - acc: 0.6643 - val_loss: 1.7740 - val_acc: 0.4982\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9028 - acc: 0.6678 - val_loss: 1.8077 - val_acc: 0.5040\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8939 - acc: 0.6696 - val_loss: 1.8057 - val_acc: 0.4982\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8847 - acc: 0.6714 - val_loss: 1.8059 - val_acc: 0.5007\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8771 - acc: 0.6778 - val_loss: 1.8387 - val_acc: 0.5003\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8711 - acc: 0.6771 - val_loss: 1.8841 - val_acc: 0.4993\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.8667 - acc: 0.6790 - val_loss: 1.8783 - val_acc: 0.4895\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8563 - acc: 0.6832 - val_loss: 1.9368 - val_acc: 0.4973\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8461 - acc: 0.6863 - val_loss: 1.8977 - val_acc: 0.4923\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8458 - acc: 0.6855 - val_loss: 1.9487 - val_acc: 0.4950\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.8353 - acc: 0.6885 - val_loss: 1.9450 - val_acc: 0.4969\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8326 - acc: 0.6929 - val_loss: 1.9634 - val_acc: 0.4929\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8213 - acc: 0.6942 - val_loss: 2.0166 - val_acc: 0.4966\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8145 - acc: 0.6959 - val_loss: 2.0113 - val_acc: 0.4986\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8076 - acc: 0.6986 - val_loss: 1.9833 - val_acc: 0.4878\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8053 - acc: 0.7020 - val_loss: 2.0670 - val_acc: 0.4923\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7983 - acc: 0.7023 - val_loss: 2.0581 - val_acc: 0.4966\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7906 - acc: 0.7057 - val_loss: 2.1196 - val_acc: 0.4966\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.7830 - acc: 0.7079 - val_loss: 2.1470 - val_acc: 0.4928\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.7762 - acc: 0.7078 - val_loss: 2.0106 - val_acc: 0.4901\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.7695 - acc: 0.7146 - val_loss: 2.1371 - val_acc: 0.4909\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 0.7656 - acc: 0.7130 - val_loss: 2.1757 - val_acc: 0.4895\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 7s 178us/step - loss: 0.7579 - acc: 0.7167 - val_loss: 2.1778 - val_acc: 0.4930\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 7s 184us/step - loss: 0.7479 - acc: 0.7188 - val_loss: 2.1679 - val_acc: 0.4939\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 7s 183us/step - loss: 0.7463 - acc: 0.7204 - val_loss: 2.2668 - val_acc: 0.4910\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 0.7383 - acc: 0.7224 - val_loss: 2.2475 - val_acc: 0.4914\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 0.7339 - acc: 0.7245 - val_loss: 2.2839 - val_acc: 0.4865\n",
      "10000/10000 [==============================] - 2s 215us/step\n",
      "Validation loss: 2.2839359016120433\n",
      "Validation accuracy (NORMALIZED): 0.4865000063031912\n",
      "================================================\n",
      "Split: 3\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 9s 228us/step - loss: 2.3529 - acc: 0.1860 - val_loss: 1.9272 - val_acc: 0.3197\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 1.9755 - acc: 0.2702 - val_loss: 1.7607 - val_acc: 0.3809\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.8790 - acc: 0.3171 - val_loss: 1.6702 - val_acc: 0.4125\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 7s 178us/step - loss: 1.8112 - acc: 0.3472 - val_loss: 1.6190 - val_acc: 0.4328\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 7s 186us/step - loss: 1.7659 - acc: 0.3683 - val_loss: 1.6015 - val_acc: 0.4486\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 1.7163 - acc: 0.3910 - val_loss: 1.5827 - val_acc: 0.4514\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 7s 184us/step - loss: 1.6804 - acc: 0.4052 - val_loss: 1.5396 - val_acc: 0.4707\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 7s 177us/step - loss: 1.6517 - acc: 0.4174 - val_loss: 1.5399 - val_acc: 0.4646\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.6281 - acc: 0.4295 - val_loss: 1.5094 - val_acc: 0.4703\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 7s 186us/step - loss: 1.6088 - acc: 0.4371 - val_loss: 1.4895 - val_acc: 0.4786\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 8s 189us/step - loss: 1.5868 - acc: 0.4435 - val_loss: 1.4836 - val_acc: 0.4852\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 8s 190us/step - loss: 1.5748 - acc: 0.4512 - val_loss: 1.4696 - val_acc: 0.4885\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 7s 179us/step - loss: 1.5500 - acc: 0.4631 - val_loss: 1.4695 - val_acc: 0.4934\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 7s 177us/step - loss: 1.5291 - acc: 0.4706 - val_loss: 1.4691 - val_acc: 0.4920\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 7s 178us/step - loss: 1.5148 - acc: 0.4732 - val_loss: 1.4503 - val_acc: 0.4947\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 1.4979 - acc: 0.4794 - val_loss: 1.4443 - val_acc: 0.4980\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4825 - acc: 0.4874 - val_loss: 1.4396 - val_acc: 0.5001\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4678 - acc: 0.4883 - val_loss: 1.4409 - val_acc: 0.5017\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4526 - acc: 0.4924 - val_loss: 1.4316 - val_acc: 0.5039\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4427 - acc: 0.4981 - val_loss: 1.4356 - val_acc: 0.5014\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4309 - acc: 0.5039 - val_loss: 1.4181 - val_acc: 0.5050\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4169 - acc: 0.5061 - val_loss: 1.4212 - val_acc: 0.5052\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4046 - acc: 0.5095 - val_loss: 1.4342 - val_acc: 0.4999\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.3935 - acc: 0.5136 - val_loss: 1.4212 - val_acc: 0.5073\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.3813 - acc: 0.5191 - val_loss: 1.4126 - val_acc: 0.5091\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.3700 - acc: 0.5210 - val_loss: 1.4139 - val_acc: 0.5076\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.3605 - acc: 0.5264 - val_loss: 1.4238 - val_acc: 0.5066\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.3485 - acc: 0.5280 - val_loss: 1.4156 - val_acc: 0.5112\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.3368 - acc: 0.5324 - val_loss: 1.4195 - val_acc: 0.5120\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 1.3273 - acc: 0.5363 - val_loss: 1.4057 - val_acc: 0.5098\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.3176 - acc: 0.5384 - val_loss: 1.4298 - val_acc: 0.5125\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 7s 180us/step - loss: 1.3099 - acc: 0.5390 - val_loss: 1.4064 - val_acc: 0.5133\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 1.2953 - acc: 0.5481 - val_loss: 1.4055 - val_acc: 0.5124\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 8s 188us/step - loss: 1.2871 - acc: 0.5476 - val_loss: 1.4148 - val_acc: 0.5160\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.2788 - acc: 0.5504 - val_loss: 1.4345 - val_acc: 0.5154\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 8s 200us/step - loss: 1.2698 - acc: 0.5539 - val_loss: 1.4173 - val_acc: 0.5150\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 1.2582 - acc: 0.5569 - val_loss: 1.4331 - val_acc: 0.5133\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 1.2464 - acc: 0.5572 - val_loss: 1.4372 - val_acc: 0.5146\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 1.2416 - acc: 0.5629 - val_loss: 1.4323 - val_acc: 0.5141\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 8s 188us/step - loss: 1.2283 - acc: 0.5646 - val_loss: 1.4568 - val_acc: 0.5168\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 1.2202 - acc: 0.5669 - val_loss: 1.4357 - val_acc: 0.5158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 1.2094 - acc: 0.5692 - val_loss: 1.4554 - val_acc: 0.5193\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 8s 195us/step - loss: 1.2061 - acc: 0.5729 - val_loss: 1.4357 - val_acc: 0.5187\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 8s 198us/step - loss: 1.1922 - acc: 0.5742 - val_loss: 1.4315 - val_acc: 0.5132\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 1.1829 - acc: 0.5762 - val_loss: 1.4692 - val_acc: 0.5138\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 8s 206us/step - loss: 1.1725 - acc: 0.5817 - val_loss: 1.4619 - val_acc: 0.5203\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 8s 201us/step - loss: 1.1609 - acc: 0.5858 - val_loss: 1.4681 - val_acc: 0.5166\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 8s 205us/step - loss: 1.1561 - acc: 0.5873 - val_loss: 1.4614 - val_acc: 0.5166\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 8s 195us/step - loss: 1.1431 - acc: 0.5891 - val_loss: 1.4860 - val_acc: 0.5149\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 8s 205us/step - loss: 1.1380 - acc: 0.5920 - val_loss: 1.4692 - val_acc: 0.5153\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 8s 212us/step - loss: 1.1258 - acc: 0.5949 - val_loss: 1.5169 - val_acc: 0.5156\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 8s 202us/step - loss: 1.1156 - acc: 0.5998 - val_loss: 1.4984 - val_acc: 0.5100\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 8s 199us/step - loss: 1.1104 - acc: 0.6006 - val_loss: 1.5043 - val_acc: 0.5190\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 8s 198us/step - loss: 1.0995 - acc: 0.6026 - val_loss: 1.5012 - val_acc: 0.5106\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 8s 201us/step - loss: 1.0922 - acc: 0.6064 - val_loss: 1.5023 - val_acc: 0.5168\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 8s 208us/step - loss: 1.0833 - acc: 0.6087 - val_loss: 1.5153 - val_acc: 0.5110\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 8s 203us/step - loss: 1.0729 - acc: 0.6114 - val_loss: 1.5378 - val_acc: 0.5166\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 8s 194us/step - loss: 1.0642 - acc: 0.6145 - val_loss: 1.5367 - val_acc: 0.5148\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.0581 - acc: 0.6174 - val_loss: 1.5799 - val_acc: 0.5186\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.0485 - acc: 0.6191 - val_loss: 1.5806 - val_acc: 0.5121\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 1.0377 - acc: 0.6232 - val_loss: 1.5914 - val_acc: 0.5089\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 7s 178us/step - loss: 1.0321 - acc: 0.6226 - val_loss: 1.5885 - val_acc: 0.5123\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 9s 214us/step - loss: 1.0201 - acc: 0.6285 - val_loss: 1.6585 - val_acc: 0.5161\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 8s 201us/step - loss: 1.0141 - acc: 0.6306 - val_loss: 1.6128 - val_acc: 0.5103\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 8s 188us/step - loss: 1.0055 - acc: 0.6316 - val_loss: 1.6024 - val_acc: 0.5094\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 0.9947 - acc: 0.6373 - val_loss: 1.5848 - val_acc: 0.5074\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 8s 199us/step - loss: 0.9881 - acc: 0.6412 - val_loss: 1.6988 - val_acc: 0.5075\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 9s 216us/step - loss: 0.9817 - acc: 0.6395 - val_loss: 1.6712 - val_acc: 0.5131\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 9s 215us/step - loss: 0.9724 - acc: 0.6420 - val_loss: 1.6357 - val_acc: 0.5053\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 8s 189us/step - loss: 0.9631 - acc: 0.6467 - val_loss: 1.6859 - val_acc: 0.5098\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 7s 186us/step - loss: 0.9554 - acc: 0.6502 - val_loss: 1.7286 - val_acc: 0.5095\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 7s 186us/step - loss: 0.9506 - acc: 0.6526 - val_loss: 1.7368 - val_acc: 0.5112\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 0.9428 - acc: 0.6538 - val_loss: 1.7235 - val_acc: 0.5054\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 0.9322 - acc: 0.6587 - val_loss: 1.7317 - val_acc: 0.5046\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 0.9242 - acc: 0.6606 - val_loss: 1.7545 - val_acc: 0.5042\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 7s 186us/step - loss: 0.9183 - acc: 0.6619 - val_loss: 1.7056 - val_acc: 0.5029\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 0.9087 - acc: 0.6652 - val_loss: 1.7912 - val_acc: 0.5058\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 8s 195us/step - loss: 0.9050 - acc: 0.6647 - val_loss: 1.7501 - val_acc: 0.5036\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 0.8972 - acc: 0.6684 - val_loss: 1.8114 - val_acc: 0.5000\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 0.8886 - acc: 0.6707 - val_loss: 1.8482 - val_acc: 0.4980\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 8s 194us/step - loss: 0.8797 - acc: 0.6768 - val_loss: 1.8619 - val_acc: 0.5032\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 0.8738 - acc: 0.6761 - val_loss: 1.8422 - val_acc: 0.5030\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 8s 202us/step - loss: 0.8669 - acc: 0.6812 - val_loss: 1.8756 - val_acc: 0.4997\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 8s 191us/step - loss: 0.8622 - acc: 0.6817 - val_loss: 1.8827 - val_acc: 0.4994\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 8s 206us/step - loss: 0.8494 - acc: 0.6840 - val_loss: 1.9178 - val_acc: 0.4999\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 9s 214us/step - loss: 0.8463 - acc: 0.6866 - val_loss: 1.9274 - val_acc: 0.4983\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 8s 203us/step - loss: 0.8405 - acc: 0.6874 - val_loss: 1.9594 - val_acc: 0.5034\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 0.8328 - acc: 0.6903 - val_loss: 1.9582 - val_acc: 0.4990\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 8s 190us/step - loss: 0.8311 - acc: 0.6910 - val_loss: 2.0288 - val_acc: 0.5023\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 8s 204us/step - loss: 0.8234 - acc: 0.6935 - val_loss: 1.9166 - val_acc: 0.4987\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 8s 207us/step - loss: 0.8124 - acc: 0.6958 - val_loss: 1.9811 - val_acc: 0.4994\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 8s 201us/step - loss: 0.8081 - acc: 0.6967 - val_loss: 2.0445 - val_acc: 0.4997\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 8s 198us/step - loss: 0.8022 - acc: 0.7017 - val_loss: 1.9681 - val_acc: 0.4972\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 0.7946 - acc: 0.7032 - val_loss: 1.9683 - val_acc: 0.4959\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 8s 189us/step - loss: 0.7899 - acc: 0.7056 - val_loss: 2.0146 - val_acc: 0.5001\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 7s 186us/step - loss: 0.7820 - acc: 0.7082 - val_loss: 2.0470 - val_acc: 0.4995\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 0.7757 - acc: 0.7102 - val_loss: 2.0688 - val_acc: 0.4930\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 0.7730 - acc: 0.7107 - val_loss: 2.0977 - val_acc: 0.4929\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 8s 191us/step - loss: 0.7658 - acc: 0.7139 - val_loss: 2.0685 - val_acc: 0.4928\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 0.7589 - acc: 0.7155 - val_loss: 2.1058 - val_acc: 0.4877\n",
      "10000/10000 [==============================] - 2s 209us/step\n",
      "Validation loss: 2.105846211463213\n",
      "Validation accuracy (NORMALIZED): 0.4877000060528517\n",
      "================================================\n",
      "Split: 4\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 9s 224us/step - loss: 2.4177 - acc: 0.1462 - val_loss: 2.1157 - val_acc: 0.2327\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 2.1487 - acc: 0.1877 - val_loss: 2.0329 - val_acc: 0.2609\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 8s 193us/step - loss: 2.0846 - acc: 0.2219 - val_loss: 1.9043 - val_acc: 0.3354\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 8s 188us/step - loss: 1.9888 - acc: 0.2789 - val_loss: 1.7964 - val_acc: 0.4013\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.9141 - acc: 0.2983 - val_loss: 1.7189 - val_acc: 0.4103\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.8584 - acc: 0.3129 - val_loss: 1.6476 - val_acc: 0.4313\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.8136 - acc: 0.3320 - val_loss: 1.6300 - val_acc: 0.4439\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.7833 - acc: 0.3499 - val_loss: 1.6190 - val_acc: 0.4452\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.7455 - acc: 0.3735 - val_loss: 1.5722 - val_acc: 0.4548\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.7108 - acc: 0.3898 - val_loss: 1.5466 - val_acc: 0.4588\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.6837 - acc: 0.4027 - val_loss: 1.5348 - val_acc: 0.4640\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.6473 - acc: 0.4235 - val_loss: 1.5109 - val_acc: 0.4741\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 7s 181us/step - loss: 1.6222 - acc: 0.4325 - val_loss: 1.5088 - val_acc: 0.4749\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.5916 - acc: 0.4436 - val_loss: 1.4976 - val_acc: 0.4812\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.5729 - acc: 0.4520 - val_loss: 1.4720 - val_acc: 0.4878\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.5499 - acc: 0.4620 - val_loss: 1.4698 - val_acc: 0.4882\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.5335 - acc: 0.4657 - val_loss: 1.4640 - val_acc: 0.4928\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.5205 - acc: 0.4723 - val_loss: 1.4602 - val_acc: 0.4971\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.5076 - acc: 0.4754 - val_loss: 1.4532 - val_acc: 0.4926\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.4902 - acc: 0.4848 - val_loss: 1.4363 - val_acc: 0.4980\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.4732 - acc: 0.4900 - val_loss: 1.4318 - val_acc: 0.5014\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.4597 - acc: 0.4933 - val_loss: 1.4411 - val_acc: 0.5030\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.4493 - acc: 0.4991 - val_loss: 1.4313 - val_acc: 0.5066\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.4349 - acc: 0.5022 - val_loss: 1.4244 - val_acc: 0.5097\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.4190 - acc: 0.5085 - val_loss: 1.4291 - val_acc: 0.5052\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.4060 - acc: 0.5140 - val_loss: 1.4262 - val_acc: 0.5059\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3935 - acc: 0.5168 - val_loss: 1.4254 - val_acc: 0.5099\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3800 - acc: 0.5200 - val_loss: 1.4276 - val_acc: 0.5043\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3672 - acc: 0.5257 - val_loss: 1.4328 - val_acc: 0.5056\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.3558 - acc: 0.5293 - val_loss: 1.4263 - val_acc: 0.5113\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.3430 - acc: 0.5314 - val_loss: 1.4088 - val_acc: 0.5161\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3315 - acc: 0.5388 - val_loss: 1.4200 - val_acc: 0.5051\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.3210 - acc: 0.5387 - val_loss: 1.4169 - val_acc: 0.5143\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.3142 - acc: 0.5430 - val_loss: 1.4399 - val_acc: 0.5145\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3041 - acc: 0.5455 - val_loss: 1.4213 - val_acc: 0.5131\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2875 - acc: 0.5488 - val_loss: 1.4267 - val_acc: 0.5112\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2812 - acc: 0.5511 - val_loss: 1.4328 - val_acc: 0.5131\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2708 - acc: 0.5558 - val_loss: 1.4329 - val_acc: 0.5140\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.2602 - acc: 0.5559 - val_loss: 1.4356 - val_acc: 0.5140\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2471 - acc: 0.5624 - val_loss: 1.4376 - val_acc: 0.5136\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2384 - acc: 0.5649 - val_loss: 1.4424 - val_acc: 0.5128\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.2287 - acc: 0.5674 - val_loss: 1.4554 - val_acc: 0.5135\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2200 - acc: 0.5696 - val_loss: 1.4660 - val_acc: 0.5149\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2115 - acc: 0.5712 - val_loss: 1.4732 - val_acc: 0.5167\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.1963 - acc: 0.5767 - val_loss: 1.4543 - val_acc: 0.5144\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.1824 - acc: 0.5793 - val_loss: 1.4858 - val_acc: 0.5120\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.1741 - acc: 0.5829 - val_loss: 1.4877 - val_acc: 0.5115\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.1672 - acc: 0.5866 - val_loss: 1.4735 - val_acc: 0.5128\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.1556 - acc: 0.5916 - val_loss: 1.4813 - val_acc: 0.5127\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.1447 - acc: 0.5909 - val_loss: 1.5145 - val_acc: 0.5131\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.1389 - acc: 0.5949 - val_loss: 1.5031 - val_acc: 0.5110\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.1284 - acc: 0.5963 - val_loss: 1.5259 - val_acc: 0.5049\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.1182 - acc: 0.6010 - val_loss: 1.5135 - val_acc: 0.5077\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.1067 - acc: 0.6037 - val_loss: 1.5498 - val_acc: 0.5097\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0982 - acc: 0.6048 - val_loss: 1.5396 - val_acc: 0.5090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0883 - acc: 0.6111 - val_loss: 1.5601 - val_acc: 0.5082\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.0801 - acc: 0.6126 - val_loss: 1.5351 - val_acc: 0.5076\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0712 - acc: 0.6147 - val_loss: 1.5663 - val_acc: 0.5062\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0613 - acc: 0.6176 - val_loss: 1.5740 - val_acc: 0.5056\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0540 - acc: 0.6221 - val_loss: 1.6195 - val_acc: 0.5032\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0446 - acc: 0.6217 - val_loss: 1.6082 - val_acc: 0.5057\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0356 - acc: 0.6272 - val_loss: 1.6135 - val_acc: 0.5043\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0295 - acc: 0.6279 - val_loss: 1.6600 - val_acc: 0.5010\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0183 - acc: 0.6317 - val_loss: 1.6285 - val_acc: 0.5020\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0108 - acc: 0.6363 - val_loss: 1.6485 - val_acc: 0.5002\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0046 - acc: 0.6356 - val_loss: 1.6371 - val_acc: 0.5022\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9924 - acc: 0.6410 - val_loss: 1.6774 - val_acc: 0.5005\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.9865 - acc: 0.6414 - val_loss: 1.6930 - val_acc: 0.5004\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9762 - acc: 0.6446 - val_loss: 1.6927 - val_acc: 0.4985\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.9710 - acc: 0.6461 - val_loss: 1.7095 - val_acc: 0.5029\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.9639 - acc: 0.6494 - val_loss: 1.7678 - val_acc: 0.5040\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9509 - acc: 0.6540 - val_loss: 1.7945 - val_acc: 0.5013\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9435 - acc: 0.6562 - val_loss: 1.7360 - val_acc: 0.5028\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 0.9313 - acc: 0.6604 - val_loss: 1.7778 - val_acc: 0.5019\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9253 - acc: 0.6615 - val_loss: 1.8110 - val_acc: 0.4996\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9232 - acc: 0.6626 - val_loss: 1.8189 - val_acc: 0.4948\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9128 - acc: 0.6665 - val_loss: 1.8200 - val_acc: 0.4991\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9043 - acc: 0.6673 - val_loss: 1.8216 - val_acc: 0.5014\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8981 - acc: 0.6699 - val_loss: 1.9099 - val_acc: 0.4980\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.8919 - acc: 0.6718 - val_loss: 1.8340 - val_acc: 0.4929\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8794 - acc: 0.6759 - val_loss: 1.9109 - val_acc: 0.4969\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8731 - acc: 0.6767 - val_loss: 1.8951 - val_acc: 0.4973\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8672 - acc: 0.6810 - val_loss: 1.9463 - val_acc: 0.4992\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8581 - acc: 0.6855 - val_loss: 1.8981 - val_acc: 0.4934\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8534 - acc: 0.6865 - val_loss: 1.9568 - val_acc: 0.4941\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8412 - acc: 0.6897 - val_loss: 1.8982 - val_acc: 0.4953\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8361 - acc: 0.6917 - val_loss: 1.9530 - val_acc: 0.4925\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8328 - acc: 0.6922 - val_loss: 1.9519 - val_acc: 0.4910\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8212 - acc: 0.6962 - val_loss: 1.9915 - val_acc: 0.4871\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.8161 - acc: 0.6966 - val_loss: 1.9996 - val_acc: 0.4919\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8162 - acc: 0.6991 - val_loss: 1.9972 - val_acc: 0.4887\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8014 - acc: 0.7038 - val_loss: 2.0173 - val_acc: 0.4875\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 0.7938 - acc: 0.7071 - val_loss: 2.1120 - val_acc: 0.4938\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 0.7948 - acc: 0.7055 - val_loss: 2.0347 - val_acc: 0.4855\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.7849 - acc: 0.7092 - val_loss: 2.0678 - val_acc: 0.4875\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.7764 - acc: 0.7101 - val_loss: 2.1066 - val_acc: 0.4934\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.7723 - acc: 0.7145 - val_loss: 2.1096 - val_acc: 0.4922\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.7624 - acc: 0.7167 - val_loss: 2.1478 - val_acc: 0.4857\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.7623 - acc: 0.7155 - val_loss: 2.1506 - val_acc: 0.4834\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.7561 - acc: 0.7175 - val_loss: 2.1762 - val_acc: 0.4856\n",
      "10000/10000 [==============================] - 2s 204us/step\n",
      "Validation loss: 2.1761686961352824\n",
      "Validation accuracy (NORMALIZED): 0.48560000701248646\n",
      "[[2.0585452838242055, 0.49370000678300857], [2.1327066971957684, 0.4882000065147877], [2.2839359016120433, 0.4865000063031912], [2.105846211463213, 0.4877000060528517], [2.1761686961352824, 0.48560000701248646]]\n"
     ]
    }
   ],
   "source": [
    "#=====================================\n",
    "# Training model and Evaluation\n",
    "#=====================================\n",
    "\n",
    "y_train_categorical = to_categorical(y_train, num_classes=n_classes)\n",
    "y_val_categorical = to_categorical(y_val, num_classes=n_classes)\n",
    "\n",
    "scores = []\n",
    "for i in range(folds):\n",
    "    #--- Loading model\n",
    "    model = get_squeezenet_ft2()    \n",
    "    \n",
    "    #--- Evaluating the model for split i\n",
    "    print(\"================================================\")\n",
    "    print(\"Split: \"+str(i))\n",
    "    print(\"Number of Epochs: \"+str(n_epochs))\n",
    "    print(\"Train shape:\",X_train[i].shape)\n",
    "    print(\"Train batch size: \"+str(train_batch_size))\n",
    "    print(\"Val batch size: \"+str(val_batch_size))\n",
    "    print(\"================================================\")\n",
    "    \n",
    "    # Training with data augmentation\n",
    "    aug.fit(X_train[i])\n",
    "    model.fit_generator(aug.flow(X_train[i],y_train_categorical[i], batch_size=train_batch_size),\n",
    "                        steps_per_epoch=X_train.shape[1]//train_batch_size,\n",
    "                        epochs=n_epochs, \n",
    "                        verbose=1)\n",
    "    \n",
    "    #--- Evaluating the model for split i\n",
    "    score = model.evaluate(x=X_val[i], y=y_val_categorical[i], batch_size=val_batch_size, verbose=1)\n",
    "    scores.append(score)\n",
    "    print('Validation loss:', score[0])\n",
    "    print('Validation accuracy (NORMALIZED):', score[1])\n",
    "    \n",
    "#--- Showing scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Validation loss: 2.1649977488458156\n",
      "Mean Validation accuracy (NORMALIZED): 0.4909800066113473\n"
     ]
    }
   ],
   "source": [
    "#=====================================\n",
    "# Evaluate on validation\n",
    "#=====================================\n",
    "\n",
    "#--- The evaluation of the model \n",
    "np_aux = np.array(scores).mean(axis=0)\n",
    "\n",
    "print('Mean Validation loss:', np_aux[0])\n",
    "print('Mean Validation accuracy (NORMALIZED):', np_aux[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----------\n",
    "-----------\n",
    "# Tensorboard\n",
    "\n",
    "Tensorboard is a visualization tool for Tensorflow. Among other things, it allows us to monitor the progress of our training, plot metrics per epochs, visualize the architecture's schematics. \n",
    "\n",
    "Just like for Early Stopping, we will use the [Tensorboard callback](https://keras.io/callbacks/#tensorboard) to log the information about our training. An example of usage, would be:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Just an example, DON'T RUN! \n",
    "### You will need to change <<LOG_DIR>>\n",
    "import keras.callbacks as callbacks\n",
    "tbCallBack = callbacks.TensorBoard(log_dir = \"./<<LOG_DIR>>\")\n",
    "model.fit(..., callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your training progresses, Keras will log the metrics (e.g., loss, accuracy) to `<<LOG_DIR>>` (**make sure `<<LOG_DIR>>` is a valid directory)**. On your terminal, you will need to run Tensorboard, assign a port and access it via browser (just like jupyter).\n",
    "\n",
    "#### ----> MAKE SURE YOU USE A DIFFERENT PORT FOR JUPYTER AND TENSORBOARD <----\n",
    "\n",
    "### Docker\n",
    "For those using docker, open a new terminal and create a new container (using the same image) running Tensorboard:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ docker run -it -p <<port_host>>:<<port_container>>\n",
    "            --volume=<<LOG_DIR>>:<<LOG_DIR>>\n",
    "            --name=<<container_name>> <<docker_image>> \n",
    "            tensorboard --logdir=<<LOG_DIR>> --port=<<port_container>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ docker run -it -p 8887:8887\n",
    "            --volume=/your/path/ml2018/:/ml2018\n",
    "            --name=mdc_container_tensorboard mdc-keras:cpu\n",
    "            tensorboard --logdir=/ml2018/logs --port=8887"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting Tensorboard, access it via browser on `http://localhost:<<port_container>>`.\n",
    "\n",
    "### Anaconda\n",
    "$ tensorboard --logdir=<<LOG_DIR>> --port=<<port>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting Tensorboard, access it via browser on `http://localhost:<<port>>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----------\n",
    "-----------\n",
    "\n",
    "# Fine-tuning all layers\n",
    "\n",
    "What if we fine-tune all layers of SqueezeNet?\n",
    "<img src=\"unfrozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Compiling model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 15, 15, 64)   1792        input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv1 (Activation)         (None, 15, 15, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 7, 7, 64)     0           relu_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fire2/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     1040        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire2/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire2/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire2/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/concat (Concatenate)      (None, 7, 7, 128)    0           fire2/relu_expand1x1[0][0]       \n",
      "                                                                 fire2/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire3/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     2064        fire2/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire3/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire3/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire3/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/concat (Concatenate)      (None, 7, 7, 128)    0           fire3/relu_expand1x1[0][0]       \n",
      "                                                                 fire3/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 3, 3, 128)    0           fire3/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire4/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     4128        pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire4/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire4/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire4/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/concat (Concatenate)      (None, 3, 3, 256)    0           fire4/relu_expand1x1[0][0]       \n",
      "                                                                 fire4/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire5/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     8224        fire4/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire5/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire5/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire5/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/concat (Concatenate)      (None, 3, 3, 256)    0           fire5/relu_expand1x1[0][0]       \n",
      "                                                                 fire5/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 1, 1, 256)    0           fire5/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire6/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     12336       pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire6/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire6/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire6/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/concat (Concatenate)      (None, 1, 1, 384)    0           fire6/relu_expand1x1[0][0]       \n",
      "                                                                 fire6/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire7/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     18480       fire6/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire7/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire7/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire7/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/concat (Concatenate)      (None, 1, 1, 384)    0           fire7/relu_expand1x1[0][0]       \n",
      "                                                                 fire7/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire8/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     24640       fire7/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire8/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire8/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire8/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/concat (Concatenate)      (None, 1, 1, 512)    0           fire8/relu_expand1x1[0][0]       \n",
      "                                                                 fire8/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire9/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     32832       fire8/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire9/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire9/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire9/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/concat (Concatenate)      (None, 1, 1, 512)    0           fire9/relu_expand1x1[0][0]       \n",
      "                                                                 fire9/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "drop9 (Dropout)                 (None, 1, 1, 512)    0           fire9/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv10 (Conv2D)                 (None, 1, 1, 10)     5130        drop9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv10 (Activation)        (None, 1, 1, 10)     0           conv10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_30 (Gl (None, 10)           0           relu_conv10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "loss (Activation)               (None, 10)           0           global_average_pooling2d_30[0][0]\n",
      "==================================================================================================\n",
      "Total params: 727,626\n",
      "Trainable params: 727,626\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_squeezenet_ft3():\n",
    "    squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "    for layer in squeezeNetModel.layers:\n",
    "        layer.trainable = True       #by default they are all trainable, but just for clarification\n",
    "\n",
    "    #=====================================\n",
    "    # Add new classification layers\n",
    "    #=====================================\n",
    "\n",
    "    #--- Removing layers until drop 9\n",
    "    squeezeNetModel.layers.pop() #Convolution2D\n",
    "    squeezeNetModel.layers.pop() #Activation ReLU\n",
    "    squeezeNetModel.layers.pop() #Global Avg Pool\n",
    "    squeezeNetModel.layers.pop() #Activation Softmax\n",
    "\n",
    "    #--- Adding classification layer for 10 classes\n",
    "    out = Convolution2D(n_classes, (1, 1), padding='valid', name='conv10')(squeezeNetModel.layers[-1].output)\n",
    "    out = Activation('relu', name='relu_conv10')(out)\n",
    "\n",
    "    out = GlobalAveragePooling2D()(out)\n",
    "    out = Activation('softmax', name='loss')(out)\n",
    "\n",
    "    #=====================================\n",
    "    # New Model\n",
    "    #=====================================\n",
    "    model = Model(squeezeNetModel.inputs, out, name='squeezenet_new')\n",
    "    opt = optimizer=optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)#SGD(lr=learning_rate)\n",
    "    print(\"--- Compiling model\")\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_squeezenet_ft3()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing tensorboard\n",
      "Log Dir:  logs/1542642629.2368855\n",
      "--- Start training\n",
      "--- Compiling model\n",
      "================================================\n",
      "Split: 0\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 16s 392us/step - loss: 2.2826 - acc: 0.1584 - val_loss: 2.0243 - val_acc: 0.3246\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 15s 367us/step - loss: 1.8054 - acc: 0.3678 - val_loss: 1.4007 - val_acc: 0.5291\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 1.4157 - acc: 0.5390 - val_loss: 1.1710 - val_acc: 0.6234\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 14s 361us/step - loss: 1.1942 - acc: 0.6240 - val_loss: 1.0712 - val_acc: 0.6540\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 1.0527 - acc: 0.6676 - val_loss: 1.0020 - val_acc: 0.6721\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 14s 358us/step - loss: 0.9525 - acc: 0.7007 - val_loss: 0.9535 - val_acc: 0.6929\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.8725 - acc: 0.7213 - val_loss: 0.9082 - val_acc: 0.7036\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 15s 365us/step - loss: 0.8113 - acc: 0.7412 - val_loss: 0.8879 - val_acc: 0.7110\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 14s 350us/step - loss: 0.7575 - acc: 0.7553 - val_loss: 0.8574 - val_acc: 0.7233\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 14s 341us/step - loss: 0.7107 - acc: 0.7684 - val_loss: 0.8761 - val_acc: 0.7185\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 14s 341us/step - loss: 0.6693 - acc: 0.7809 - val_loss: 0.8877 - val_acc: 0.7206\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 14s 340us/step - loss: 0.6235 - acc: 0.7955 - val_loss: 0.8457 - val_acc: 0.7346\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.5883 - acc: 0.8086 - val_loss: 0.9110 - val_acc: 0.7283\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 14s 340us/step - loss: 0.5578 - acc: 0.8156 - val_loss: 0.9102 - val_acc: 0.7318\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 14s 341us/step - loss: 0.5236 - acc: 0.8270 - val_loss: 0.8758 - val_acc: 0.7426\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.4958 - acc: 0.8378 - val_loss: 0.8914 - val_acc: 0.7346\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.4624 - acc: 0.8467 - val_loss: 0.9346 - val_acc: 0.7374\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.4343 - acc: 0.8544 - val_loss: 0.9551 - val_acc: 0.7481\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 14s 361us/step - loss: 0.4115 - acc: 0.8636 - val_loss: 0.9922 - val_acc: 0.7402\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.3852 - acc: 0.8720 - val_loss: 1.0606 - val_acc: 0.7383\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.3637 - acc: 0.8794 - val_loss: 0.9857 - val_acc: 0.7443\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.3482 - acc: 0.8835 - val_loss: 1.0805 - val_acc: 0.7442\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.3279 - acc: 0.8910 - val_loss: 1.1101 - val_acc: 0.7421\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 14s 361us/step - loss: 0.3054 - acc: 0.8975 - val_loss: 1.1249 - val_acc: 0.7385\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 14s 360us/step - loss: 0.2949 - acc: 0.9018 - val_loss: 1.1642 - val_acc: 0.7404\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 14s 359us/step - loss: 0.2735 - acc: 0.9090 - val_loss: 1.2287 - val_acc: 0.7427\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.2606 - acc: 0.9132 - val_loss: 1.2378 - val_acc: 0.7370\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.2474 - acc: 0.9182 - val_loss: 1.3227 - val_acc: 0.7421\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.2403 - acc: 0.9199 - val_loss: 1.2490 - val_acc: 0.7369\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.2259 - acc: 0.9250 - val_loss: 1.2281 - val_acc: 0.7392\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.2227 - acc: 0.9277 - val_loss: 1.3771 - val_acc: 0.7352\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.2061 - acc: 0.9327 - val_loss: 1.3755 - val_acc: 0.7352\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.1908 - acc: 0.9373 - val_loss: 1.4438 - val_acc: 0.7323\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.1937 - acc: 0.9371 - val_loss: 1.4769 - val_acc: 0.7387\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1797 - acc: 0.9418 - val_loss: 1.4162 - val_acc: 0.7411\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.1751 - acc: 0.9436 - val_loss: 1.3741 - val_acc: 0.7353\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1638 - acc: 0.9468 - val_loss: 1.5546 - val_acc: 0.7373\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 15s 363us/step - loss: 0.1578 - acc: 0.9492 - val_loss: 1.4818 - val_acc: 0.7346\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 15s 373us/step - loss: 0.1570 - acc: 0.9488 - val_loss: 1.4609 - val_acc: 0.7411\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 16s 393us/step - loss: 0.1474 - acc: 0.9536 - val_loss: 1.4811 - val_acc: 0.7412\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 15s 384us/step - loss: 0.1520 - acc: 0.9507 - val_loss: 1.4596 - val_acc: 0.7412\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 15s 379us/step - loss: 0.1417 - acc: 0.9554 - val_loss: 1.4938 - val_acc: 0.7332\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 15s 380us/step - loss: 0.1351 - acc: 0.9570 - val_loss: 1.5210 - val_acc: 0.7369\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.1330 - acc: 0.9578 - val_loss: 1.6520 - val_acc: 0.7322\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.1308 - acc: 0.9585 - val_loss: 1.6766 - val_acc: 0.7328\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 14s 360us/step - loss: 0.1217 - acc: 0.9607 - val_loss: 1.6608 - val_acc: 0.7319\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 14s 360us/step - loss: 0.1225 - acc: 0.9618 - val_loss: 1.5466 - val_acc: 0.7347\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1161 - acc: 0.9642 - val_loss: 1.6969 - val_acc: 0.7355\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1219 - acc: 0.9613 - val_loss: 1.5421 - val_acc: 0.7328\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1020 - acc: 0.9672 - val_loss: 1.6831 - val_acc: 0.7324\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.1167 - acc: 0.9626 - val_loss: 1.6704 - val_acc: 0.7289\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.1069 - acc: 0.9661 - val_loss: 1.5403 - val_acc: 0.7338\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.0943 - acc: 0.9710 - val_loss: 1.6773 - val_acc: 0.7226\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1086 - acc: 0.9656 - val_loss: 1.6911 - val_acc: 0.7399\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1064 - acc: 0.9667 - val_loss: 1.6962 - val_acc: 0.7344\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 14s 351us/step - loss: 0.1008 - acc: 0.9690 - val_loss: 1.6785 - val_acc: 0.7424\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 14s 350us/step - loss: 0.0892 - acc: 0.9716 - val_loss: 1.6539 - val_acc: 0.7356\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0945 - acc: 0.9693 - val_loss: 1.7892 - val_acc: 0.7382\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 14s 350us/step - loss: 0.0886 - acc: 0.9727 - val_loss: 1.8203 - val_acc: 0.7388\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0907 - acc: 0.9714 - val_loss: 1.7188 - val_acc: 0.7402\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 14s 350us/step - loss: 0.0856 - acc: 0.9737 - val_loss: 1.7729 - val_acc: 0.7461\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0785 - acc: 0.9759 - val_loss: 1.7610 - val_acc: 0.7385\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 14s 351us/step - loss: 0.0858 - acc: 0.9733 - val_loss: 1.7213 - val_acc: 0.7416\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 14s 351us/step - loss: 0.0903 - acc: 0.9722 - val_loss: 1.7564 - val_acc: 0.7403\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 14s 350us/step - loss: 0.0847 - acc: 0.9744 - val_loss: 1.7787 - val_acc: 0.7382\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.0822 - acc: 0.9748 - val_loss: 1.7931 - val_acc: 0.7335\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 14s 359us/step - loss: 0.0777 - acc: 0.9760 - val_loss: 1.7032 - val_acc: 0.7383\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 15s 379us/step - loss: 0.0795 - acc: 0.9754 - val_loss: 1.6464 - val_acc: 0.7369\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.0809 - acc: 0.9748 - val_loss: 1.7775 - val_acc: 0.7368\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.0723 - acc: 0.9775 - val_loss: 1.7275 - val_acc: 0.7306\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 14s 358us/step - loss: 0.0779 - acc: 0.9755 - val_loss: 1.7946 - val_acc: 0.7388\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 15s 364us/step - loss: 0.0735 - acc: 0.9770 - val_loss: 1.7981 - val_acc: 0.7391\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 15s 378us/step - loss: 0.0680 - acc: 0.9792 - val_loss: 1.7991 - val_acc: 0.7357\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 15s 372us/step - loss: 0.0736 - acc: 0.9774 - val_loss: 1.8401 - val_acc: 0.7438\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.0686 - acc: 0.9786 - val_loss: 1.9093 - val_acc: 0.7411\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 15s 378us/step - loss: 0.0666 - acc: 0.9804 - val_loss: 1.8415 - val_acc: 0.7373\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 15s 366us/step - loss: 0.0679 - acc: 0.9779 - val_loss: 1.9234 - val_acc: 0.7411\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 15s 377us/step - loss: 0.0704 - acc: 0.9788 - val_loss: 1.8468 - val_acc: 0.7389\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0680 - acc: 0.9786 - val_loss: 1.7180 - val_acc: 0.7401\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.0637 - acc: 0.9804 - val_loss: 1.8805 - val_acc: 0.7341\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0671 - acc: 0.9802 - val_loss: 1.8278 - val_acc: 0.7375\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0631 - acc: 0.9803 - val_loss: 1.7727 - val_acc: 0.7363\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0653 - acc: 0.9801 - val_loss: 1.7772 - val_acc: 0.7329\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0638 - acc: 0.9809 - val_loss: 1.9311 - val_acc: 0.7403\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0596 - acc: 0.9815 - val_loss: 1.8713 - val_acc: 0.7378\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0631 - acc: 0.9807 - val_loss: 1.9635 - val_acc: 0.7407\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0644 - acc: 0.9803 - val_loss: 1.9374 - val_acc: 0.7277\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.0522 - acc: 0.9838 - val_loss: 2.0094 - val_acc: 0.7383\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 14s 341us/step - loss: 0.0658 - acc: 0.9791 - val_loss: 1.8479 - val_acc: 0.7414\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.0570 - acc: 0.9820 - val_loss: 1.9176 - val_acc: 0.7314\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0557 - acc: 0.9825 - val_loss: 2.1087 - val_acc: 0.7345\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.0649 - acc: 0.9803 - val_loss: 1.9388 - val_acc: 0.7436\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 14s 341us/step - loss: 0.0563 - acc: 0.9835 - val_loss: 2.0658 - val_acc: 0.7210\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0513 - acc: 0.9841 - val_loss: 2.1117 - val_acc: 0.7358\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0613 - acc: 0.9819 - val_loss: 1.9965 - val_acc: 0.7337\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0624 - acc: 0.9814 - val_loss: 1.8934 - val_acc: 0.7398\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0541 - acc: 0.9836 - val_loss: 1.6626 - val_acc: 0.7439\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0531 - acc: 0.9836 - val_loss: 1.9285 - val_acc: 0.7399\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0500 - acc: 0.9845 - val_loss: 1.9148 - val_acc: 0.7438\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.0584 - acc: 0.9831 - val_loss: 1.9138 - val_acc: 0.7472\n",
      "10000/10000 [==============================] - 2s 222us/step\n",
      "Validation loss: 1.913811366270146\n",
      "Validation accuracy (NORMALIZED): 0.7471999988853931\n",
      "--- Compiling model\n",
      "================================================\n",
      "Split: 1\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 15s 373us/step - loss: 2.2353 - acc: 0.1820 - val_loss: 1.7860 - val_acc: 0.3601\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 1.7086 - acc: 0.4031 - val_loss: 1.3292 - val_acc: 0.5516\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 1.3474 - acc: 0.5597 - val_loss: 1.1217 - val_acc: 0.6309\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 1.1304 - acc: 0.6367 - val_loss: 1.0401 - val_acc: 0.6535\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 1.0149 - acc: 0.6730 - val_loss: 0.9778 - val_acc: 0.6730\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.9262 - acc: 0.7020 - val_loss: 0.9358 - val_acc: 0.6858\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.8594 - acc: 0.7233 - val_loss: 0.9008 - val_acc: 0.7015\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.7994 - acc: 0.7432 - val_loss: 0.9303 - val_acc: 0.6969\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.7500 - acc: 0.7582 - val_loss: 0.9505 - val_acc: 0.7049\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.7073 - acc: 0.7701 - val_loss: 0.8956 - val_acc: 0.7076\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.6588 - acc: 0.7851 - val_loss: 0.9258 - val_acc: 0.7197\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.6152 - acc: 0.7976 - val_loss: 0.9212 - val_acc: 0.7192\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.5794 - acc: 0.8088 - val_loss: 0.9094 - val_acc: 0.7312\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.5504 - acc: 0.8180 - val_loss: 0.9613 - val_acc: 0.7307\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.5158 - acc: 0.8286 - val_loss: 0.9091 - val_acc: 0.7346\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.4845 - acc: 0.8389 - val_loss: 1.0272 - val_acc: 0.7258\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.4599 - acc: 0.8471 - val_loss: 0.9688 - val_acc: 0.7326\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 14s 348us/step - loss: 0.4283 - acc: 0.8552 - val_loss: 1.1061 - val_acc: 0.7209\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.4087 - acc: 0.8638 - val_loss: 1.0886 - val_acc: 0.7296\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.3852 - acc: 0.8705 - val_loss: 1.0506 - val_acc: 0.7363\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.3623 - acc: 0.8791 - val_loss: 1.0998 - val_acc: 0.7335\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.3376 - acc: 0.8846 - val_loss: 1.0746 - val_acc: 0.7335\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.3266 - acc: 0.8890 - val_loss: 1.0832 - val_acc: 0.7401\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.3066 - acc: 0.8951 - val_loss: 1.1506 - val_acc: 0.7322\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.2885 - acc: 0.9023 - val_loss: 1.1842 - val_acc: 0.7296\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.2772 - acc: 0.9050 - val_loss: 1.2532 - val_acc: 0.7328\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.2643 - acc: 0.9108 - val_loss: 1.2920 - val_acc: 0.7303\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.2533 - acc: 0.9144 - val_loss: 1.3707 - val_acc: 0.7311\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.2372 - acc: 0.9192 - val_loss: 1.4037 - val_acc: 0.7323\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.2354 - acc: 0.9192 - val_loss: 1.3628 - val_acc: 0.7334\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.2206 - acc: 0.9258 - val_loss: 1.4273 - val_acc: 0.7329\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.2151 - acc: 0.9275 - val_loss: 1.4921 - val_acc: 0.7289\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1996 - acc: 0.9320 - val_loss: 1.4405 - val_acc: 0.7303\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.1923 - acc: 0.9356 - val_loss: 1.4603 - val_acc: 0.7303\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1855 - acc: 0.9376 - val_loss: 1.5352 - val_acc: 0.7307\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.1852 - acc: 0.9383 - val_loss: 1.4851 - val_acc: 0.7321\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.1629 - acc: 0.9445 - val_loss: 1.7094 - val_acc: 0.7267\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.1688 - acc: 0.9448 - val_loss: 1.5791 - val_acc: 0.7304\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.1647 - acc: 0.9456 - val_loss: 1.5458 - val_acc: 0.7265\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.1582 - acc: 0.9479 - val_loss: 1.5718 - val_acc: 0.7309\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.1435 - acc: 0.9513 - val_loss: 1.7659 - val_acc: 0.7297\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1401 - acc: 0.9550 - val_loss: 1.6571 - val_acc: 0.7321\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.1445 - acc: 0.9530 - val_loss: 1.5945 - val_acc: 0.7291\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.1400 - acc: 0.9541 - val_loss: 1.6224 - val_acc: 0.7331\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.1280 - acc: 0.9579 - val_loss: 1.7218 - val_acc: 0.7272\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.1277 - acc: 0.9590 - val_loss: 1.7289 - val_acc: 0.7332\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1273 - acc: 0.9594 - val_loss: 1.7107 - val_acc: 0.7356\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.1219 - acc: 0.9612 - val_loss: 1.6403 - val_acc: 0.7289\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.1143 - acc: 0.9640 - val_loss: 1.8088 - val_acc: 0.7280\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.1171 - acc: 0.9620 - val_loss: 1.7957 - val_acc: 0.7367\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.1092 - acc: 0.9652 - val_loss: 1.7993 - val_acc: 0.7282\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1068 - acc: 0.9659 - val_loss: 1.6090 - val_acc: 0.7329\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1018 - acc: 0.9672 - val_loss: 1.7202 - val_acc: 0.7295\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.1071 - acc: 0.9649 - val_loss: 1.8053 - val_acc: 0.7308\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0930 - acc: 0.9707 - val_loss: 1.8035 - val_acc: 0.7271\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1082 - acc: 0.9659 - val_loss: 1.7465 - val_acc: 0.7308\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0880 - acc: 0.9718 - val_loss: 1.8381 - val_acc: 0.7359\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0987 - acc: 0.9684 - val_loss: 1.6567 - val_acc: 0.7260\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0893 - acc: 0.9717 - val_loss: 1.8818 - val_acc: 0.7341\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0919 - acc: 0.9705 - val_loss: 1.8212 - val_acc: 0.7302\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0816 - acc: 0.9740 - val_loss: 1.8937 - val_acc: 0.7317\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0948 - acc: 0.9697 - val_loss: 1.8327 - val_acc: 0.7232\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0833 - acc: 0.9732 - val_loss: 2.0837 - val_acc: 0.7291\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0816 - acc: 0.9742 - val_loss: 1.9020 - val_acc: 0.7275\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0818 - acc: 0.9746 - val_loss: 2.0181 - val_acc: 0.7313\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0797 - acc: 0.9743 - val_loss: 1.8525 - val_acc: 0.7323\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0794 - acc: 0.9753 - val_loss: 1.8940 - val_acc: 0.7340\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0779 - acc: 0.9761 - val_loss: 1.9153 - val_acc: 0.7316\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0767 - acc: 0.9760 - val_loss: 1.9344 - val_acc: 0.7307\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0802 - acc: 0.9752 - val_loss: 2.0351 - val_acc: 0.7258\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0761 - acc: 0.9766 - val_loss: 1.8695 - val_acc: 0.7268\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.0780 - acc: 0.9754 - val_loss: 1.8545 - val_acc: 0.7330\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0728 - acc: 0.9772 - val_loss: 1.8055 - val_acc: 0.7363\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0677 - acc: 0.9785 - val_loss: 1.8031 - val_acc: 0.7349\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0717 - acc: 0.9774 - val_loss: 1.8534 - val_acc: 0.7382\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0683 - acc: 0.9797 - val_loss: 1.9494 - val_acc: 0.7290\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0672 - acc: 0.9792 - val_loss: 1.9389 - val_acc: 0.7311\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0714 - acc: 0.9784 - val_loss: 1.9569 - val_acc: 0.7284\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0591 - acc: 0.9811 - val_loss: 2.0136 - val_acc: 0.7381\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0681 - acc: 0.9788 - val_loss: 1.9837 - val_acc: 0.7357\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0712 - acc: 0.9783 - val_loss: 1.7958 - val_acc: 0.7278\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0616 - acc: 0.9815 - val_loss: 1.8132 - val_acc: 0.7346\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0580 - acc: 0.9824 - val_loss: 1.9345 - val_acc: 0.7317\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0650 - acc: 0.9799 - val_loss: 2.0562 - val_acc: 0.7348\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 14s 360us/step - loss: 0.0687 - acc: 0.9786 - val_loss: 1.9406 - val_acc: 0.7335\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 15s 373us/step - loss: 0.0570 - acc: 0.9834 - val_loss: 1.8168 - val_acc: 0.7327\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.0630 - acc: 0.9808 - val_loss: 1.8887 - val_acc: 0.7332\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0569 - acc: 0.9832 - val_loss: 1.9222 - val_acc: 0.7342\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.0698 - acc: 0.9793 - val_loss: 1.9930 - val_acc: 0.7256\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 16s 388us/step - loss: 0.0582 - acc: 0.9831 - val_loss: 1.9279 - val_acc: 0.7277\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 15s 369us/step - loss: 0.0584 - acc: 0.9820 - val_loss: 2.0230 - val_acc: 0.7343\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.0594 - acc: 0.9814 - val_loss: 1.9647 - val_acc: 0.7300\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 16s 405us/step - loss: 0.0534 - acc: 0.9827 - val_loss: 2.0080 - val_acc: 0.7346\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 14s 354us/step - loss: 0.0625 - acc: 0.9812 - val_loss: 2.0414 - val_acc: 0.7333\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0542 - acc: 0.9838 - val_loss: 2.0107 - val_acc: 0.7332\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0572 - acc: 0.9831 - val_loss: 1.8756 - val_acc: 0.7307\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.0640 - acc: 0.9816 - val_loss: 1.9872 - val_acc: 0.7337\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 14s 348us/step - loss: 0.0550 - acc: 0.9832 - val_loss: 2.1134 - val_acc: 0.7346\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 14s 351us/step - loss: 0.0558 - acc: 0.9829 - val_loss: 1.9612 - val_acc: 0.7352\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 14s 361us/step - loss: 0.0518 - acc: 0.9839 - val_loss: 2.0201 - val_acc: 0.7296\n",
      "10000/10000 [==============================] - 3s 255us/step\n",
      "Validation loss: 2.020084795694748\n",
      "Validation accuracy (NORMALIZED): 0.729600000411272\n",
      "--- Compiling model\n",
      "================================================\n",
      "Split: 2\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 17s 427us/step - loss: 2.1298 - acc: 0.2531 - val_loss: 1.5103 - val_acc: 0.4914\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 15s 365us/step - loss: 1.5092 - acc: 0.4870 - val_loss: 1.1688 - val_acc: 0.6054\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 15s 370us/step - loss: 1.2227 - acc: 0.5954 - val_loss: 1.0762 - val_acc: 0.6336\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 16s 391us/step - loss: 1.0818 - acc: 0.6445 - val_loss: 1.0504 - val_acc: 0.6488\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 15s 383us/step - loss: 0.9712 - acc: 0.6859 - val_loss: 1.0304 - val_acc: 0.6667\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 15s 383us/step - loss: 0.8945 - acc: 0.7124 - val_loss: 0.9285 - val_acc: 0.6930\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.8229 - acc: 0.7336 - val_loss: 0.8736 - val_acc: 0.7076\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.7702 - acc: 0.7480 - val_loss: 0.8979 - val_acc: 0.7127\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 15s 386us/step - loss: 0.7232 - acc: 0.7613 - val_loss: 0.8541 - val_acc: 0.7122\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 15s 380us/step - loss: 0.6798 - acc: 0.7773 - val_loss: 0.8746 - val_acc: 0.7155\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 15s 382us/step - loss: 0.6417 - acc: 0.7884 - val_loss: 0.8396 - val_acc: 0.7314\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 16s 405us/step - loss: 0.5989 - acc: 0.8028 - val_loss: 0.8469 - val_acc: 0.7317\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 16s 392us/step - loss: 0.5640 - acc: 0.8127 - val_loss: 0.9131 - val_acc: 0.7248\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 15s 387us/step - loss: 0.5324 - acc: 0.8202 - val_loss: 0.9012 - val_acc: 0.7324\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 16s 393us/step - loss: 0.4993 - acc: 0.8310 - val_loss: 0.9376 - val_acc: 0.7294\n",
      "Epoch 16/100\n",
      "31040/40000 [======================>.......] - ETA: 3s - loss: 0.4646 - acc: 0.8433"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "#=====================================\n",
    "# Tensorboard callback\n",
    "#=====================================\n",
    "\n",
    "print(\"--- Preparing tensorboard\")\n",
    "log_dir = \"logs/{}\".format(time())\n",
    "print(\"Log Dir: \", log_dir)\n",
    "tbCallBack = TensorBoard(log_dir=log_dir, write_graph=True)\n",
    "\n",
    "#=====================================\n",
    "# Training model and Evaluation\n",
    "#=====================================\n",
    "print(\"--- Start training\")\n",
    "y_train_categorical = to_categorical(y_train, num_classes=n_classes)\n",
    "y_val_categorical = to_categorical(y_val, num_classes=n_classes)\n",
    "scores = []\n",
    "\n",
    "for i in range(folds):\n",
    "    #--- Loading model\n",
    "    model = get_squeezenet_ft3()\n",
    "    \n",
    "    #--- Evaluating the model for split i\n",
    "    print(\"================================================\")\n",
    "    print(\"Split: \"+str(i))\n",
    "    print(\"Number of Epochs: \"+str(n_epochs))\n",
    "    print(\"Train shape:\",X_train[i].shape)\n",
    "    print(\"Train batch size: \"+str(train_batch_size))\n",
    "    print(\"Val batch size: \"+str(val_batch_size))\n",
    "    #print(\"Optimizer:\", opt)\n",
    "    print(\"================================================\")\n",
    "          \n",
    "    #--- Training with data augmentation\n",
    "    aug.fit(X_train[i])\n",
    "    model.fit_generator(aug.flow(X_train[i],y_train_categorical[i], batch_size=train_batch_size),\n",
    "                        steps_per_epoch=X_train.shape[1]//train_batch_size,\n",
    "                        epochs=n_epochs, \n",
    "                        verbose=1)\n",
    "          \n",
    "    #--- Evaluating the model for split i\n",
    "    score = model.evaluate(x=X_val[i], y=y_val_categorical[i], batch_size=val_batch_size, verbose=1)\n",
    "    scores.append(score)\n",
    "    print('Validation loss:', score[0])\n",
    "    print('Validation accuracy (NORMALIZED):', score[1])\n",
    "    \n",
    "#--- Showing scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Validation loss: 1.967186710528987\n",
      "Mean Validation accuracy (NORMALIZED): 0.7352199997365474\n"
     ]
    }
   ],
   "source": [
    "#=====================================\n",
    "# Evaluate on validation\n",
    "#=====================================\n",
    "\n",
    "#--- The evaluation of the model \n",
    "np_aux = np.array(scores).mean(axis=0)\n",
    "\n",
    "print('Mean Validation loss:', np_aux[0])\n",
    "print('Mean Validation accuracy (NORMALIZED):', np_aux[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your best model on test\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "Now that we are working on more complex tasks and our trainings are starting to take more time it is usually a good idea to save the trained model from time to time. [Keras has a lot of ways of saving and loading the model](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model), but in this exercise we will use the simplest of them all: `model.save()`. It saves the architecture, the weights, the choice of loss function/optimizer/metrics and even the current state of the training, so you can resume your training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a model\n",
    "Once we have our model trained, we can load it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.181651924790132\n",
      "Test accuracy (NORMALIZED): 0.729099999576807\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "del model  # Will delete model, only to check if load_model is working\n",
    "\n",
    "# returns a compiled model identical to the previous one\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# evaluate test set again... should give us the same result\n",
    "# ...\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NORMALIZED):', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
