{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensorflow version: 1.12.0-rc0\n",
    "scikit-learn version: 0.17\n",
    "keras version: 2.2.4\n",
    "tensorboard version: 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "In this assignment, we will use the weights of a network pre-trained in a particular problem as starting point to train our CNN to a different problem. As training a network from scratch is time-consuming and demands a lot of data, this is a frequent strategy, specially if both datasets (the one used for pre-training and the target) shares similar structures/elements/concepts. \n",
    "\n",
    "This is specially true when working with images. Most filters learned in initial convolutional layers will detect low-level elements, such as borders, corners and color blobs, which are common to most problems in the image domain. \n",
    "\n",
    "In this notebook, we will load the SqueezeNet architecture trained in the ImageNet dataset and fine-tune it to CIFAR-10.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.5/dist-packages (2.2.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (3.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (1.11.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (1.0.6)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras==2.2.4) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.17.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "from random import sample, seed\n",
    "seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.figsize'] = (15,15) # Make the figures a bit bigger\n",
    "\n",
    "# Keras imports\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, concatenate, Dropout, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "if (sklearn.__version__ == '0.20.0'):\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "else:\n",
    "    from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "#======================================\n",
    "# Global definitions\n",
    "#======================================\n",
    "n_epochs         = 100\n",
    "learning_rate    = 1e-4\n",
    "n_classes        = 10\n",
    "train_batch_size = 32\n",
    "val_batch_size   = 10\n",
    "    \n",
    "#Utility to plot\n",
    "def plotImages(imgList):\n",
    "    for i in range(len(imgList)):\n",
    "        plotImage(imgList[i])\n",
    "        \n",
    "        \n",
    "def plotImage(img):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.imshow(np.uint8(img), interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SqueezeNet definition\n",
    "These methods define our architecture and load the weights obtained using ImageNet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fire Module Definition\n",
    "sq1x1 = \"squeeze1x1\"\n",
    "exp1x1 = \"expand1x1\"\n",
    "exp3x3 = \"expand3x3\"\n",
    "relu = \"relu_\"\n",
    "\n",
    "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
    "    s_id = 'fire' + str(fire_id) + '/'\n",
    "\n",
    "    channel_axis = 3\n",
    "    \n",
    "    x = Convolution2D(squeeze, (1, 1), padding='valid', name=s_id + sq1x1)(x)\n",
    "    x = Activation('relu', name=s_id + relu + sq1x1)(x)\n",
    "\n",
    "    left = Convolution2D(expand, (1, 1), padding='valid', name=s_id + exp1x1)(x)\n",
    "    left = Activation('relu', name=s_id + relu + exp1x1)(left)\n",
    "\n",
    "    right = Convolution2D(expand, (3, 3), padding='same', name=s_id + exp3x3)(x)\n",
    "    right = Activation('relu', name=s_id + relu + exp3x3)(right)\n",
    "\n",
    "    x = concatenate([left, right], axis=channel_axis, name=s_id + 'concat')\n",
    "    return x\n",
    "\n",
    "#SqueezeNet model definition\n",
    "def SqueezeNet(input_shape):\n",
    "    img_input = Input(shape=input_shape) #placeholder\n",
    "    \n",
    "    x = Convolution2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n",
    "    x = Activation('relu', name='relu_conv1')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
    "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
    "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
    "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
    "    \n",
    "    x = Dropout(0.5, name='drop9')(x)\n",
    "\n",
    "    x = Convolution2D(1000, (1, 1), padding='valid', name='conv10')(x)\n",
    "    x = Activation('relu', name='relu_conv10')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Activation('softmax', name='loss')(x)\n",
    "\n",
    "    model = Model(img_input, x, name='squeezenet')\n",
    "\n",
    "    # Download and load ImageNet weights\n",
    "    model.load_weights('./squeezenet_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The class are **airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val data. X:  (50000, 32, 32, 3) , Y:  (50000, 1)\n",
      "Test data. X:  (10000, 32, 32, 3) , Y:  (10000, 1)\n",
      "--- Splitting data into train and val\n",
      "Train data. X: (5, 40000, 32, 32, 3) Y: (5, 40000, 1)\n",
      "Val data. X: (5, 10000, 32, 32, 3) Y: (5, 10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "(trainVal_data, trainVal_label), (X_test, y_test) = cifar10.load_data()\n",
    "print(\"Train/Val data. X: \", trainVal_data.shape, \", Y: \", trainVal_label.shape)\n",
    "print(\"Test data. X: \", X_test.shape, \", Y: \", y_test.shape)\n",
    "\n",
    "# Normalizing the data\n",
    "trainVal_data = (trainVal_data / 127.5) - 1.\n",
    "X_test = (X_test  / 127.5) - 1.\n",
    "\n",
    "#=====================================\n",
    "# Prepare the data\n",
    "#=====================================\n",
    "\n",
    "#--- Dividing the data into training and validation\n",
    "folds = 5\n",
    "if (sklearn.__version__ == '0.20.0'):\n",
    "    sss = StratifiedShuffleSplit(folds, test_size=0.2, random_state=42)\n",
    "    sss = sss.split(trainVal_data,trainVal_label)\n",
    "else:\n",
    "    sss = StratifiedShuffleSplit(trainVal_label, folds, test_size=0.2, random_state=42)\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_val = []\n",
    "y_val = []\n",
    "for train_index, val_index in sss:\n",
    "    X_train.append(trainVal_data[train_index])\n",
    "    X_val.append(trainVal_data[val_index])\n",
    "    y_train.append(trainVal_label[train_index])\n",
    "    y_val.append(trainVal_label[val_index])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "    \n",
    "print(\"--- Splitting data into train and val\")\n",
    "print(\"Train data. X:\",X_train.shape,\"Y:\",y_train.shape)\n",
    "print(\"Val data. X:\",X_val.shape,\"Y:\",y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000 4000 4000 4000 4000 4000 4000 4000 4000 4000]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000 4000 4000 4000 4000 4000 4000 4000 4000 4000]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000 4000 4000 4000 4000 4000 4000 4000 4000 4000]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000 4000 4000 4000 4000 4000 4000 4000 4000 4000]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000 4000 4000 4000 4000 4000 4000 4000 4000 4000]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cheking if the splits are balanced\n",
    "for i in range(folds):\n",
    "    hist = np.histogram(np.squeeze(y_train[i]))[0]\n",
    "    print(hist)    \n",
    "    plt.bar(hist,np.amax(hist),alpha=0.5)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## SqueezeNet with frozen layers\n",
    "Our initial attempt will be to remove SqueezeNet's top layers --- responsible for the classification into ImageNet classes --- and train a new set of layers to our CIFAR-10 classes. We will also freeze the layers before `drop9`. Our architecture will be like this:\n",
    "\n",
    "<img src=\"frozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/azaelmsousa/AzaEnv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 15, 15, 64)   1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv1 (Activation)         (None, 15, 15, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 7, 7, 64)     0           relu_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fire2/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     1040        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire2/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire2/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire2/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/concat (Concatenate)      (None, 7, 7, 128)    0           fire2/relu_expand1x1[0][0]       \n",
      "                                                                 fire2/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire3/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     2064        fire2/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire3/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire3/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire3/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/concat (Concatenate)      (None, 7, 7, 128)    0           fire3/relu_expand1x1[0][0]       \n",
      "                                                                 fire3/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 3, 3, 128)    0           fire3/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire4/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     4128        pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire4/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire4/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire4/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/concat (Concatenate)      (None, 3, 3, 256)    0           fire4/relu_expand1x1[0][0]       \n",
      "                                                                 fire4/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire5/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     8224        fire4/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire5/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire5/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire5/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/concat (Concatenate)      (None, 3, 3, 256)    0           fire5/relu_expand1x1[0][0]       \n",
      "                                                                 fire5/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 1, 1, 256)    0           fire5/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire6/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     12336       pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire6/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire6/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire6/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/concat (Concatenate)      (None, 1, 1, 384)    0           fire6/relu_expand1x1[0][0]       \n",
      "                                                                 fire6/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire7/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     18480       fire6/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire7/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire7/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire7/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/concat (Concatenate)      (None, 1, 1, 384)    0           fire7/relu_expand1x1[0][0]       \n",
      "                                                                 fire7/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire8/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     24640       fire7/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire8/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire8/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire8/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/concat (Concatenate)      (None, 1, 1, 512)    0           fire8/relu_expand1x1[0][0]       \n",
      "                                                                 fire8/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire9/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     32832       fire8/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire9/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire9/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire9/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/concat (Concatenate)      (None, 1, 1, 512)    0           fire9/relu_expand1x1[0][0]       \n",
      "                                                                 fire9/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "drop9 (Dropout)                 (None, 1, 1, 512)    0           fire9/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv10 (Conv2D)                 (None, 1, 1, 10)     5130        drop9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv10 (Activation)        (None, 1, 1, 10)     0           conv10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 10)           0           relu_conv10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "loss (Activation)               (None, 10)           0           global_average_pooling2d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 727,626\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 722,496\n",
      "__________________________________________________________________________________________________\n",
      "<keras.engine.input_layer.InputLayer object at 0x7fa0c72520b8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c7252668> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c72524e0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fa0c72525c0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c72525f8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c68b7588> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c68b7940> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c68cbd30> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c68cb908> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c6874438> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c68747b8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c69e4b38> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c688c518> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c688c710> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c6834828> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c68343c8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c684eac8> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c684eba8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fa0c6874390> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c684e908> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c67f4cc0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c67f4f60> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c681b198> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c681b0b8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c67b3908> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c67c5390> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c67f4cf8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c67dba58> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c67db470> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c6774cf8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c67749b0> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c679e5c0> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c679ea20> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fa0c67b39e8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c679ec18> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c729e358> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c729e898> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c72a2240> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c72a2a90> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c728b2b0> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c728bc88> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c7227198> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c72915c0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c7291470> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c7231c88> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c7231ba8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c72cef28> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c72cea20> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c728bb38> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c72e3358> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c72e37b8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c730efd0> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c730e198> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c731c080> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c731ce80> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c72cedd8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c7321278> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c7321400> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0c7350358> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c7350cf8> False\n",
      "<keras.layers.core.Activation object at 0x7fa0c7347cf8> False\n",
      "<keras.layers.merge.Concatenate object at 0x7fa0c7347b70> False\n",
      "<keras.layers.core.Dropout object at 0x7fa0c731c4a8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fa0db9075c0> True\n",
      "<keras.layers.core.Activation object at 0x7fa0c45a7860> True\n",
      "<keras.layers.pooling.GlobalAveragePooling2D object at 0x7fa0c661b208> True\n",
      "<keras.layers.core.Activation object at 0x7fa0c661b080> True\n"
     ]
    }
   ],
   "source": [
    "def get_squeezenet_ft():\n",
    "    #=====================================\n",
    "    # Freezing layers\n",
    "    #=====================================\n",
    "\n",
    "    squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "    #freeze layers\n",
    "    for layer in squeezeNetModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #=====================================\n",
    "    # Add new classification layers\n",
    "    #=====================================\n",
    "\n",
    "    #--- Removing layers until drop 9\n",
    "    squeezeNetModel.layers.pop() #Convolution2D\n",
    "    squeezeNetModel.layers.pop() #Activation ReLU\n",
    "    squeezeNetModel.layers.pop() #Global Avg Pool\n",
    "    squeezeNetModel.layers.pop() #Activation Softmax\n",
    "\n",
    "    #--- Adding classification layer for 10 classes\n",
    "    out = Convolution2D(n_classes, (1, 1), padding='valid', name='conv10')(squeezeNetModel.layers[-1].output)\n",
    "    out = Activation('relu', name='relu_conv10')(out)\n",
    "\n",
    "    out = GlobalAveragePooling2D()(out)\n",
    "    out = Activation('softmax', name='loss')(out)\n",
    "\n",
    "    #=====================================\n",
    "    # New Model\n",
    "    #=====================================\n",
    "    model = Model(squeezeNetModel.inputs, out, name='squeezenet_new')\n",
    "    #=====================================\n",
    "    # Compile model\n",
    "    #=====================================\n",
    "\n",
    "    #--- Compile the model\n",
    "    # It means to configure the model for training.\n",
    "    # Other types of optimizer:\n",
    "    #    optimizers.Adam(lr=learning_rate)\n",
    "    opt = optimizer=optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)#SGD(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model  = get_squeezenet_ft()\n",
    "model.summary()\n",
    "\n",
    "#--- Check the trainable status of the individual layers\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Split: 0\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 10s 246us/step - loss: 2.3094 - acc: 0.1072 - val_loss: 2.3005 - val_acc: 0.1193\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 2.2793 - acc: 0.1523 - val_loss: 2.2384 - val_acc: 0.2278\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 8s 193us/step - loss: 2.2292 - acc: 0.1897 - val_loss: 2.1418 - val_acc: 0.2798\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 8s 202us/step - loss: 2.1631 - acc: 0.2296 - val_loss: 2.0812 - val_acc: 0.3167\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 7s 181us/step - loss: 2.1132 - acc: 0.2521 - val_loss: 2.0198 - val_acc: 0.3256\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 8s 196us/step - loss: 2.0658 - acc: 0.2694 - val_loss: 1.9563 - val_acc: 0.3339\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 8s 202us/step - loss: 2.0049 - acc: 0.2797 - val_loss: 1.9139 - val_acc: 0.3442\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.9698 - acc: 0.2926 - val_loss: 1.8881 - val_acc: 0.3567\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 7s 177us/step - loss: 1.9476 - acc: 0.3019 - val_loss: 1.8689 - val_acc: 0.3634\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.9302 - acc: 0.3070 - val_loss: 1.8535 - val_acc: 0.3666\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.9173 - acc: 0.3123 - val_loss: 1.8410 - val_acc: 0.3669\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 8s 191us/step - loss: 1.9088 - acc: 0.3119 - val_loss: 1.8301 - val_acc: 0.3726\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 8s 198us/step - loss: 1.9022 - acc: 0.3143 - val_loss: 1.8231 - val_acc: 0.3737\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 1.8976 - acc: 0.3157 - val_loss: 1.8143 - val_acc: 0.3750\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 6s 160us/step - loss: 1.8904 - acc: 0.3195 - val_loss: 1.8081 - val_acc: 0.3742\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 6s 159us/step - loss: 1.8875 - acc: 0.3168 - val_loss: 1.8036 - val_acc: 0.3730\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 6s 158us/step - loss: 1.8838 - acc: 0.3233 - val_loss: 1.7972 - val_acc: 0.3756\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 1.8811 - acc: 0.3207 - val_loss: 1.7937 - val_acc: 0.3767\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.8783 - acc: 0.3217 - val_loss: 1.7896 - val_acc: 0.3778\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 6s 160us/step - loss: 1.8737 - acc: 0.3238 - val_loss: 1.7858 - val_acc: 0.3772\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 6s 162us/step - loss: 1.8716 - acc: 0.3279 - val_loss: 1.7824 - val_acc: 0.3769\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 6s 162us/step - loss: 1.8701 - acc: 0.3264 - val_loss: 1.7802 - val_acc: 0.3758\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 7s 181us/step - loss: 1.8713 - acc: 0.3247 - val_loss: 1.7769 - val_acc: 0.3790\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.8621 - acc: 0.3273 - val_loss: 1.7745 - val_acc: 0.3822\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 8s 198us/step - loss: 1.8630 - acc: 0.3268 - val_loss: 1.7716 - val_acc: 0.3792\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 7s 178us/step - loss: 1.8630 - acc: 0.3288 - val_loss: 1.7704 - val_acc: 0.3768\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 8s 190us/step - loss: 1.8597 - acc: 0.3284 - val_loss: 1.7683 - val_acc: 0.3817\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.8616 - acc: 0.3295 - val_loss: 1.7658 - val_acc: 0.3762\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 1.8607 - acc: 0.3270 - val_loss: 1.7641 - val_acc: 0.3807\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 6s 157us/step - loss: 1.8582 - acc: 0.3286 - val_loss: 1.7631 - val_acc: 0.3796\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 6s 158us/step - loss: 1.8563 - acc: 0.3251 - val_loss: 1.7616 - val_acc: 0.3802\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 7s 183us/step - loss: 1.8526 - acc: 0.3310 - val_loss: 1.7596 - val_acc: 0.3784\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.8564 - acc: 0.3313 - val_loss: 1.7588 - val_acc: 0.3800\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 7s 180us/step - loss: 1.8556 - acc: 0.3275 - val_loss: 1.7579 - val_acc: 0.3795\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.8506 - acc: 0.3307 - val_loss: 1.7556 - val_acc: 0.3801\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.8548 - acc: 0.3278 - val_loss: 1.7550 - val_acc: 0.3837\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.8502 - acc: 0.3301 - val_loss: 1.7541 - val_acc: 0.3806\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.8518 - acc: 0.3317 - val_loss: 1.7530 - val_acc: 0.3824\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 1.8503 - acc: 0.3283 - val_loss: 1.7515 - val_acc: 0.3809\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.8471 - acc: 0.3319 - val_loss: 1.7508 - val_acc: 0.3832\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 7s 186us/step - loss: 1.8483 - acc: 0.3309 - val_loss: 1.7499 - val_acc: 0.3828\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 7s 183us/step - loss: 1.8499 - acc: 0.3286 - val_loss: 1.7496 - val_acc: 0.3815\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.8476 - acc: 0.3308 - val_loss: 1.7493 - val_acc: 0.3826\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 7s 183us/step - loss: 1.8490 - acc: 0.3333 - val_loss: 1.7487 - val_acc: 0.3814\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 1.8473 - acc: 0.3301 - val_loss: 1.7472 - val_acc: 0.3844\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.8463 - acc: 0.3339 - val_loss: 1.7465 - val_acc: 0.3809\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 7s 179us/step - loss: 1.8469 - acc: 0.3321 - val_loss: 1.7461 - val_acc: 0.3849\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.8475 - acc: 0.3322 - val_loss: 1.7454 - val_acc: 0.3812\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 1.8438 - acc: 0.3329 - val_loss: 1.7451 - val_acc: 0.3851\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8437 - acc: 0.3315 - val_loss: 1.7440 - val_acc: 0.3837\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 6s 162us/step - loss: 1.8447 - acc: 0.3317 - val_loss: 1.7433 - val_acc: 0.3849\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8440 - acc: 0.3337 - val_loss: 1.7431 - val_acc: 0.3840\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 1.8393 - acc: 0.3322 - val_loss: 1.7422 - val_acc: 0.3825\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8429 - acc: 0.3329 - val_loss: 1.7420 - val_acc: 0.3829\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 1.8432 - acc: 0.3335 - val_loss: 1.7418 - val_acc: 0.3809\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.8406 - acc: 0.3338 - val_loss: 1.7416 - val_acc: 0.3827\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 7s 179us/step - loss: 1.8445 - acc: 0.3328 - val_loss: 1.7404 - val_acc: 0.3851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.8408 - acc: 0.3326 - val_loss: 1.7399 - val_acc: 0.3863\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 1.8409 - acc: 0.3332 - val_loss: 1.7391 - val_acc: 0.3851\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 7s 177us/step - loss: 1.8415 - acc: 0.3342 - val_loss: 1.7398 - val_acc: 0.3849\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.8442 - acc: 0.3339 - val_loss: 1.7391 - val_acc: 0.3844\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.8417 - acc: 0.3327 - val_loss: 1.7384 - val_acc: 0.3838\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.8441 - acc: 0.3311 - val_loss: 1.7384 - val_acc: 0.3862\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.8400 - acc: 0.3346 - val_loss: 1.7384 - val_acc: 0.3843\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8413 - acc: 0.3328 - val_loss: 1.7388 - val_acc: 0.3853\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.8410 - acc: 0.3347 - val_loss: 1.7373 - val_acc: 0.3876\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 8s 200us/step - loss: 1.8441 - acc: 0.3343 - val_loss: 1.7368 - val_acc: 0.3846\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 9s 213us/step - loss: 1.8381 - acc: 0.3331 - val_loss: 1.7369 - val_acc: 0.3831\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 8s 196us/step - loss: 1.8390 - acc: 0.3328 - val_loss: 1.7359 - val_acc: 0.3846\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.8382 - acc: 0.3360 - val_loss: 1.7356 - val_acc: 0.3885\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 1.8391 - acc: 0.3358 - val_loss: 1.7362 - val_acc: 0.3839\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.8398 - acc: 0.3325 - val_loss: 1.7353 - val_acc: 0.3836\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.8355 - acc: 0.3367 - val_loss: 1.7345 - val_acc: 0.3847\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.8371 - acc: 0.3355 - val_loss: 1.7350 - val_acc: 0.3867\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.8410 - acc: 0.3349 - val_loss: 1.7344 - val_acc: 0.3856\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8382 - acc: 0.3351 - val_loss: 1.7342 - val_acc: 0.3845\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8371 - acc: 0.3350 - val_loss: 1.7347 - val_acc: 0.3868\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8364 - acc: 0.3367 - val_loss: 1.7339 - val_acc: 0.3889\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.8341 - acc: 0.3374 - val_loss: 1.7344 - val_acc: 0.3883\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.8345 - acc: 0.3349 - val_loss: 1.7327 - val_acc: 0.3865\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 1.8365 - acc: 0.3359 - val_loss: 1.7343 - val_acc: 0.3857\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.8362 - acc: 0.3350 - val_loss: 1.7335 - val_acc: 0.3893\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.8345 - acc: 0.3342 - val_loss: 1.7331 - val_acc: 0.3880\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.8369 - acc: 0.3368 - val_loss: 1.7326 - val_acc: 0.3872\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.8339 - acc: 0.3338 - val_loss: 1.7324 - val_acc: 0.3872\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 1.8354 - acc: 0.3370 - val_loss: 1.7325 - val_acc: 0.3842\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8360 - acc: 0.3366 - val_loss: 1.7319 - val_acc: 0.3849\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.8334 - acc: 0.3363 - val_loss: 1.7312 - val_acc: 0.3860\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8383 - acc: 0.3346 - val_loss: 1.7319 - val_acc: 0.3844\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.8374 - acc: 0.3326 - val_loss: 1.7313 - val_acc: 0.3852\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.8330 - acc: 0.3372 - val_loss: 1.7312 - val_acc: 0.3875\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.8320 - acc: 0.3365 - val_loss: 1.7313 - val_acc: 0.3868\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.8336 - acc: 0.3362 - val_loss: 1.7308 - val_acc: 0.3898\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8349 - acc: 0.3364 - val_loss: 1.7306 - val_acc: 0.3880\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8378 - acc: 0.3329 - val_loss: 1.7305 - val_acc: 0.3866\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8381 - acc: 0.3333 - val_loss: 1.7310 - val_acc: 0.3864\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8309 - acc: 0.3341 - val_loss: 1.7304 - val_acc: 0.3872\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.8349 - acc: 0.3371 - val_loss: 1.7305 - val_acc: 0.3877\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: 1.8331 - acc: 0.3380 - val_loss: 1.7309 - val_acc: 0.3886\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.8322 - acc: 0.3361 - val_loss: 1.7302 - val_acc: 0.3874\n",
      "10000/10000 [==============================] - 3s 306us/step\n",
      "Validation loss: 1.730198492527008\n",
      "Validation accuracy (NORMALIZED): 0.38740000676363706\n",
      "================================================\n",
      "Split: 1\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 10s 243us/step - loss: 2.3206 - acc: 0.1241 - val_loss: 2.2614 - val_acc: 0.1692\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 2.2569 - acc: 0.1617 - val_loss: 2.2038 - val_acc: 0.2169\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 6s 162us/step - loss: 2.1992 - acc: 0.1980 - val_loss: 2.1007 - val_acc: 0.2885\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 2.1083 - acc: 0.2370 - val_loss: 1.9957 - val_acc: 0.3366\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 2.0398 - acc: 0.2591 - val_loss: 1.9492 - val_acc: 0.3531\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 1.9922 - acc: 0.2808 - val_loss: 1.9171 - val_acc: 0.3575\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 1.9688 - acc: 0.2889 - val_loss: 1.8945 - val_acc: 0.3608\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 8s 196us/step - loss: 1.9455 - acc: 0.3018 - val_loss: 1.8757 - val_acc: 0.3647\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 8s 212us/step - loss: 1.9336 - acc: 0.3047 - val_loss: 1.8614 - val_acc: 0.3672\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 8s 188us/step - loss: 1.9195 - acc: 0.3093 - val_loss: 1.8492 - val_acc: 0.3674\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 7s 178us/step - loss: 1.9082 - acc: 0.3156 - val_loss: 1.8390 - val_acc: 0.3704\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.9017 - acc: 0.3157 - val_loss: 1.8300 - val_acc: 0.3709\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 1.8973 - acc: 0.3170 - val_loss: 1.8227 - val_acc: 0.3715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 7s 177us/step - loss: 1.8908 - acc: 0.3164 - val_loss: 1.8166 - val_acc: 0.3740\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 1.8853 - acc: 0.3212 - val_loss: 1.8115 - val_acc: 0.3713\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.8832 - acc: 0.3213 - val_loss: 1.8058 - val_acc: 0.3731\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 8s 198us/step - loss: 1.8765 - acc: 0.3204 - val_loss: 1.8015 - val_acc: 0.3721\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 8s 194us/step - loss: 1.8752 - acc: 0.3207 - val_loss: 1.7975 - val_acc: 0.3749\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 7s 181us/step - loss: 1.8706 - acc: 0.3241 - val_loss: 1.7940 - val_acc: 0.3756\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 1.8696 - acc: 0.3263 - val_loss: 1.7909 - val_acc: 0.3738\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 8s 198us/step - loss: 1.8645 - acc: 0.3261 - val_loss: 1.7879 - val_acc: 0.3775\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 1.8671 - acc: 0.3223 - val_loss: 1.7851 - val_acc: 0.3770\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 8s 198us/step - loss: 1.8617 - acc: 0.3273 - val_loss: 1.7842 - val_acc: 0.3765\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 8s 190us/step - loss: 1.8617 - acc: 0.3257 - val_loss: 1.7820 - val_acc: 0.3749\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 8s 195us/step - loss: 1.8589 - acc: 0.3301 - val_loss: 1.7782 - val_acc: 0.3795\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 8s 189us/step - loss: 1.8597 - acc: 0.3293 - val_loss: 1.7761 - val_acc: 0.3784\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 7s 178us/step - loss: 1.8592 - acc: 0.3265 - val_loss: 1.7746 - val_acc: 0.3775\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 7s 177us/step - loss: 1.8617 - acc: 0.3261 - val_loss: 1.7728 - val_acc: 0.3790\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.8558 - acc: 0.3281 - val_loss: 1.7714 - val_acc: 0.3796\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 8s 197us/step - loss: 1.8571 - acc: 0.3272 - val_loss: 1.7699 - val_acc: 0.3792\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 1.8540 - acc: 0.3285 - val_loss: 1.7684 - val_acc: 0.3810\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 7s 179us/step - loss: 1.8505 - acc: 0.3307 - val_loss: 1.7669 - val_acc: 0.3802\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 7s 177us/step - loss: 1.8505 - acc: 0.3308 - val_loss: 1.7657 - val_acc: 0.3789\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.8498 - acc: 0.3325 - val_loss: 1.7647 - val_acc: 0.3799\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 7s 183us/step - loss: 1.8488 - acc: 0.3285 - val_loss: 1.7637 - val_acc: 0.3792\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 8s 190us/step - loss: 1.8502 - acc: 0.3288 - val_loss: 1.7626 - val_acc: 0.3810\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 7s 183us/step - loss: 1.8481 - acc: 0.3320 - val_loss: 1.7618 - val_acc: 0.3812\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.8470 - acc: 0.3317 - val_loss: 1.7608 - val_acc: 0.3803\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.8481 - acc: 0.3308 - val_loss: 1.7598 - val_acc: 0.3801\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.8465 - acc: 0.3325 - val_loss: 1.7590 - val_acc: 0.3819\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.8471 - acc: 0.3308 - val_loss: 1.7582 - val_acc: 0.3813\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.8422 - acc: 0.3350 - val_loss: 1.7580 - val_acc: 0.3818\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 1.8434 - acc: 0.3346 - val_loss: 1.7573 - val_acc: 0.3799\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 8s 205us/step - loss: 1.8423 - acc: 0.3324 - val_loss: 1.7560 - val_acc: 0.3810\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 8s 193us/step - loss: 1.8463 - acc: 0.3289 - val_loss: 1.7551 - val_acc: 0.3811\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 1.8423 - acc: 0.3321 - val_loss: 1.7546 - val_acc: 0.3805\n",
      "Epoch 47/100\n",
      "36416/40000 [==========================>...] - ETA: 0s - loss: 1.8404 - acc: 0.3316"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b018087e0515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val_categorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m               verbose=1)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#--- Evaluating the model for split i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AzaEnv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/AzaEnv/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AzaEnv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AzaEnv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#=====================================\n",
    "# Training model and Evaluation\n",
    "#=====================================\n",
    "\n",
    "y_train_categorical = to_categorical(y_train, num_classes=n_classes)\n",
    "y_val_categorical = to_categorical(y_val, num_classes=n_classes)\n",
    "\n",
    "scores = []\n",
    "for i in range(folds):\n",
    "    #--- Loading model\n",
    "    model  = get_squeezenet_ft()\n",
    "    \n",
    "    #--- Evaluating the model for split i\n",
    "    print(\"================================================\")\n",
    "    print(\"Split: \"+str(i))\n",
    "    print(\"Number of Epochs: \"+str(n_epochs))\n",
    "    print(\"Train shape:\",X_train[i].shape)\n",
    "    print(\"Train batch size: \"+str(train_batch_size))\n",
    "    print(\"Val batch size: \"+str(val_batch_size))\n",
    "    print(\"================================================\")\n",
    "    \n",
    "    # Training without data augmentation\n",
    "    model.fit(X_train[i],y_train_categorical[i], batch_size=train_batch_size,\n",
    "              epochs=n_epochs, \n",
    "              validation_data=(X_val[i],y_val_categorical[i]),\n",
    "              verbose=1)\n",
    "    \n",
    "    #--- Evaluating the model for split i\n",
    "    score = model.evaluate(x=X_val[i], y=y_val_categorical[i], batch_size=val_batch_size, verbose=1)\n",
    "    scores.append(score)\n",
    "    print('Validation loss:', score[0])\n",
    "    print('Validation accuracy (NORMALIZED):', score[1])\n",
    "    \n",
    "#--- Showing scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Validation loss: 2.0218363160133364\n",
      "Mean Validation accuracy (NORMALIZED): 0.28714000583887095\n"
     ]
    }
   ],
   "source": [
    "#=====================================\n",
    "# Evaluate on validation\n",
    "#=====================================\n",
    "\n",
    "#--- The evaluation of the model \n",
    "np_aux = np.array(scores).mean(axis=0)\n",
    "\n",
    "print('Mean Validation loss:', np_aux[0])\n",
    "print('Mean Validation accuracy (NORMALIZED):', np_aux[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "-----------------\n",
    "\n",
    "# Training last 2 Fire Modules + classification layers\n",
    "As we could see, the frozen network performed very poorly. By freezing most layers, we do not allow SqueezeNet to adapt its weights to features present in CIFAR-10.\n",
    "\n",
    "Let's try to unfreeze the last two fire modules and train once more. The architecture will be:\n",
    "<img src=\"partFrozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 15, 15, 64)   1792        input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv1 (Activation)         (None, 15, 15, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 7, 7, 64)     0           relu_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fire2/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     1040        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire2/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire2/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire2/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/concat (Concatenate)      (None, 7, 7, 128)    0           fire2/relu_expand1x1[0][0]       \n",
      "                                                                 fire2/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire3/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     2064        fire2/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire3/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire3/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire3/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/concat (Concatenate)      (None, 7, 7, 128)    0           fire3/relu_expand1x1[0][0]       \n",
      "                                                                 fire3/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 3, 3, 128)    0           fire3/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire4/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     4128        pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire4/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire4/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire4/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/concat (Concatenate)      (None, 3, 3, 256)    0           fire4/relu_expand1x1[0][0]       \n",
      "                                                                 fire4/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire5/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     8224        fire4/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire5/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire5/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire5/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/concat (Concatenate)      (None, 3, 3, 256)    0           fire5/relu_expand1x1[0][0]       \n",
      "                                                                 fire5/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 1, 1, 256)    0           fire5/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire6/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     12336       pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire6/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire6/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire6/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/concat (Concatenate)      (None, 1, 1, 384)    0           fire6/relu_expand1x1[0][0]       \n",
      "                                                                 fire6/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire7/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     18480       fire6/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire7/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire7/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire7/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/concat (Concatenate)      (None, 1, 1, 384)    0           fire7/relu_expand1x1[0][0]       \n",
      "                                                                 fire7/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire8/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     24640       fire7/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire8/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire8/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire8/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/concat (Concatenate)      (None, 1, 1, 512)    0           fire8/relu_expand1x1[0][0]       \n",
      "                                                                 fire8/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire9/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     32832       fire8/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire9/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire9/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire9/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/concat (Concatenate)      (None, 1, 1, 512)    0           fire9/relu_expand1x1[0][0]       \n",
      "                                                                 fire9/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "drop9 (Dropout)                 (None, 1, 1, 512)    0           fire9/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv10 (Conv2D)                 (None, 1, 1, 10)     5130        drop9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv10 (Activation)        (None, 1, 1, 10)     0           conv10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_18 (Gl (None, 10)           0           relu_conv10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "loss (Activation)               (None, 10)           0           global_average_pooling2d_18[0][0]\n",
      "==================================================================================================\n",
      "Total params: 727,626\n",
      "Trainable params: 391,306\n",
      "Non-trainable params: 336,320\n",
      "__________________________________________________________________________________________________\n",
      "input_9 False\n",
      "conv1 False\n",
      "relu_conv1 False\n",
      "pool1 False\n",
      "fire2/squeeze1x1 False\n",
      "fire2/relu_squeeze1x1 False\n",
      "fire2/expand1x1 False\n",
      "fire2/expand3x3 False\n",
      "fire2/relu_expand1x1 False\n",
      "fire2/relu_expand3x3 False\n",
      "fire2/concat False\n",
      "fire3/squeeze1x1 False\n",
      "fire3/relu_squeeze1x1 False\n",
      "fire3/expand1x1 False\n",
      "fire3/expand3x3 False\n",
      "fire3/relu_expand1x1 False\n",
      "fire3/relu_expand3x3 False\n",
      "fire3/concat False\n",
      "pool3 False\n",
      "fire4/squeeze1x1 False\n",
      "fire4/relu_squeeze1x1 False\n",
      "fire4/expand1x1 False\n",
      "fire4/expand3x3 False\n",
      "fire4/relu_expand1x1 False\n",
      "fire4/relu_expand3x3 False\n",
      "fire4/concat False\n",
      "fire5/squeeze1x1 False\n",
      "fire5/relu_squeeze1x1 False\n",
      "fire5/expand1x1 False\n",
      "fire5/expand3x3 False\n",
      "fire5/relu_expand1x1 False\n",
      "fire5/relu_expand3x3 False\n",
      "fire5/concat False\n",
      "pool5 False\n",
      "fire6/squeeze1x1 False\n",
      "fire6/relu_squeeze1x1 False\n",
      "fire6/expand1x1 False\n",
      "fire6/expand3x3 False\n",
      "fire6/relu_expand1x1 False\n",
      "fire6/relu_expand3x3 False\n",
      "fire6/concat False\n",
      "fire7/squeeze1x1 False\n",
      "fire7/relu_squeeze1x1 False\n",
      "fire7/expand1x1 False\n",
      "fire7/expand3x3 False\n",
      "fire7/relu_expand1x1 False\n",
      "fire7/relu_expand3x3 False\n",
      "fire7/concat False\n",
      "fire8/squeeze1x1 True\n",
      "fire8/relu_squeeze1x1 True\n",
      "fire8/expand1x1 True\n",
      "fire8/expand3x3 True\n",
      "fire8/relu_expand1x1 True\n",
      "fire8/relu_expand3x3 True\n",
      "fire8/concat True\n",
      "fire9/squeeze1x1 True\n",
      "fire9/relu_squeeze1x1 True\n",
      "fire9/expand1x1 True\n",
      "fire9/expand3x3 True\n",
      "fire9/relu_expand1x1 True\n",
      "fire9/relu_expand3x3 True\n",
      "fire9/concat True\n",
      "drop9 True\n",
      "conv10 True\n",
      "relu_conv10 True\n",
      "global_average_pooling2d_18 True\n",
      "loss True\n"
     ]
    }
   ],
   "source": [
    "def get_squeezenet_ft2():\n",
    "    squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "    #=====================================\n",
    "    # Freezing mentioned layers\n",
    "    #=====================================\n",
    "\n",
    "    trainable_layer_index = 19\n",
    "    for i in range(len(squeezeNetModel.layers)-trainable_layer_index):\n",
    "        squeezeNetModel.layers[i].trainable = False\n",
    "\n",
    "    #=====================================\n",
    "    # Add new classification layers\n",
    "    #=====================================\n",
    "\n",
    "    #--- Removing layers until drop 9\n",
    "    squeezeNetModel.layers.pop() #Convolution2D\n",
    "    squeezeNetModel.layers.pop() #Activation ReLU\n",
    "    squeezeNetModel.layers.pop() #Global Avg Pool\n",
    "    squeezeNetModel.layers.pop() #Activation Softmax\n",
    "\n",
    "    #--- Adding classification layer for 10 classes\n",
    "    out = Convolution2D(n_classes, (1, 1), padding='valid', name='conv10')(squeezeNetModel.layers[-1].output)\n",
    "    out = Activation('relu', name='relu_conv10')(out)\n",
    "\n",
    "    out = GlobalAveragePooling2D()(out)\n",
    "    out = Activation('softmax', name='loss')(out)\n",
    "\n",
    "    #=====================================\n",
    "    # New Model\n",
    "    #=====================================\n",
    "    model = Model(squeezeNetModel.inputs, out, name='squeezenet_new')\n",
    "    opt = optimizer=optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)#SGD(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_squeezenet_ft2()\n",
    "model.summary()\n",
    "\n",
    "#--- Check the trainable status of the individual layers\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Split: 0\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 8s 199us/step - loss: 2.3657 - acc: 0.1563 - val_loss: 2.1477 - val_acc: 0.2350\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 7s 184us/step - loss: 2.1195 - acc: 0.2282 - val_loss: 1.9936 - val_acc: 0.3222\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 2.0531 - acc: 0.2727 - val_loss: 1.8918 - val_acc: 0.3797\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.9549 - acc: 0.3215 - val_loss: 1.7618 - val_acc: 0.4315\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.8505 - acc: 0.3544 - val_loss: 1.6496 - val_acc: 0.4389\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.7884 - acc: 0.3707 - val_loss: 1.6109 - val_acc: 0.4553\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 1.7457 - acc: 0.3842 - val_loss: 1.5789 - val_acc: 0.4615\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.7069 - acc: 0.3946 - val_loss: 1.5430 - val_acc: 0.4700\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.6764 - acc: 0.4077 - val_loss: 1.5324 - val_acc: 0.4717\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.6468 - acc: 0.4233 - val_loss: 1.5133 - val_acc: 0.4796\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 1.6233 - acc: 0.4325 - val_loss: 1.5036 - val_acc: 0.4737\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.5975 - acc: 0.4448 - val_loss: 1.4741 - val_acc: 0.4925\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.5766 - acc: 0.4521 - val_loss: 1.4810 - val_acc: 0.4836\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 1.5618 - acc: 0.4583 - val_loss: 1.4664 - val_acc: 0.4913\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.5399 - acc: 0.4651 - val_loss: 1.4561 - val_acc: 0.4921\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 1.5177 - acc: 0.4746 - val_loss: 1.4594 - val_acc: 0.4930\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.5078 - acc: 0.4806 - val_loss: 1.4490 - val_acc: 0.4982\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.4913 - acc: 0.4841 - val_loss: 1.4419 - val_acc: 0.5008\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4770 - acc: 0.4905 - val_loss: 1.4317 - val_acc: 0.5038\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.4647 - acc: 0.4951 - val_loss: 1.4256 - val_acc: 0.5092\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.4483 - acc: 0.5015 - val_loss: 1.4229 - val_acc: 0.5106\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4352 - acc: 0.5074 - val_loss: 1.4165 - val_acc: 0.5098\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.4247 - acc: 0.5085 - val_loss: 1.4190 - val_acc: 0.5068\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 7s 181us/step - loss: 1.4095 - acc: 0.5142 - val_loss: 1.4250 - val_acc: 0.5110\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 1.3997 - acc: 0.5188 - val_loss: 1.4101 - val_acc: 0.5113\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 7s 179us/step - loss: 1.3855 - acc: 0.5225 - val_loss: 1.4150 - val_acc: 0.5084\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.3766 - acc: 0.5252 - val_loss: 1.4073 - val_acc: 0.5135\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.3651 - acc: 0.5296 - val_loss: 1.4240 - val_acc: 0.5088\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.3485 - acc: 0.5326 - val_loss: 1.4127 - val_acc: 0.5092\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.3418 - acc: 0.5374 - val_loss: 1.4159 - val_acc: 0.5104\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.3300 - acc: 0.5402 - val_loss: 1.4079 - val_acc: 0.5193\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3178 - acc: 0.5427 - val_loss: 1.4123 - val_acc: 0.5168\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3084 - acc: 0.5448 - val_loss: 1.4110 - val_acc: 0.5150\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2996 - acc: 0.5492 - val_loss: 1.4403 - val_acc: 0.5155\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2889 - acc: 0.5514 - val_loss: 1.4213 - val_acc: 0.5150\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.2756 - acc: 0.5530 - val_loss: 1.4218 - val_acc: 0.5134\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2683 - acc: 0.5584 - val_loss: 1.4272 - val_acc: 0.5161\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2607 - acc: 0.5591 - val_loss: 1.4391 - val_acc: 0.5105\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2482 - acc: 0.5612 - val_loss: 1.4264 - val_acc: 0.5142\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 1.2379 - acc: 0.5648 - val_loss: 1.4399 - val_acc: 0.5127\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 7s 183us/step - loss: 1.2276 - acc: 0.5702 - val_loss: 1.4386 - val_acc: 0.5173\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 7s 177us/step - loss: 1.2206 - acc: 0.5707 - val_loss: 1.4536 - val_acc: 0.5181\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.2083 - acc: 0.5739 - val_loss: 1.4524 - val_acc: 0.5168\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.1957 - acc: 0.5801 - val_loss: 1.4432 - val_acc: 0.5199\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.1910 - acc: 0.5781 - val_loss: 1.4571 - val_acc: 0.5147\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 6s 159us/step - loss: 1.1808 - acc: 0.5851 - val_loss: 1.4563 - val_acc: 0.5155\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1739 - acc: 0.5860 - val_loss: 1.4837 - val_acc: 0.5155\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.1633 - acc: 0.5898 - val_loss: 1.4881 - val_acc: 0.5157\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 1.1512 - acc: 0.5918 - val_loss: 1.4857 - val_acc: 0.5187\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 6s 162us/step - loss: 1.1390 - acc: 0.5927 - val_loss: 1.4668 - val_acc: 0.5140\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 6s 160us/step - loss: 1.1348 - acc: 0.5956 - val_loss: 1.5095 - val_acc: 0.5073\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 6s 161us/step - loss: 1.1242 - acc: 0.5995 - val_loss: 1.5080 - val_acc: 0.5165\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.1151 - acc: 0.6019 - val_loss: 1.5260 - val_acc: 0.5181\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1079 - acc: 0.6056 - val_loss: 1.4969 - val_acc: 0.5128\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 1.1000 - acc: 0.6099 - val_loss: 1.5495 - val_acc: 0.5159\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.0875 - acc: 0.6105 - val_loss: 1.5194 - val_acc: 0.5102\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.0788 - acc: 0.6160 - val_loss: 1.5150 - val_acc: 0.5127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0673 - acc: 0.6181 - val_loss: 1.5291 - val_acc: 0.5124\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.0608 - acc: 0.6196 - val_loss: 1.5571 - val_acc: 0.5182\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0516 - acc: 0.6214 - val_loss: 1.5548 - val_acc: 0.5149\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 7s 177us/step - loss: 1.0409 - acc: 0.6244 - val_loss: 1.5887 - val_acc: 0.5093\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.0350 - acc: 0.6271 - val_loss: 1.5692 - val_acc: 0.5141\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0254 - acc: 0.6320 - val_loss: 1.5882 - val_acc: 0.5154\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 1.0142 - acc: 0.6336 - val_loss: 1.6141 - val_acc: 0.5137\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 1.0131 - acc: 0.6337 - val_loss: 1.6335 - val_acc: 0.5113\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 7s 165us/step - loss: 1.0063 - acc: 0.6365 - val_loss: 1.6437 - val_acc: 0.5159\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.9941 - acc: 0.6394 - val_loss: 1.6623 - val_acc: 0.5118\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.9852 - acc: 0.6439 - val_loss: 1.6613 - val_acc: 0.5019\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 6s 162us/step - loss: 0.9776 - acc: 0.6468 - val_loss: 1.6756 - val_acc: 0.5095\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 0.9690 - acc: 0.6472 - val_loss: 1.6613 - val_acc: 0.5091\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.9610 - acc: 0.6531 - val_loss: 1.6725 - val_acc: 0.5050\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 6s 162us/step - loss: 0.9511 - acc: 0.6540 - val_loss: 1.6733 - val_acc: 0.5083\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 6s 162us/step - loss: 0.9470 - acc: 0.6554 - val_loss: 1.7078 - val_acc: 0.5062\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 0.9380 - acc: 0.6568 - val_loss: 1.6887 - val_acc: 0.5008\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 6s 160us/step - loss: 0.9260 - acc: 0.6631 - val_loss: 1.7164 - val_acc: 0.5094\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 6s 161us/step - loss: 0.9248 - acc: 0.6631 - val_loss: 1.7026 - val_acc: 0.4991\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 6s 161us/step - loss: 0.9146 - acc: 0.6690 - val_loss: 1.7386 - val_acc: 0.5030\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 0.9061 - acc: 0.6680 - val_loss: 1.7790 - val_acc: 0.5020\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 7s 163us/step - loss: 0.9021 - acc: 0.6720 - val_loss: 1.7948 - val_acc: 0.5045\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.8918 - acc: 0.6740 - val_loss: 1.7900 - val_acc: 0.5050\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 0.8819 - acc: 0.6774 - val_loss: 1.7490 - val_acc: 0.4968\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 7s 179us/step - loss: 0.8767 - acc: 0.6800 - val_loss: 1.8720 - val_acc: 0.4995\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 0.8716 - acc: 0.6822 - val_loss: 1.8208 - val_acc: 0.5011\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 0.8599 - acc: 0.6854 - val_loss: 1.8939 - val_acc: 0.5060\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.8589 - acc: 0.6852 - val_loss: 1.8528 - val_acc: 0.5047\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.8471 - acc: 0.6898 - val_loss: 1.8408 - val_acc: 0.4963\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 0.8429 - acc: 0.6897 - val_loss: 1.8636 - val_acc: 0.5043\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 0.8389 - acc: 0.6915 - val_loss: 1.8759 - val_acc: 0.4974\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8299 - acc: 0.6933 - val_loss: 1.9649 - val_acc: 0.4969\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 0.8254 - acc: 0.6951 - val_loss: 1.9618 - val_acc: 0.4980\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.8199 - acc: 0.6973 - val_loss: 1.9121 - val_acc: 0.4941\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.8100 - acc: 0.7018 - val_loss: 1.9602 - val_acc: 0.4970\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.8068 - acc: 0.7005 - val_loss: 1.9308 - val_acc: 0.4966\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.7983 - acc: 0.7065 - val_loss: 1.9999 - val_acc: 0.4962\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.7902 - acc: 0.7063 - val_loss: 1.9608 - val_acc: 0.4957\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.7858 - acc: 0.7069 - val_loss: 2.0397 - val_acc: 0.4951\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 0.7797 - acc: 0.7104 - val_loss: 2.0647 - val_acc: 0.4974\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 7s 167us/step - loss: 0.7773 - acc: 0.7152 - val_loss: 1.9809 - val_acc: 0.4965\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.7690 - acc: 0.7144 - val_loss: 2.1065 - val_acc: 0.4950\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 7s 166us/step - loss: 0.7582 - acc: 0.7200 - val_loss: 2.0585 - val_acc: 0.4937\n",
      "10000/10000 [==============================] - 2s 198us/step\n",
      "Validation loss: 2.0585452838242055\n",
      "Validation accuracy (NORMALIZED): 0.49370000678300857\n",
      "================================================\n",
      "Split: 1\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 8s 204us/step - loss: 2.3974 - acc: 0.1593 - val_loss: 1.9914 - val_acc: 0.3206\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 2.0475 - acc: 0.2529 - val_loss: 1.8458 - val_acc: 0.3751\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.9572 - acc: 0.2942 - val_loss: 1.7959 - val_acc: 0.4070\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.8674 - acc: 0.3378 - val_loss: 1.7013 - val_acc: 0.4270\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.7886 - acc: 0.3655 - val_loss: 1.6304 - val_acc: 0.4419\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.7309 - acc: 0.3854 - val_loss: 1.5825 - val_acc: 0.4483\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.6904 - acc: 0.4060 - val_loss: 1.5581 - val_acc: 0.4589\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.6467 - acc: 0.4203 - val_loss: 1.5412 - val_acc: 0.4646\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.6287 - acc: 0.4315 - val_loss: 1.5277 - val_acc: 0.4654\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.6087 - acc: 0.4393 - val_loss: 1.5148 - val_acc: 0.4690\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.5900 - acc: 0.4479 - val_loss: 1.5192 - val_acc: 0.4733\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.5683 - acc: 0.4565 - val_loss: 1.4931 - val_acc: 0.4793\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.5508 - acc: 0.4636 - val_loss: 1.4805 - val_acc: 0.4894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.5318 - acc: 0.4700 - val_loss: 1.4710 - val_acc: 0.4922\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.5210 - acc: 0.4749 - val_loss: 1.4678 - val_acc: 0.4900\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.5017 - acc: 0.4800 - val_loss: 1.4590 - val_acc: 0.4970\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4825 - acc: 0.4881 - val_loss: 1.4547 - val_acc: 0.4987\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4667 - acc: 0.4936 - val_loss: 1.4573 - val_acc: 0.4913\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4584 - acc: 0.4994 - val_loss: 1.4581 - val_acc: 0.4953\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4440 - acc: 0.5003 - val_loss: 1.4391 - val_acc: 0.4997\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4332 - acc: 0.5039 - val_loss: 1.4407 - val_acc: 0.5001\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4157 - acc: 0.5138 - val_loss: 1.4502 - val_acc: 0.4982\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.4047 - acc: 0.5146 - val_loss: 1.4374 - val_acc: 0.5037\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3949 - acc: 0.5191 - val_loss: 1.4310 - val_acc: 0.4998\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3804 - acc: 0.5247 - val_loss: 1.4402 - val_acc: 0.5066\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3685 - acc: 0.5249 - val_loss: 1.4317 - val_acc: 0.5061\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3633 - acc: 0.5297 - val_loss: 1.4268 - val_acc: 0.5054\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3508 - acc: 0.5313 - val_loss: 1.4409 - val_acc: 0.5080\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3399 - acc: 0.5345 - val_loss: 1.4266 - val_acc: 0.5049\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3271 - acc: 0.5397 - val_loss: 1.4370 - val_acc: 0.5077\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3155 - acc: 0.5421 - val_loss: 1.4348 - val_acc: 0.5114\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3050 - acc: 0.5455 - val_loss: 1.4487 - val_acc: 0.5098\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2950 - acc: 0.5492 - val_loss: 1.4473 - val_acc: 0.5075\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2908 - acc: 0.5467 - val_loss: 1.4572 - val_acc: 0.5005\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2812 - acc: 0.5515 - val_loss: 1.4336 - val_acc: 0.5095\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.2671 - acc: 0.5554 - val_loss: 1.4452 - val_acc: 0.5067\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2591 - acc: 0.5573 - val_loss: 1.4430 - val_acc: 0.5089\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.2476 - acc: 0.5588 - val_loss: 1.4483 - val_acc: 0.5120\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2373 - acc: 0.5658 - val_loss: 1.4595 - val_acc: 0.5073\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.2274 - acc: 0.5661 - val_loss: 1.4687 - val_acc: 0.5051\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2211 - acc: 0.5706 - val_loss: 1.4526 - val_acc: 0.5090\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.2090 - acc: 0.5728 - val_loss: 1.4592 - val_acc: 0.5043\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2006 - acc: 0.5774 - val_loss: 1.4771 - val_acc: 0.5054\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1907 - acc: 0.5781 - val_loss: 1.4895 - val_acc: 0.5072\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1820 - acc: 0.5806 - val_loss: 1.4903 - val_acc: 0.5125\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1733 - acc: 0.5824 - val_loss: 1.4828 - val_acc: 0.5069\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1623 - acc: 0.5862 - val_loss: 1.5053 - val_acc: 0.5057\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1595 - acc: 0.5869 - val_loss: 1.4863 - val_acc: 0.5076\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1437 - acc: 0.5907 - val_loss: 1.5032 - val_acc: 0.5114\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1356 - acc: 0.5945 - val_loss: 1.5502 - val_acc: 0.5099\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1249 - acc: 0.5981 - val_loss: 1.5151 - val_acc: 0.5092\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1178 - acc: 0.6008 - val_loss: 1.5196 - val_acc: 0.5049\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1107 - acc: 0.6039 - val_loss: 1.5692 - val_acc: 0.5055\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1006 - acc: 0.6048 - val_loss: 1.5714 - val_acc: 0.5062\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0856 - acc: 0.6063 - val_loss: 1.6059 - val_acc: 0.5047\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0822 - acc: 0.6116 - val_loss: 1.5975 - val_acc: 0.5040\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0767 - acc: 0.6122 - val_loss: 1.5969 - val_acc: 0.5012\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0638 - acc: 0.6139 - val_loss: 1.5930 - val_acc: 0.5060\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0568 - acc: 0.6174 - val_loss: 1.5921 - val_acc: 0.5014\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0491 - acc: 0.6218 - val_loss: 1.6179 - val_acc: 0.5030\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0381 - acc: 0.6237 - val_loss: 1.6303 - val_acc: 0.5089\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0271 - acc: 0.6253 - val_loss: 1.7057 - val_acc: 0.4955\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0233 - acc: 0.6292 - val_loss: 1.6373 - val_acc: 0.4944\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0141 - acc: 0.6311 - val_loss: 1.6472 - val_acc: 0.4983\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0071 - acc: 0.6348 - val_loss: 1.6614 - val_acc: 0.5055\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9969 - acc: 0.6385 - val_loss: 1.7192 - val_acc: 0.5016\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9944 - acc: 0.6403 - val_loss: 1.7035 - val_acc: 0.4993\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9843 - acc: 0.6433 - val_loss: 1.6487 - val_acc: 0.5028\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9725 - acc: 0.6455 - val_loss: 1.7158 - val_acc: 0.4989\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9663 - acc: 0.6458 - val_loss: 1.7733 - val_acc: 0.5018\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9614 - acc: 0.6502 - val_loss: 1.7703 - val_acc: 0.5006\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9527 - acc: 0.6529 - val_loss: 1.7852 - val_acc: 0.5007\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9448 - acc: 0.6534 - val_loss: 1.7584 - val_acc: 0.4983\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9382 - acc: 0.6572 - val_loss: 1.7767 - val_acc: 0.4936\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9283 - acc: 0.6595 - val_loss: 1.7854 - val_acc: 0.4966\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9251 - acc: 0.6585 - val_loss: 1.7837 - val_acc: 0.4976\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9147 - acc: 0.6634 - val_loss: 1.7882 - val_acc: 0.4931\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9104 - acc: 0.6655 - val_loss: 1.8771 - val_acc: 0.4969\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9040 - acc: 0.6673 - val_loss: 1.8823 - val_acc: 0.4874\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8936 - acc: 0.6690 - val_loss: 1.8882 - val_acc: 0.4905\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8844 - acc: 0.6718 - val_loss: 1.8590 - val_acc: 0.4918\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8763 - acc: 0.6728 - val_loss: 1.9247 - val_acc: 0.4897\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8729 - acc: 0.6790 - val_loss: 1.9635 - val_acc: 0.4953\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8668 - acc: 0.6794 - val_loss: 1.9380 - val_acc: 0.4903\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8595 - acc: 0.6808 - val_loss: 1.9070 - val_acc: 0.4931\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8501 - acc: 0.6842 - val_loss: 1.8537 - val_acc: 0.4893\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8490 - acc: 0.6853 - val_loss: 1.9412 - val_acc: 0.4916\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8432 - acc: 0.6873 - val_loss: 1.9661 - val_acc: 0.4906\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8347 - acc: 0.6913 - val_loss: 2.0146 - val_acc: 0.4876\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8320 - acc: 0.6889 - val_loss: 2.0077 - val_acc: 0.4888\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8242 - acc: 0.6912 - val_loss: 2.0343 - val_acc: 0.4832\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8152 - acc: 0.6976 - val_loss: 1.9977 - val_acc: 0.4872\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8082 - acc: 0.6970 - val_loss: 2.0844 - val_acc: 0.4879\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8017 - acc: 0.7000 - val_loss: 2.2221 - val_acc: 0.4865\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.7973 - acc: 0.7007 - val_loss: 2.1016 - val_acc: 0.4928\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7948 - acc: 0.7044 - val_loss: 2.1564 - val_acc: 0.4886\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7824 - acc: 0.7071 - val_loss: 2.1635 - val_acc: 0.4904\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7823 - acc: 0.7091 - val_loss: 2.1534 - val_acc: 0.4920\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7756 - acc: 0.7096 - val_loss: 2.1199 - val_acc: 0.4845\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7706 - acc: 0.7121 - val_loss: 2.1327 - val_acc: 0.4882\n",
      "10000/10000 [==============================] - 2s 198us/step\n",
      "Validation loss: 2.1327066971957684\n",
      "Validation accuracy (NORMALIZED): 0.4882000065147877\n",
      "================================================\n",
      "Split: 2\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 8s 204us/step - loss: 2.3866 - acc: 0.1740 - val_loss: 2.0288 - val_acc: 0.2936\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 2.0886 - acc: 0.2505 - val_loss: 1.9260 - val_acc: 0.3513\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.9841 - acc: 0.3026 - val_loss: 1.7207 - val_acc: 0.4109\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.8368 - acc: 0.3482 - val_loss: 1.6068 - val_acc: 0.4506\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.7701 - acc: 0.3746 - val_loss: 1.5767 - val_acc: 0.4489\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.7224 - acc: 0.3895 - val_loss: 1.5402 - val_acc: 0.4668\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.6917 - acc: 0.4019 - val_loss: 1.5334 - val_acc: 0.4679\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.6630 - acc: 0.4128 - val_loss: 1.5217 - val_acc: 0.4680\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.6373 - acc: 0.4217 - val_loss: 1.5014 - val_acc: 0.4797\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.6123 - acc: 0.4382 - val_loss: 1.5011 - val_acc: 0.4827\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.5864 - acc: 0.4494 - val_loss: 1.4781 - val_acc: 0.4895\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.5678 - acc: 0.4534 - val_loss: 1.4760 - val_acc: 0.4868\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.5435 - acc: 0.4640 - val_loss: 1.4614 - val_acc: 0.4887\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.5267 - acc: 0.4694 - val_loss: 1.4567 - val_acc: 0.4875\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.5115 - acc: 0.4783 - val_loss: 1.4399 - val_acc: 0.5010\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4958 - acc: 0.4826 - val_loss: 1.4347 - val_acc: 0.5022\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4850 - acc: 0.4870 - val_loss: 1.4482 - val_acc: 0.4969\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4677 - acc: 0.4910 - val_loss: 1.4258 - val_acc: 0.5021\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4452 - acc: 0.4988 - val_loss: 1.4289 - val_acc: 0.5050\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4372 - acc: 0.5031 - val_loss: 1.4289 - val_acc: 0.5068\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4243 - acc: 0.5087 - val_loss: 1.4175 - val_acc: 0.5076\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.4139 - acc: 0.5104 - val_loss: 1.4095 - val_acc: 0.5100\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3960 - acc: 0.5176 - val_loss: 1.4142 - val_acc: 0.5064\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3890 - acc: 0.5196 - val_loss: 1.4098 - val_acc: 0.5111\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3717 - acc: 0.5262 - val_loss: 1.4154 - val_acc: 0.5131\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3674 - acc: 0.5259 - val_loss: 1.4143 - val_acc: 0.5126\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3500 - acc: 0.5330 - val_loss: 1.4190 - val_acc: 0.5128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.3389 - acc: 0.5340 - val_loss: 1.4270 - val_acc: 0.5124\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3254 - acc: 0.5378 - val_loss: 1.4135 - val_acc: 0.5121\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3155 - acc: 0.5413 - val_loss: 1.4140 - val_acc: 0.5165\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.3039 - acc: 0.5456 - val_loss: 1.4176 - val_acc: 0.5174\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2974 - acc: 0.5466 - val_loss: 1.4132 - val_acc: 0.5151\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2859 - acc: 0.5520 - val_loss: 1.4390 - val_acc: 0.5125\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2729 - acc: 0.5549 - val_loss: 1.4183 - val_acc: 0.5155\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2640 - acc: 0.5581 - val_loss: 1.4229 - val_acc: 0.5127\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2533 - acc: 0.5615 - val_loss: 1.4416 - val_acc: 0.5130\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.2458 - acc: 0.5609 - val_loss: 1.4296 - val_acc: 0.5151\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.2300 - acc: 0.5678 - val_loss: 1.4276 - val_acc: 0.5137\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2215 - acc: 0.5704 - val_loss: 1.4417 - val_acc: 0.5144\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2113 - acc: 0.5722 - val_loss: 1.4501 - val_acc: 0.5141\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.2087 - acc: 0.5753 - val_loss: 1.4652 - val_acc: 0.5069\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1986 - acc: 0.5758 - val_loss: 1.4490 - val_acc: 0.5150\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1811 - acc: 0.5806 - val_loss: 1.4592 - val_acc: 0.5122\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1776 - acc: 0.5837 - val_loss: 1.4567 - val_acc: 0.5072\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1614 - acc: 0.5874 - val_loss: 1.4732 - val_acc: 0.5071\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1541 - acc: 0.5924 - val_loss: 1.4860 - val_acc: 0.5144\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1435 - acc: 0.5956 - val_loss: 1.4806 - val_acc: 0.5115\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1404 - acc: 0.5959 - val_loss: 1.4824 - val_acc: 0.5145\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1231 - acc: 0.5972 - val_loss: 1.5050 - val_acc: 0.5110\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.1150 - acc: 0.6009 - val_loss: 1.5597 - val_acc: 0.5057\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1119 - acc: 0.6025 - val_loss: 1.5081 - val_acc: 0.5073\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.1006 - acc: 0.6048 - val_loss: 1.5367 - val_acc: 0.5115\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0901 - acc: 0.6074 - val_loss: 1.5546 - val_acc: 0.5057\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0818 - acc: 0.6088 - val_loss: 1.5360 - val_acc: 0.5090\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0697 - acc: 0.6132 - val_loss: 1.5653 - val_acc: 0.5078\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0623 - acc: 0.6166 - val_loss: 1.5699 - val_acc: 0.5088\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0549 - acc: 0.6200 - val_loss: 1.5650 - val_acc: 0.5075\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0442 - acc: 0.6230 - val_loss: 1.5903 - val_acc: 0.5040\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0367 - acc: 0.6245 - val_loss: 1.6057 - val_acc: 0.5086\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0269 - acc: 0.6274 - val_loss: 1.6239 - val_acc: 0.5013\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 1.0167 - acc: 0.6300 - val_loss: 1.6207 - val_acc: 0.5048\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0070 - acc: 0.6350 - val_loss: 1.6414 - val_acc: 0.5020\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 1.0027 - acc: 0.6358 - val_loss: 1.6594 - val_acc: 0.5047\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9914 - acc: 0.6409 - val_loss: 1.6389 - val_acc: 0.5070\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9854 - acc: 0.6411 - val_loss: 1.6740 - val_acc: 0.5060\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9748 - acc: 0.6432 - val_loss: 1.6720 - val_acc: 0.5016\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9660 - acc: 0.6454 - val_loss: 1.6908 - val_acc: 0.5054\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9555 - acc: 0.6514 - val_loss: 1.7283 - val_acc: 0.5023\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9461 - acc: 0.6530 - val_loss: 1.7202 - val_acc: 0.5030\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9356 - acc: 0.6585 - val_loss: 1.7713 - val_acc: 0.5040\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.9325 - acc: 0.6592 - val_loss: 1.7717 - val_acc: 0.4998\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9241 - acc: 0.6626 - val_loss: 1.7763 - val_acc: 0.5005\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9180 - acc: 0.6626 - val_loss: 1.8037 - val_acc: 0.5048\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9084 - acc: 0.6643 - val_loss: 1.7740 - val_acc: 0.4982\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.9028 - acc: 0.6678 - val_loss: 1.8077 - val_acc: 0.5040\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8939 - acc: 0.6696 - val_loss: 1.8057 - val_acc: 0.4982\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8847 - acc: 0.6714 - val_loss: 1.8059 - val_acc: 0.5007\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8771 - acc: 0.6778 - val_loss: 1.8387 - val_acc: 0.5003\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8711 - acc: 0.6771 - val_loss: 1.8841 - val_acc: 0.4993\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.8667 - acc: 0.6790 - val_loss: 1.8783 - val_acc: 0.4895\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8563 - acc: 0.6832 - val_loss: 1.9368 - val_acc: 0.4973\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8461 - acc: 0.6863 - val_loss: 1.8977 - val_acc: 0.4923\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8458 - acc: 0.6855 - val_loss: 1.9487 - val_acc: 0.4950\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.8353 - acc: 0.6885 - val_loss: 1.9450 - val_acc: 0.4969\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8326 - acc: 0.6929 - val_loss: 1.9634 - val_acc: 0.4929\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.8213 - acc: 0.6942 - val_loss: 2.0166 - val_acc: 0.4966\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8145 - acc: 0.6959 - val_loss: 2.0113 - val_acc: 0.4986\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8076 - acc: 0.6986 - val_loss: 1.9833 - val_acc: 0.4878\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.8053 - acc: 0.7020 - val_loss: 2.0670 - val_acc: 0.4923\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7983 - acc: 0.7023 - val_loss: 2.0581 - val_acc: 0.4966\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 7s 168us/step - loss: 0.7906 - acc: 0.7057 - val_loss: 2.1196 - val_acc: 0.4966\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.7830 - acc: 0.7079 - val_loss: 2.1470 - val_acc: 0.4928\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.7762 - acc: 0.7078 - val_loss: 2.0106 - val_acc: 0.4901\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.7695 - acc: 0.7146 - val_loss: 2.1371 - val_acc: 0.4909\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 0.7656 - acc: 0.7130 - val_loss: 2.1757 - val_acc: 0.4895\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 7s 178us/step - loss: 0.7579 - acc: 0.7167 - val_loss: 2.1778 - val_acc: 0.4930\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 7s 184us/step - loss: 0.7479 - acc: 0.7188 - val_loss: 2.1679 - val_acc: 0.4939\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 7s 183us/step - loss: 0.7463 - acc: 0.7204 - val_loss: 2.2668 - val_acc: 0.4910\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 0.7383 - acc: 0.7224 - val_loss: 2.2475 - val_acc: 0.4914\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 0.7339 - acc: 0.7245 - val_loss: 2.2839 - val_acc: 0.4865\n",
      "10000/10000 [==============================] - 2s 215us/step\n",
      "Validation loss: 2.2839359016120433\n",
      "Validation accuracy (NORMALIZED): 0.4865000063031912\n",
      "================================================\n",
      "Split: 3\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 9s 228us/step - loss: 2.3529 - acc: 0.1860 - val_loss: 1.9272 - val_acc: 0.3197\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 1.9755 - acc: 0.2702 - val_loss: 1.7607 - val_acc: 0.3809\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.8790 - acc: 0.3171 - val_loss: 1.6702 - val_acc: 0.4125\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 7s 178us/step - loss: 1.8112 - acc: 0.3472 - val_loss: 1.6190 - val_acc: 0.4328\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 7s 186us/step - loss: 1.7659 - acc: 0.3683 - val_loss: 1.6015 - val_acc: 0.4486\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 1.7163 - acc: 0.3910 - val_loss: 1.5827 - val_acc: 0.4514\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 7s 184us/step - loss: 1.6804 - acc: 0.4052 - val_loss: 1.5396 - val_acc: 0.4707\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 7s 177us/step - loss: 1.6517 - acc: 0.4174 - val_loss: 1.5399 - val_acc: 0.4646\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.6281 - acc: 0.4295 - val_loss: 1.5094 - val_acc: 0.4703\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 7s 186us/step - loss: 1.6088 - acc: 0.4371 - val_loss: 1.4895 - val_acc: 0.4786\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 8s 189us/step - loss: 1.5868 - acc: 0.4435 - val_loss: 1.4836 - val_acc: 0.4852\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 8s 190us/step - loss: 1.5748 - acc: 0.4512 - val_loss: 1.4696 - val_acc: 0.4885\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 7s 179us/step - loss: 1.5500 - acc: 0.4631 - val_loss: 1.4695 - val_acc: 0.4934\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 7s 177us/step - loss: 1.5291 - acc: 0.4706 - val_loss: 1.4691 - val_acc: 0.4920\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 7s 178us/step - loss: 1.5148 - acc: 0.4732 - val_loss: 1.4503 - val_acc: 0.4947\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 1.4979 - acc: 0.4794 - val_loss: 1.4443 - val_acc: 0.4980\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4825 - acc: 0.4874 - val_loss: 1.4396 - val_acc: 0.5001\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4678 - acc: 0.4883 - val_loss: 1.4409 - val_acc: 0.5017\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4526 - acc: 0.4924 - val_loss: 1.4316 - val_acc: 0.5039\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4427 - acc: 0.4981 - val_loss: 1.4356 - val_acc: 0.5014\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4309 - acc: 0.5039 - val_loss: 1.4181 - val_acc: 0.5050\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4169 - acc: 0.5061 - val_loss: 1.4212 - val_acc: 0.5052\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.4046 - acc: 0.5095 - val_loss: 1.4342 - val_acc: 0.4999\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.3935 - acc: 0.5136 - val_loss: 1.4212 - val_acc: 0.5073\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.3813 - acc: 0.5191 - val_loss: 1.4126 - val_acc: 0.5091\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.3700 - acc: 0.5210 - val_loss: 1.4139 - val_acc: 0.5076\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.3605 - acc: 0.5264 - val_loss: 1.4238 - val_acc: 0.5066\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.3485 - acc: 0.5280 - val_loss: 1.4156 - val_acc: 0.5112\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.3368 - acc: 0.5324 - val_loss: 1.4195 - val_acc: 0.5120\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 7s 176us/step - loss: 1.3273 - acc: 0.5363 - val_loss: 1.4057 - val_acc: 0.5098\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 7s 175us/step - loss: 1.3176 - acc: 0.5384 - val_loss: 1.4298 - val_acc: 0.5125\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 7s 180us/step - loss: 1.3099 - acc: 0.5390 - val_loss: 1.4064 - val_acc: 0.5133\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 1.2953 - acc: 0.5481 - val_loss: 1.4055 - val_acc: 0.5124\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 8s 188us/step - loss: 1.2871 - acc: 0.5476 - val_loss: 1.4148 - val_acc: 0.5160\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.2788 - acc: 0.5504 - val_loss: 1.4345 - val_acc: 0.5154\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 8s 200us/step - loss: 1.2698 - acc: 0.5539 - val_loss: 1.4173 - val_acc: 0.5150\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 1.2582 - acc: 0.5569 - val_loss: 1.4331 - val_acc: 0.5133\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 1.2464 - acc: 0.5572 - val_loss: 1.4372 - val_acc: 0.5146\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 1.2416 - acc: 0.5629 - val_loss: 1.4323 - val_acc: 0.5141\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 8s 188us/step - loss: 1.2283 - acc: 0.5646 - val_loss: 1.4568 - val_acc: 0.5168\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 1.2202 - acc: 0.5669 - val_loss: 1.4357 - val_acc: 0.5158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 1.2094 - acc: 0.5692 - val_loss: 1.4554 - val_acc: 0.5193\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 8s 195us/step - loss: 1.2061 - acc: 0.5729 - val_loss: 1.4357 - val_acc: 0.5187\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 8s 198us/step - loss: 1.1922 - acc: 0.5742 - val_loss: 1.4315 - val_acc: 0.5132\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 1.1829 - acc: 0.5762 - val_loss: 1.4692 - val_acc: 0.5138\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 8s 206us/step - loss: 1.1725 - acc: 0.5817 - val_loss: 1.4619 - val_acc: 0.5203\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 8s 201us/step - loss: 1.1609 - acc: 0.5858 - val_loss: 1.4681 - val_acc: 0.5166\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 8s 205us/step - loss: 1.1561 - acc: 0.5873 - val_loss: 1.4614 - val_acc: 0.5166\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 8s 195us/step - loss: 1.1431 - acc: 0.5891 - val_loss: 1.4860 - val_acc: 0.5149\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 8s 205us/step - loss: 1.1380 - acc: 0.5920 - val_loss: 1.4692 - val_acc: 0.5153\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 8s 212us/step - loss: 1.1258 - acc: 0.5949 - val_loss: 1.5169 - val_acc: 0.5156\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 8s 202us/step - loss: 1.1156 - acc: 0.5998 - val_loss: 1.4984 - val_acc: 0.5100\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 8s 199us/step - loss: 1.1104 - acc: 0.6006 - val_loss: 1.5043 - val_acc: 0.5190\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 8s 198us/step - loss: 1.0995 - acc: 0.6026 - val_loss: 1.5012 - val_acc: 0.5106\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 8s 201us/step - loss: 1.0922 - acc: 0.6064 - val_loss: 1.5023 - val_acc: 0.5168\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 8s 208us/step - loss: 1.0833 - acc: 0.6087 - val_loss: 1.5153 - val_acc: 0.5110\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 8s 203us/step - loss: 1.0729 - acc: 0.6114 - val_loss: 1.5378 - val_acc: 0.5166\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 8s 194us/step - loss: 1.0642 - acc: 0.6145 - val_loss: 1.5367 - val_acc: 0.5148\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.0581 - acc: 0.6174 - val_loss: 1.5799 - val_acc: 0.5186\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.0485 - acc: 0.6191 - val_loss: 1.5806 - val_acc: 0.5121\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 1.0377 - acc: 0.6232 - val_loss: 1.5914 - val_acc: 0.5089\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 7s 178us/step - loss: 1.0321 - acc: 0.6226 - val_loss: 1.5885 - val_acc: 0.5123\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 9s 214us/step - loss: 1.0201 - acc: 0.6285 - val_loss: 1.6585 - val_acc: 0.5161\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 8s 201us/step - loss: 1.0141 - acc: 0.6306 - val_loss: 1.6128 - val_acc: 0.5103\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 8s 188us/step - loss: 1.0055 - acc: 0.6316 - val_loss: 1.6024 - val_acc: 0.5094\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 0.9947 - acc: 0.6373 - val_loss: 1.5848 - val_acc: 0.5074\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 8s 199us/step - loss: 0.9881 - acc: 0.6412 - val_loss: 1.6988 - val_acc: 0.5075\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 9s 216us/step - loss: 0.9817 - acc: 0.6395 - val_loss: 1.6712 - val_acc: 0.5131\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 9s 215us/step - loss: 0.9724 - acc: 0.6420 - val_loss: 1.6357 - val_acc: 0.5053\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 8s 189us/step - loss: 0.9631 - acc: 0.6467 - val_loss: 1.6859 - val_acc: 0.5098\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 7s 186us/step - loss: 0.9554 - acc: 0.6502 - val_loss: 1.7286 - val_acc: 0.5095\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 7s 186us/step - loss: 0.9506 - acc: 0.6526 - val_loss: 1.7368 - val_acc: 0.5112\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 0.9428 - acc: 0.6538 - val_loss: 1.7235 - val_acc: 0.5054\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 0.9322 - acc: 0.6587 - val_loss: 1.7317 - val_acc: 0.5046\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 0.9242 - acc: 0.6606 - val_loss: 1.7545 - val_acc: 0.5042\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 7s 186us/step - loss: 0.9183 - acc: 0.6619 - val_loss: 1.7056 - val_acc: 0.5029\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 0.9087 - acc: 0.6652 - val_loss: 1.7912 - val_acc: 0.5058\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 8s 195us/step - loss: 0.9050 - acc: 0.6647 - val_loss: 1.7501 - val_acc: 0.5036\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 0.8972 - acc: 0.6684 - val_loss: 1.8114 - val_acc: 0.5000\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 0.8886 - acc: 0.6707 - val_loss: 1.8482 - val_acc: 0.4980\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 8s 194us/step - loss: 0.8797 - acc: 0.6768 - val_loss: 1.8619 - val_acc: 0.5032\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 8s 192us/step - loss: 0.8738 - acc: 0.6761 - val_loss: 1.8422 - val_acc: 0.5030\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 8s 202us/step - loss: 0.8669 - acc: 0.6812 - val_loss: 1.8756 - val_acc: 0.4997\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 8s 191us/step - loss: 0.8622 - acc: 0.6817 - val_loss: 1.8827 - val_acc: 0.4994\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 8s 206us/step - loss: 0.8494 - acc: 0.6840 - val_loss: 1.9178 - val_acc: 0.4999\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 9s 214us/step - loss: 0.8463 - acc: 0.6866 - val_loss: 1.9274 - val_acc: 0.4983\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 8s 203us/step - loss: 0.8405 - acc: 0.6874 - val_loss: 1.9594 - val_acc: 0.5034\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 0.8328 - acc: 0.6903 - val_loss: 1.9582 - val_acc: 0.4990\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 8s 190us/step - loss: 0.8311 - acc: 0.6910 - val_loss: 2.0288 - val_acc: 0.5023\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 8s 204us/step - loss: 0.8234 - acc: 0.6935 - val_loss: 1.9166 - val_acc: 0.4987\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 8s 207us/step - loss: 0.8124 - acc: 0.6958 - val_loss: 1.9811 - val_acc: 0.4994\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 8s 201us/step - loss: 0.8081 - acc: 0.6967 - val_loss: 2.0445 - val_acc: 0.4997\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 8s 198us/step - loss: 0.8022 - acc: 0.7017 - val_loss: 1.9681 - val_acc: 0.4972\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 0.7946 - acc: 0.7032 - val_loss: 1.9683 - val_acc: 0.4959\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 8s 189us/step - loss: 0.7899 - acc: 0.7056 - val_loss: 2.0146 - val_acc: 0.5001\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 7s 186us/step - loss: 0.7820 - acc: 0.7082 - val_loss: 2.0470 - val_acc: 0.4995\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 0.7757 - acc: 0.7102 - val_loss: 2.0688 - val_acc: 0.4930\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 0.7730 - acc: 0.7107 - val_loss: 2.0977 - val_acc: 0.4929\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 8s 191us/step - loss: 0.7658 - acc: 0.7139 - val_loss: 2.0685 - val_acc: 0.4928\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 0.7589 - acc: 0.7155 - val_loss: 2.1058 - val_acc: 0.4877\n",
      "10000/10000 [==============================] - 2s 209us/step\n",
      "Validation loss: 2.105846211463213\n",
      "Validation accuracy (NORMALIZED): 0.4877000060528517\n",
      "================================================\n",
      "Split: 4\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 9s 224us/step - loss: 2.4177 - acc: 0.1462 - val_loss: 2.1157 - val_acc: 0.2327\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 7s 187us/step - loss: 2.1487 - acc: 0.1877 - val_loss: 2.0329 - val_acc: 0.2609\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 8s 193us/step - loss: 2.0846 - acc: 0.2219 - val_loss: 1.9043 - val_acc: 0.3354\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 8s 188us/step - loss: 1.9888 - acc: 0.2789 - val_loss: 1.7964 - val_acc: 0.4013\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.9141 - acc: 0.2983 - val_loss: 1.7189 - val_acc: 0.4103\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 7s 182us/step - loss: 1.8584 - acc: 0.3129 - val_loss: 1.6476 - val_acc: 0.4313\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.8136 - acc: 0.3320 - val_loss: 1.6300 - val_acc: 0.4439\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.7833 - acc: 0.3499 - val_loss: 1.6190 - val_acc: 0.4452\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.7455 - acc: 0.3735 - val_loss: 1.5722 - val_acc: 0.4548\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.7108 - acc: 0.3898 - val_loss: 1.5466 - val_acc: 0.4588\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.6837 - acc: 0.4027 - val_loss: 1.5348 - val_acc: 0.4640\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 1.6473 - acc: 0.4235 - val_loss: 1.5109 - val_acc: 0.4741\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 7s 181us/step - loss: 1.6222 - acc: 0.4325 - val_loss: 1.5088 - val_acc: 0.4749\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.5916 - acc: 0.4436 - val_loss: 1.4976 - val_acc: 0.4812\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.5729 - acc: 0.4520 - val_loss: 1.4720 - val_acc: 0.4878\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.5499 - acc: 0.4620 - val_loss: 1.4698 - val_acc: 0.4882\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.5335 - acc: 0.4657 - val_loss: 1.4640 - val_acc: 0.4928\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.5205 - acc: 0.4723 - val_loss: 1.4602 - val_acc: 0.4971\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.5076 - acc: 0.4754 - val_loss: 1.4532 - val_acc: 0.4926\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.4902 - acc: 0.4848 - val_loss: 1.4363 - val_acc: 0.4980\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.4732 - acc: 0.4900 - val_loss: 1.4318 - val_acc: 0.5014\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.4597 - acc: 0.4933 - val_loss: 1.4411 - val_acc: 0.5030\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.4493 - acc: 0.4991 - val_loss: 1.4313 - val_acc: 0.5066\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.4349 - acc: 0.5022 - val_loss: 1.4244 - val_acc: 0.5097\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.4190 - acc: 0.5085 - val_loss: 1.4291 - val_acc: 0.5052\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.4060 - acc: 0.5140 - val_loss: 1.4262 - val_acc: 0.5059\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3935 - acc: 0.5168 - val_loss: 1.4254 - val_acc: 0.5099\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3800 - acc: 0.5200 - val_loss: 1.4276 - val_acc: 0.5043\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3672 - acc: 0.5257 - val_loss: 1.4328 - val_acc: 0.5056\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.3558 - acc: 0.5293 - val_loss: 1.4263 - val_acc: 0.5113\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.3430 - acc: 0.5314 - val_loss: 1.4088 - val_acc: 0.5161\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3315 - acc: 0.5388 - val_loss: 1.4200 - val_acc: 0.5051\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.3210 - acc: 0.5387 - val_loss: 1.4169 - val_acc: 0.5143\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.3142 - acc: 0.5430 - val_loss: 1.4399 - val_acc: 0.5145\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.3041 - acc: 0.5455 - val_loss: 1.4213 - val_acc: 0.5131\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2875 - acc: 0.5488 - val_loss: 1.4267 - val_acc: 0.5112\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2812 - acc: 0.5511 - val_loss: 1.4328 - val_acc: 0.5131\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2708 - acc: 0.5558 - val_loss: 1.4329 - val_acc: 0.5140\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.2602 - acc: 0.5559 - val_loss: 1.4356 - val_acc: 0.5140\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2471 - acc: 0.5624 - val_loss: 1.4376 - val_acc: 0.5136\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2384 - acc: 0.5649 - val_loss: 1.4424 - val_acc: 0.5128\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.2287 - acc: 0.5674 - val_loss: 1.4554 - val_acc: 0.5135\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2200 - acc: 0.5696 - val_loss: 1.4660 - val_acc: 0.5149\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.2115 - acc: 0.5712 - val_loss: 1.4732 - val_acc: 0.5167\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.1963 - acc: 0.5767 - val_loss: 1.4543 - val_acc: 0.5144\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.1824 - acc: 0.5793 - val_loss: 1.4858 - val_acc: 0.5120\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.1741 - acc: 0.5829 - val_loss: 1.4877 - val_acc: 0.5115\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 1.1672 - acc: 0.5866 - val_loss: 1.4735 - val_acc: 0.5128\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.1556 - acc: 0.5916 - val_loss: 1.4813 - val_acc: 0.5127\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.1447 - acc: 0.5909 - val_loss: 1.5145 - val_acc: 0.5131\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.1389 - acc: 0.5949 - val_loss: 1.5031 - val_acc: 0.5110\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 1.1284 - acc: 0.5963 - val_loss: 1.5259 - val_acc: 0.5049\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.1182 - acc: 0.6010 - val_loss: 1.5135 - val_acc: 0.5077\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.1067 - acc: 0.6037 - val_loss: 1.5498 - val_acc: 0.5097\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0982 - acc: 0.6048 - val_loss: 1.5396 - val_acc: 0.5090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0883 - acc: 0.6111 - val_loss: 1.5601 - val_acc: 0.5082\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 1.0801 - acc: 0.6126 - val_loss: 1.5351 - val_acc: 0.5076\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0712 - acc: 0.6147 - val_loss: 1.5663 - val_acc: 0.5062\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0613 - acc: 0.6176 - val_loss: 1.5740 - val_acc: 0.5056\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0540 - acc: 0.6221 - val_loss: 1.6195 - val_acc: 0.5032\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0446 - acc: 0.6217 - val_loss: 1.6082 - val_acc: 0.5057\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0356 - acc: 0.6272 - val_loss: 1.6135 - val_acc: 0.5043\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0295 - acc: 0.6279 - val_loss: 1.6600 - val_acc: 0.5010\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0183 - acc: 0.6317 - val_loss: 1.6285 - val_acc: 0.5020\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0108 - acc: 0.6363 - val_loss: 1.6485 - val_acc: 0.5002\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 1.0046 - acc: 0.6356 - val_loss: 1.6371 - val_acc: 0.5022\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9924 - acc: 0.6410 - val_loss: 1.6774 - val_acc: 0.5005\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.9865 - acc: 0.6414 - val_loss: 1.6930 - val_acc: 0.5004\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9762 - acc: 0.6446 - val_loss: 1.6927 - val_acc: 0.4985\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.9710 - acc: 0.6461 - val_loss: 1.7095 - val_acc: 0.5029\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.9639 - acc: 0.6494 - val_loss: 1.7678 - val_acc: 0.5040\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9509 - acc: 0.6540 - val_loss: 1.7945 - val_acc: 0.5013\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9435 - acc: 0.6562 - val_loss: 1.7360 - val_acc: 0.5028\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 0.9313 - acc: 0.6604 - val_loss: 1.7778 - val_acc: 0.5019\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9253 - acc: 0.6615 - val_loss: 1.8110 - val_acc: 0.4996\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9232 - acc: 0.6626 - val_loss: 1.8189 - val_acc: 0.4948\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9128 - acc: 0.6665 - val_loss: 1.8200 - val_acc: 0.4991\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.9043 - acc: 0.6673 - val_loss: 1.8216 - val_acc: 0.5014\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8981 - acc: 0.6699 - val_loss: 1.9099 - val_acc: 0.4980\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.8919 - acc: 0.6718 - val_loss: 1.8340 - val_acc: 0.4929\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8794 - acc: 0.6759 - val_loss: 1.9109 - val_acc: 0.4969\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8731 - acc: 0.6767 - val_loss: 1.8951 - val_acc: 0.4973\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8672 - acc: 0.6810 - val_loss: 1.9463 - val_acc: 0.4992\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8581 - acc: 0.6855 - val_loss: 1.8981 - val_acc: 0.4934\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8534 - acc: 0.6865 - val_loss: 1.9568 - val_acc: 0.4941\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8412 - acc: 0.6897 - val_loss: 1.8982 - val_acc: 0.4953\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8361 - acc: 0.6917 - val_loss: 1.9530 - val_acc: 0.4925\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8328 - acc: 0.6922 - val_loss: 1.9519 - val_acc: 0.4910\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8212 - acc: 0.6962 - val_loss: 1.9915 - val_acc: 0.4871\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.8161 - acc: 0.6966 - val_loss: 1.9996 - val_acc: 0.4919\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8162 - acc: 0.6991 - val_loss: 1.9972 - val_acc: 0.4887\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.8014 - acc: 0.7038 - val_loss: 2.0173 - val_acc: 0.4875\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 7s 185us/step - loss: 0.7938 - acc: 0.7071 - val_loss: 2.1120 - val_acc: 0.4938\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 7s 174us/step - loss: 0.7948 - acc: 0.7055 - val_loss: 2.0347 - val_acc: 0.4855\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.7849 - acc: 0.7092 - val_loss: 2.0678 - val_acc: 0.4875\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.7764 - acc: 0.7101 - val_loss: 2.1066 - val_acc: 0.4934\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.7723 - acc: 0.7145 - val_loss: 2.1096 - val_acc: 0.4922\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.7624 - acc: 0.7167 - val_loss: 2.1478 - val_acc: 0.4857\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.7623 - acc: 0.7155 - val_loss: 2.1506 - val_acc: 0.4834\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.7561 - acc: 0.7175 - val_loss: 2.1762 - val_acc: 0.4856\n",
      "10000/10000 [==============================] - 2s 204us/step\n",
      "Validation loss: 2.1761686961352824\n",
      "Validation accuracy (NORMALIZED): 0.48560000701248646\n",
      "[[2.0585452838242055, 0.49370000678300857], [2.1327066971957684, 0.4882000065147877], [2.2839359016120433, 0.4865000063031912], [2.105846211463213, 0.4877000060528517], [2.1761686961352824, 0.48560000701248646]]\n"
     ]
    }
   ],
   "source": [
    "#=====================================\n",
    "# Training model and Evaluation\n",
    "#=====================================\n",
    "\n",
    "y_train_categorical = to_categorical(y_train, num_classes=n_classes)\n",
    "y_val_categorical = to_categorical(y_val, num_classes=n_classes)\n",
    "\n",
    "scores = []\n",
    "for i in range(folds):\n",
    "    #--- Loading model\n",
    "    model = get_squeezenet_ft2()    \n",
    "    \n",
    "    #--- Evaluating the model for split i\n",
    "    print(\"================================================\")\n",
    "    print(\"Split: \"+str(i))\n",
    "    print(\"Number of Epochs: \"+str(n_epochs))\n",
    "    print(\"Train shape:\",X_train[i].shape)\n",
    "    print(\"Train batch size: \"+str(train_batch_size))\n",
    "    print(\"Val batch size: \"+str(val_batch_size))\n",
    "    print(\"================================================\")\n",
    "    \n",
    "    # Training without data augmentation\n",
    "    model.fit(X_train[i],y_train_categorical[i], batch_size=train_batch_size,\n",
    "              epochs=n_epochs, \n",
    "              validation_data=(X_val[i],y_val_categorical[i]),              \n",
    "              verbose=1)\n",
    "    \n",
    "    #--- Evaluating the model for split i\n",
    "    score = model.evaluate(x=X_val[i], y=y_val_categorical[i], batch_size=val_batch_size, verbose=1)\n",
    "    scores.append(score)\n",
    "    print('Validation loss:', score[0])\n",
    "    print('Validation accuracy (NORMALIZED):', score[1])\n",
    "    \n",
    "#--- Showing scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Validation loss: 2.1649977488458156\n",
      "Mean Validation accuracy (NORMALIZED): 0.4909800066113473\n"
     ]
    }
   ],
   "source": [
    "#=====================================\n",
    "# Evaluate on validation\n",
    "#=====================================\n",
    "\n",
    "#--- The evaluation of the model \n",
    "np_aux = np.array(scores).mean(axis=0)\n",
    "\n",
    "print('Mean Validation loss:', np_aux[0])\n",
    "print('Mean Validation accuracy (NORMALIZED):', np_aux[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----------\n",
    "-----------\n",
    "# Tensorboard\n",
    "\n",
    "Tensorboard is a visualization tool for Tensorflow. Among other things, it allows us to monitor the progress of our training, plot metrics per epochs, visualize the architecture's schematics. \n",
    "\n",
    "Just like for Early Stopping, we will use the [Tensorboard callback](https://keras.io/callbacks/#tensorboard) to log the information about our training. An example of usage, would be:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Just an example, DON'T RUN! \n",
    "### You will need to change <<LOG_DIR>>\n",
    "import keras.callbacks as callbacks\n",
    "tbCallBack = callbacks.TensorBoard(log_dir = \"./<<LOG_DIR>>\")\n",
    "model.fit(..., callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your training progresses, Keras will log the metrics (e.g., loss, accuracy) to `<<LOG_DIR>>` (**make sure `<<LOG_DIR>>` is a valid directory)**. On your terminal, you will need to run Tensorboard, assign a port and access it via browser (just like jupyter).\n",
    "\n",
    "#### ----> MAKE SURE YOU USE A DIFFERENT PORT FOR JUPYTER AND TENSORBOARD <----\n",
    "\n",
    "### Docker\n",
    "For those using docker, open a new terminal and create a new container (using the same image) running Tensorboard:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ docker run -it -p <<port_host>>:<<port_container>>\n",
    "            --volume=<<LOG_DIR>>:<<LOG_DIR>>\n",
    "            --name=<<container_name>> <<docker_image>> \n",
    "            tensorboard --logdir=<<LOG_DIR>> --port=<<port_container>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ docker run -it -p 8887:8887\n",
    "            --volume=/your/path/ml2018/:/ml2018\n",
    "            --name=mdc_container_tensorboard mdc-keras:cpu\n",
    "            tensorboard --logdir=/ml2018/logs --port=8887"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting Tensorboard, access it via browser on `http://localhost:<<port_container>>`.\n",
    "\n",
    "### Anaconda\n",
    "$ tensorboard --logdir=<<LOG_DIR>> --port=<<port>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting Tensorboard, access it via browser on `http://localhost:<<port>>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----------\n",
    "-----------\n",
    "\n",
    "# Fine-tuning all layers\n",
    "\n",
    "What if we fine-tune all layers of SqueezeNet?\n",
    "<img src=\"unfrozenSqueezeNet.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Compiling model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 15, 15, 64)   1792        input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv1 (Activation)         (None, 15, 15, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 7, 7, 64)     0           relu_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fire2/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     1040        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire2/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire2/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire2/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/concat (Concatenate)      (None, 7, 7, 128)    0           fire2/relu_expand1x1[0][0]       \n",
      "                                                                 fire2/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire3/squeeze1x1 (Conv2D)       (None, 7, 7, 16)     2064        fire2/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_squeeze1x1 (Activati (None, 7, 7, 16)     0           fire3/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand1x1 (Conv2D)        (None, 7, 7, 64)     1088        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand3x3 (Conv2D)        (None, 7, 7, 64)     9280        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand1x1 (Activatio (None, 7, 7, 64)     0           fire3/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand3x3 (Activatio (None, 7, 7, 64)     0           fire3/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/concat (Concatenate)      (None, 7, 7, 128)    0           fire3/relu_expand1x1[0][0]       \n",
      "                                                                 fire3/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 3, 3, 128)    0           fire3/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire4/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     4128        pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire4/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire4/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire4/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/concat (Concatenate)      (None, 3, 3, 256)    0           fire4/relu_expand1x1[0][0]       \n",
      "                                                                 fire4/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire5/squeeze1x1 (Conv2D)       (None, 3, 3, 32)     8224        fire4/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_squeeze1x1 (Activati (None, 3, 3, 32)     0           fire5/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand1x1 (Conv2D)        (None, 3, 3, 128)    4224        fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand3x3 (Conv2D)        (None, 3, 3, 128)    36992       fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand1x1 (Activatio (None, 3, 3, 128)    0           fire5/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand3x3 (Activatio (None, 3, 3, 128)    0           fire5/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/concat (Concatenate)      (None, 3, 3, 256)    0           fire5/relu_expand1x1[0][0]       \n",
      "                                                                 fire5/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 1, 1, 256)    0           fire5/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire6/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     12336       pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire6/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire6/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire6/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/concat (Concatenate)      (None, 1, 1, 384)    0           fire6/relu_expand1x1[0][0]       \n",
      "                                                                 fire6/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire7/squeeze1x1 (Conv2D)       (None, 1, 1, 48)     18480       fire6/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_squeeze1x1 (Activati (None, 1, 1, 48)     0           fire7/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand1x1 (Conv2D)        (None, 1, 1, 192)    9408        fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand3x3 (Conv2D)        (None, 1, 1, 192)    83136       fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand1x1 (Activatio (None, 1, 1, 192)    0           fire7/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand3x3 (Activatio (None, 1, 1, 192)    0           fire7/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/concat (Concatenate)      (None, 1, 1, 384)    0           fire7/relu_expand1x1[0][0]       \n",
      "                                                                 fire7/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire8/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     24640       fire7/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire8/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire8/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire8/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/concat (Concatenate)      (None, 1, 1, 512)    0           fire8/relu_expand1x1[0][0]       \n",
      "                                                                 fire8/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire9/squeeze1x1 (Conv2D)       (None, 1, 1, 64)     32832       fire8/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_squeeze1x1 (Activati (None, 1, 1, 64)     0           fire9/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand1x1 (Conv2D)        (None, 1, 1, 256)    16640       fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand3x3 (Conv2D)        (None, 1, 1, 256)    147712      fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand1x1 (Activatio (None, 1, 1, 256)    0           fire9/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand3x3 (Activatio (None, 1, 1, 256)    0           fire9/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/concat (Concatenate)      (None, 1, 1, 512)    0           fire9/relu_expand1x1[0][0]       \n",
      "                                                                 fire9/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "drop9 (Dropout)                 (None, 1, 1, 512)    0           fire9/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv10 (Conv2D)                 (None, 1, 1, 10)     5130        drop9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv10 (Activation)        (None, 1, 1, 10)     0           conv10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_30 (Gl (None, 10)           0           relu_conv10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "loss (Activation)               (None, 10)           0           global_average_pooling2d_30[0][0]\n",
      "==================================================================================================\n",
      "Total params: 727,626\n",
      "Trainable params: 727,626\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_squeezenet_ft3():\n",
    "    squeezeNetModel = SqueezeNet((32,32,3))\n",
    "\n",
    "    for layer in squeezeNetModel.layers:\n",
    "        layer.trainable = True       #by default they are all trainable, but just for clarification\n",
    "\n",
    "    #=====================================\n",
    "    # Add new classification layers\n",
    "    #=====================================\n",
    "\n",
    "    #--- Removing layers until drop 9\n",
    "    squeezeNetModel.layers.pop() #Convolution2D\n",
    "    squeezeNetModel.layers.pop() #Activation ReLU\n",
    "    squeezeNetModel.layers.pop() #Global Avg Pool\n",
    "    squeezeNetModel.layers.pop() #Activation Softmax\n",
    "\n",
    "    #--- Adding classification layer for 10 classes\n",
    "    out = Convolution2D(n_classes, (1, 1), padding='valid', name='conv10')(squeezeNetModel.layers[-1].output)\n",
    "    out = Activation('relu', name='relu_conv10')(out)\n",
    "\n",
    "    out = GlobalAveragePooling2D()(out)\n",
    "    out = Activation('softmax', name='loss')(out)\n",
    "\n",
    "    #=====================================\n",
    "    # New Model\n",
    "    #=====================================\n",
    "    model = Model(squeezeNetModel.inputs, out, name='squeezenet_new')\n",
    "    opt = optimizer=optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)#SGD(lr=learning_rate)\n",
    "    print(\"--- Compiling model\")\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_squeezenet_ft3()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing tensorboard\n",
      "Log Dir:  logs/1542642629.2368855\n",
      "--- Start training\n",
      "--- Compiling model\n",
      "================================================\n",
      "Split: 0\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 16s 392us/step - loss: 2.2826 - acc: 0.1584 - val_loss: 2.0243 - val_acc: 0.3246\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 15s 367us/step - loss: 1.8054 - acc: 0.3678 - val_loss: 1.4007 - val_acc: 0.5291\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 1.4157 - acc: 0.5390 - val_loss: 1.1710 - val_acc: 0.6234\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 14s 361us/step - loss: 1.1942 - acc: 0.6240 - val_loss: 1.0712 - val_acc: 0.6540\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 1.0527 - acc: 0.6676 - val_loss: 1.0020 - val_acc: 0.6721\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 14s 358us/step - loss: 0.9525 - acc: 0.7007 - val_loss: 0.9535 - val_acc: 0.6929\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.8725 - acc: 0.7213 - val_loss: 0.9082 - val_acc: 0.7036\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 15s 365us/step - loss: 0.8113 - acc: 0.7412 - val_loss: 0.8879 - val_acc: 0.7110\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 14s 350us/step - loss: 0.7575 - acc: 0.7553 - val_loss: 0.8574 - val_acc: 0.7233\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 14s 341us/step - loss: 0.7107 - acc: 0.7684 - val_loss: 0.8761 - val_acc: 0.7185\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 14s 341us/step - loss: 0.6693 - acc: 0.7809 - val_loss: 0.8877 - val_acc: 0.7206\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 14s 340us/step - loss: 0.6235 - acc: 0.7955 - val_loss: 0.8457 - val_acc: 0.7346\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.5883 - acc: 0.8086 - val_loss: 0.9110 - val_acc: 0.7283\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 14s 340us/step - loss: 0.5578 - acc: 0.8156 - val_loss: 0.9102 - val_acc: 0.7318\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 14s 341us/step - loss: 0.5236 - acc: 0.8270 - val_loss: 0.8758 - val_acc: 0.7426\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.4958 - acc: 0.8378 - val_loss: 0.8914 - val_acc: 0.7346\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.4624 - acc: 0.8467 - val_loss: 0.9346 - val_acc: 0.7374\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.4343 - acc: 0.8544 - val_loss: 0.9551 - val_acc: 0.7481\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 14s 361us/step - loss: 0.4115 - acc: 0.8636 - val_loss: 0.9922 - val_acc: 0.7402\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.3852 - acc: 0.8720 - val_loss: 1.0606 - val_acc: 0.7383\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.3637 - acc: 0.8794 - val_loss: 0.9857 - val_acc: 0.7443\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.3482 - acc: 0.8835 - val_loss: 1.0805 - val_acc: 0.7442\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.3279 - acc: 0.8910 - val_loss: 1.1101 - val_acc: 0.7421\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 14s 361us/step - loss: 0.3054 - acc: 0.8975 - val_loss: 1.1249 - val_acc: 0.7385\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 14s 360us/step - loss: 0.2949 - acc: 0.9018 - val_loss: 1.1642 - val_acc: 0.7404\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 14s 359us/step - loss: 0.2735 - acc: 0.9090 - val_loss: 1.2287 - val_acc: 0.7427\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.2606 - acc: 0.9132 - val_loss: 1.2378 - val_acc: 0.7370\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.2474 - acc: 0.9182 - val_loss: 1.3227 - val_acc: 0.7421\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.2403 - acc: 0.9199 - val_loss: 1.2490 - val_acc: 0.7369\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.2259 - acc: 0.9250 - val_loss: 1.2281 - val_acc: 0.7392\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.2227 - acc: 0.9277 - val_loss: 1.3771 - val_acc: 0.7352\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.2061 - acc: 0.9327 - val_loss: 1.3755 - val_acc: 0.7352\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.1908 - acc: 0.9373 - val_loss: 1.4438 - val_acc: 0.7323\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.1937 - acc: 0.9371 - val_loss: 1.4769 - val_acc: 0.7387\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1797 - acc: 0.9418 - val_loss: 1.4162 - val_acc: 0.7411\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.1751 - acc: 0.9436 - val_loss: 1.3741 - val_acc: 0.7353\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1638 - acc: 0.9468 - val_loss: 1.5546 - val_acc: 0.7373\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 15s 363us/step - loss: 0.1578 - acc: 0.9492 - val_loss: 1.4818 - val_acc: 0.7346\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 15s 373us/step - loss: 0.1570 - acc: 0.9488 - val_loss: 1.4609 - val_acc: 0.7411\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 16s 393us/step - loss: 0.1474 - acc: 0.9536 - val_loss: 1.4811 - val_acc: 0.7412\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 15s 384us/step - loss: 0.1520 - acc: 0.9507 - val_loss: 1.4596 - val_acc: 0.7412\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 15s 379us/step - loss: 0.1417 - acc: 0.9554 - val_loss: 1.4938 - val_acc: 0.7332\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 15s 380us/step - loss: 0.1351 - acc: 0.9570 - val_loss: 1.5210 - val_acc: 0.7369\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.1330 - acc: 0.9578 - val_loss: 1.6520 - val_acc: 0.7322\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.1308 - acc: 0.9585 - val_loss: 1.6766 - val_acc: 0.7328\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 14s 360us/step - loss: 0.1217 - acc: 0.9607 - val_loss: 1.6608 - val_acc: 0.7319\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 14s 360us/step - loss: 0.1225 - acc: 0.9618 - val_loss: 1.5466 - val_acc: 0.7347\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1161 - acc: 0.9642 - val_loss: 1.6969 - val_acc: 0.7355\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1219 - acc: 0.9613 - val_loss: 1.5421 - val_acc: 0.7328\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1020 - acc: 0.9672 - val_loss: 1.6831 - val_acc: 0.7324\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.1167 - acc: 0.9626 - val_loss: 1.6704 - val_acc: 0.7289\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.1069 - acc: 0.9661 - val_loss: 1.5403 - val_acc: 0.7338\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.0943 - acc: 0.9710 - val_loss: 1.6773 - val_acc: 0.7226\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1086 - acc: 0.9656 - val_loss: 1.6911 - val_acc: 0.7399\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1064 - acc: 0.9667 - val_loss: 1.6962 - val_acc: 0.7344\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 14s 351us/step - loss: 0.1008 - acc: 0.9690 - val_loss: 1.6785 - val_acc: 0.7424\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 14s 350us/step - loss: 0.0892 - acc: 0.9716 - val_loss: 1.6539 - val_acc: 0.7356\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0945 - acc: 0.9693 - val_loss: 1.7892 - val_acc: 0.7382\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 14s 350us/step - loss: 0.0886 - acc: 0.9727 - val_loss: 1.8203 - val_acc: 0.7388\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0907 - acc: 0.9714 - val_loss: 1.7188 - val_acc: 0.7402\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 14s 350us/step - loss: 0.0856 - acc: 0.9737 - val_loss: 1.7729 - val_acc: 0.7461\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0785 - acc: 0.9759 - val_loss: 1.7610 - val_acc: 0.7385\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 14s 351us/step - loss: 0.0858 - acc: 0.9733 - val_loss: 1.7213 - val_acc: 0.7416\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 14s 351us/step - loss: 0.0903 - acc: 0.9722 - val_loss: 1.7564 - val_acc: 0.7403\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 14s 350us/step - loss: 0.0847 - acc: 0.9744 - val_loss: 1.7787 - val_acc: 0.7382\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.0822 - acc: 0.9748 - val_loss: 1.7931 - val_acc: 0.7335\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 14s 359us/step - loss: 0.0777 - acc: 0.9760 - val_loss: 1.7032 - val_acc: 0.7383\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 15s 379us/step - loss: 0.0795 - acc: 0.9754 - val_loss: 1.6464 - val_acc: 0.7369\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.0809 - acc: 0.9748 - val_loss: 1.7775 - val_acc: 0.7368\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.0723 - acc: 0.9775 - val_loss: 1.7275 - val_acc: 0.7306\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 14s 358us/step - loss: 0.0779 - acc: 0.9755 - val_loss: 1.7946 - val_acc: 0.7388\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 15s 364us/step - loss: 0.0735 - acc: 0.9770 - val_loss: 1.7981 - val_acc: 0.7391\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 15s 378us/step - loss: 0.0680 - acc: 0.9792 - val_loss: 1.7991 - val_acc: 0.7357\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 15s 372us/step - loss: 0.0736 - acc: 0.9774 - val_loss: 1.8401 - val_acc: 0.7438\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.0686 - acc: 0.9786 - val_loss: 1.9093 - val_acc: 0.7411\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 15s 378us/step - loss: 0.0666 - acc: 0.9804 - val_loss: 1.8415 - val_acc: 0.7373\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 15s 366us/step - loss: 0.0679 - acc: 0.9779 - val_loss: 1.9234 - val_acc: 0.7411\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 15s 377us/step - loss: 0.0704 - acc: 0.9788 - val_loss: 1.8468 - val_acc: 0.7389\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0680 - acc: 0.9786 - val_loss: 1.7180 - val_acc: 0.7401\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.0637 - acc: 0.9804 - val_loss: 1.8805 - val_acc: 0.7341\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0671 - acc: 0.9802 - val_loss: 1.8278 - val_acc: 0.7375\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0631 - acc: 0.9803 - val_loss: 1.7727 - val_acc: 0.7363\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0653 - acc: 0.9801 - val_loss: 1.7772 - val_acc: 0.7329\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0638 - acc: 0.9809 - val_loss: 1.9311 - val_acc: 0.7403\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0596 - acc: 0.9815 - val_loss: 1.8713 - val_acc: 0.7378\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0631 - acc: 0.9807 - val_loss: 1.9635 - val_acc: 0.7407\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0644 - acc: 0.9803 - val_loss: 1.9374 - val_acc: 0.7277\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.0522 - acc: 0.9838 - val_loss: 2.0094 - val_acc: 0.7383\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 14s 341us/step - loss: 0.0658 - acc: 0.9791 - val_loss: 1.8479 - val_acc: 0.7414\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.0570 - acc: 0.9820 - val_loss: 1.9176 - val_acc: 0.7314\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0557 - acc: 0.9825 - val_loss: 2.1087 - val_acc: 0.7345\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.0649 - acc: 0.9803 - val_loss: 1.9388 - val_acc: 0.7436\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 14s 341us/step - loss: 0.0563 - acc: 0.9835 - val_loss: 2.0658 - val_acc: 0.7210\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0513 - acc: 0.9841 - val_loss: 2.1117 - val_acc: 0.7358\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0613 - acc: 0.9819 - val_loss: 1.9965 - val_acc: 0.7337\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0624 - acc: 0.9814 - val_loss: 1.8934 - val_acc: 0.7398\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0541 - acc: 0.9836 - val_loss: 1.6626 - val_acc: 0.7439\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0531 - acc: 0.9836 - val_loss: 1.9285 - val_acc: 0.7399\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0500 - acc: 0.9845 - val_loss: 1.9148 - val_acc: 0.7438\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.0584 - acc: 0.9831 - val_loss: 1.9138 - val_acc: 0.7472\n",
      "10000/10000 [==============================] - 2s 222us/step\n",
      "Validation loss: 1.913811366270146\n",
      "Validation accuracy (NORMALIZED): 0.7471999988853931\n",
      "--- Compiling model\n",
      "================================================\n",
      "Split: 1\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 15s 373us/step - loss: 2.2353 - acc: 0.1820 - val_loss: 1.7860 - val_acc: 0.3601\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 1.7086 - acc: 0.4031 - val_loss: 1.3292 - val_acc: 0.5516\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 1.3474 - acc: 0.5597 - val_loss: 1.1217 - val_acc: 0.6309\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 1.1304 - acc: 0.6367 - val_loss: 1.0401 - val_acc: 0.6535\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 1.0149 - acc: 0.6730 - val_loss: 0.9778 - val_acc: 0.6730\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.9262 - acc: 0.7020 - val_loss: 0.9358 - val_acc: 0.6858\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.8594 - acc: 0.7233 - val_loss: 0.9008 - val_acc: 0.7015\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.7994 - acc: 0.7432 - val_loss: 0.9303 - val_acc: 0.6969\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.7500 - acc: 0.7582 - val_loss: 0.9505 - val_acc: 0.7049\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.7073 - acc: 0.7701 - val_loss: 0.8956 - val_acc: 0.7076\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.6588 - acc: 0.7851 - val_loss: 0.9258 - val_acc: 0.7197\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.6152 - acc: 0.7976 - val_loss: 0.9212 - val_acc: 0.7192\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.5794 - acc: 0.8088 - val_loss: 0.9094 - val_acc: 0.7312\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.5504 - acc: 0.8180 - val_loss: 0.9613 - val_acc: 0.7307\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.5158 - acc: 0.8286 - val_loss: 0.9091 - val_acc: 0.7346\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.4845 - acc: 0.8389 - val_loss: 1.0272 - val_acc: 0.7258\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.4599 - acc: 0.8471 - val_loss: 0.9688 - val_acc: 0.7326\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 14s 348us/step - loss: 0.4283 - acc: 0.8552 - val_loss: 1.1061 - val_acc: 0.7209\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.4087 - acc: 0.8638 - val_loss: 1.0886 - val_acc: 0.7296\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.3852 - acc: 0.8705 - val_loss: 1.0506 - val_acc: 0.7363\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.3623 - acc: 0.8791 - val_loss: 1.0998 - val_acc: 0.7335\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.3376 - acc: 0.8846 - val_loss: 1.0746 - val_acc: 0.7335\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.3266 - acc: 0.8890 - val_loss: 1.0832 - val_acc: 0.7401\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.3066 - acc: 0.8951 - val_loss: 1.1506 - val_acc: 0.7322\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.2885 - acc: 0.9023 - val_loss: 1.1842 - val_acc: 0.7296\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.2772 - acc: 0.9050 - val_loss: 1.2532 - val_acc: 0.7328\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.2643 - acc: 0.9108 - val_loss: 1.2920 - val_acc: 0.7303\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.2533 - acc: 0.9144 - val_loss: 1.3707 - val_acc: 0.7311\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.2372 - acc: 0.9192 - val_loss: 1.4037 - val_acc: 0.7323\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.2354 - acc: 0.9192 - val_loss: 1.3628 - val_acc: 0.7334\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.2206 - acc: 0.9258 - val_loss: 1.4273 - val_acc: 0.7329\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.2151 - acc: 0.9275 - val_loss: 1.4921 - val_acc: 0.7289\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1996 - acc: 0.9320 - val_loss: 1.4405 - val_acc: 0.7303\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.1923 - acc: 0.9356 - val_loss: 1.4603 - val_acc: 0.7303\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1855 - acc: 0.9376 - val_loss: 1.5352 - val_acc: 0.7307\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.1852 - acc: 0.9383 - val_loss: 1.4851 - val_acc: 0.7321\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.1629 - acc: 0.9445 - val_loss: 1.7094 - val_acc: 0.7267\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.1688 - acc: 0.9448 - val_loss: 1.5791 - val_acc: 0.7304\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.1647 - acc: 0.9456 - val_loss: 1.5458 - val_acc: 0.7265\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.1582 - acc: 0.9479 - val_loss: 1.5718 - val_acc: 0.7309\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.1435 - acc: 0.9513 - val_loss: 1.7659 - val_acc: 0.7297\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1401 - acc: 0.9550 - val_loss: 1.6571 - val_acc: 0.7321\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.1445 - acc: 0.9530 - val_loss: 1.5945 - val_acc: 0.7291\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 14s 342us/step - loss: 0.1400 - acc: 0.9541 - val_loss: 1.6224 - val_acc: 0.7331\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.1280 - acc: 0.9579 - val_loss: 1.7218 - val_acc: 0.7272\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.1277 - acc: 0.9590 - val_loss: 1.7289 - val_acc: 0.7332\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1273 - acc: 0.9594 - val_loss: 1.7107 - val_acc: 0.7356\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.1219 - acc: 0.9612 - val_loss: 1.6403 - val_acc: 0.7289\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.1143 - acc: 0.9640 - val_loss: 1.8088 - val_acc: 0.7280\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.1171 - acc: 0.9620 - val_loss: 1.7957 - val_acc: 0.7367\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.1092 - acc: 0.9652 - val_loss: 1.7993 - val_acc: 0.7282\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1068 - acc: 0.9659 - val_loss: 1.6090 - val_acc: 0.7329\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1018 - acc: 0.9672 - val_loss: 1.7202 - val_acc: 0.7295\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.1071 - acc: 0.9649 - val_loss: 1.8053 - val_acc: 0.7308\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0930 - acc: 0.9707 - val_loss: 1.8035 - val_acc: 0.7271\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.1082 - acc: 0.9659 - val_loss: 1.7465 - val_acc: 0.7308\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0880 - acc: 0.9718 - val_loss: 1.8381 - val_acc: 0.7359\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0987 - acc: 0.9684 - val_loss: 1.6567 - val_acc: 0.7260\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0893 - acc: 0.9717 - val_loss: 1.8818 - val_acc: 0.7341\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0919 - acc: 0.9705 - val_loss: 1.8212 - val_acc: 0.7302\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0816 - acc: 0.9740 - val_loss: 1.8937 - val_acc: 0.7317\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0948 - acc: 0.9697 - val_loss: 1.8327 - val_acc: 0.7232\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0833 - acc: 0.9732 - val_loss: 2.0837 - val_acc: 0.7291\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0816 - acc: 0.9742 - val_loss: 1.9020 - val_acc: 0.7275\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0818 - acc: 0.9746 - val_loss: 2.0181 - val_acc: 0.7313\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0797 - acc: 0.9743 - val_loss: 1.8525 - val_acc: 0.7323\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0794 - acc: 0.9753 - val_loss: 1.8940 - val_acc: 0.7340\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0779 - acc: 0.9761 - val_loss: 1.9153 - val_acc: 0.7316\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0767 - acc: 0.9760 - val_loss: 1.9344 - val_acc: 0.7307\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0802 - acc: 0.9752 - val_loss: 2.0351 - val_acc: 0.7258\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0761 - acc: 0.9766 - val_loss: 1.8695 - val_acc: 0.7268\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 14s 347us/step - loss: 0.0780 - acc: 0.9754 - val_loss: 1.8545 - val_acc: 0.7330\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0728 - acc: 0.9772 - val_loss: 1.8055 - val_acc: 0.7363\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0677 - acc: 0.9785 - val_loss: 1.8031 - val_acc: 0.7349\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0717 - acc: 0.9774 - val_loss: 1.8534 - val_acc: 0.7382\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0683 - acc: 0.9797 - val_loss: 1.9494 - val_acc: 0.7290\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0672 - acc: 0.9792 - val_loss: 1.9389 - val_acc: 0.7311\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0714 - acc: 0.9784 - val_loss: 1.9569 - val_acc: 0.7284\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0591 - acc: 0.9811 - val_loss: 2.0136 - val_acc: 0.7381\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0681 - acc: 0.9788 - val_loss: 1.9837 - val_acc: 0.7357\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0712 - acc: 0.9783 - val_loss: 1.7958 - val_acc: 0.7278\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 0.0616 - acc: 0.9815 - val_loss: 1.8132 - val_acc: 0.7346\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0580 - acc: 0.9824 - val_loss: 1.9345 - val_acc: 0.7317\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 14s 343us/step - loss: 0.0650 - acc: 0.9799 - val_loss: 2.0562 - val_acc: 0.7348\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 14s 360us/step - loss: 0.0687 - acc: 0.9786 - val_loss: 1.9406 - val_acc: 0.7335\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 15s 373us/step - loss: 0.0570 - acc: 0.9834 - val_loss: 1.8168 - val_acc: 0.7327\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.0630 - acc: 0.9808 - val_loss: 1.8887 - val_acc: 0.7332\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 0.0569 - acc: 0.9832 - val_loss: 1.9222 - val_acc: 0.7342\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.0698 - acc: 0.9793 - val_loss: 1.9930 - val_acc: 0.7256\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 16s 388us/step - loss: 0.0582 - acc: 0.9831 - val_loss: 1.9279 - val_acc: 0.7277\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 15s 369us/step - loss: 0.0584 - acc: 0.9820 - val_loss: 2.0230 - val_acc: 0.7343\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.0594 - acc: 0.9814 - val_loss: 1.9647 - val_acc: 0.7300\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 16s 405us/step - loss: 0.0534 - acc: 0.9827 - val_loss: 2.0080 - val_acc: 0.7346\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 14s 354us/step - loss: 0.0625 - acc: 0.9812 - val_loss: 2.0414 - val_acc: 0.7333\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 14s 345us/step - loss: 0.0542 - acc: 0.9838 - val_loss: 2.0107 - val_acc: 0.7332\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 14s 346us/step - loss: 0.0572 - acc: 0.9831 - val_loss: 1.8756 - val_acc: 0.7307\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.0640 - acc: 0.9816 - val_loss: 1.9872 - val_acc: 0.7337\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 14s 348us/step - loss: 0.0550 - acc: 0.9832 - val_loss: 2.1134 - val_acc: 0.7346\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 14s 351us/step - loss: 0.0558 - acc: 0.9829 - val_loss: 1.9612 - val_acc: 0.7352\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 14s 361us/step - loss: 0.0518 - acc: 0.9839 - val_loss: 2.0201 - val_acc: 0.7296\n",
      "10000/10000 [==============================] - 3s 255us/step\n",
      "Validation loss: 2.020084795694748\n",
      "Validation accuracy (NORMALIZED): 0.729600000411272\n",
      "--- Compiling model\n",
      "================================================\n",
      "Split: 2\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 17s 427us/step - loss: 2.1298 - acc: 0.2531 - val_loss: 1.5103 - val_acc: 0.4914\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 15s 365us/step - loss: 1.5092 - acc: 0.4870 - val_loss: 1.1688 - val_acc: 0.6054\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 15s 370us/step - loss: 1.2227 - acc: 0.5954 - val_loss: 1.0762 - val_acc: 0.6336\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 16s 391us/step - loss: 1.0818 - acc: 0.6445 - val_loss: 1.0504 - val_acc: 0.6488\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 15s 383us/step - loss: 0.9712 - acc: 0.6859 - val_loss: 1.0304 - val_acc: 0.6667\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 15s 383us/step - loss: 0.8945 - acc: 0.7124 - val_loss: 0.9285 - val_acc: 0.6930\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.8229 - acc: 0.7336 - val_loss: 0.8736 - val_acc: 0.7076\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.7702 - acc: 0.7480 - val_loss: 0.8979 - val_acc: 0.7127\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 15s 386us/step - loss: 0.7232 - acc: 0.7613 - val_loss: 0.8541 - val_acc: 0.7122\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 15s 380us/step - loss: 0.6798 - acc: 0.7773 - val_loss: 0.8746 - val_acc: 0.7155\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 15s 382us/step - loss: 0.6417 - acc: 0.7884 - val_loss: 0.8396 - val_acc: 0.7314\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 16s 405us/step - loss: 0.5989 - acc: 0.8028 - val_loss: 0.8469 - val_acc: 0.7317\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 16s 392us/step - loss: 0.5640 - acc: 0.8127 - val_loss: 0.9131 - val_acc: 0.7248\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 15s 387us/step - loss: 0.5324 - acc: 0.8202 - val_loss: 0.9012 - val_acc: 0.7324\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 16s 393us/step - loss: 0.4993 - acc: 0.8310 - val_loss: 0.9376 - val_acc: 0.7294\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 16s 388us/step - loss: 0.4683 - acc: 0.8420 - val_loss: 0.9586 - val_acc: 0.7368\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 16s 405us/step - loss: 0.4456 - acc: 0.8511 - val_loss: 0.9585 - val_acc: 0.7372\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 16s 391us/step - loss: 0.4205 - acc: 0.8594 - val_loss: 0.9434 - val_acc: 0.7385\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 0.3909 - acc: 0.8677 - val_loss: 0.9803 - val_acc: 0.7424\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 15s 378us/step - loss: 0.3686 - acc: 0.8755 - val_loss: 1.0343 - val_acc: 0.7376\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 15s 379us/step - loss: 0.3537 - acc: 0.8812 - val_loss: 1.0684 - val_acc: 0.7389\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 16s 388us/step - loss: 0.3301 - acc: 0.8878 - val_loss: 1.1111 - val_acc: 0.7295\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.3122 - acc: 0.8939 - val_loss: 1.1732 - val_acc: 0.7394\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 15s 385us/step - loss: 0.2953 - acc: 0.8979 - val_loss: 1.1351 - val_acc: 0.7371\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 16s 405us/step - loss: 0.2741 - acc: 0.9070 - val_loss: 1.2005 - val_acc: 0.7364\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 16s 388us/step - loss: 0.2660 - acc: 0.9104 - val_loss: 1.2237 - val_acc: 0.7395\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 15s 369us/step - loss: 0.2531 - acc: 0.9154 - val_loss: 1.2277 - val_acc: 0.7356\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.2463 - acc: 0.9182 - val_loss: 1.2330 - val_acc: 0.7321\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 14s 354us/step - loss: 0.2270 - acc: 0.9231 - val_loss: 1.3555 - val_acc: 0.7330\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.2129 - acc: 0.9273 - val_loss: 1.3064 - val_acc: 0.7379\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 0.2100 - acc: 0.9307 - val_loss: 1.4144 - val_acc: 0.7393\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 14s 354us/step - loss: 0.2013 - acc: 0.9332 - val_loss: 1.2869 - val_acc: 0.7349\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1933 - acc: 0.9352 - val_loss: 1.4104 - val_acc: 0.7380\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1813 - acc: 0.9401 - val_loss: 1.3320 - val_acc: 0.7389\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.1759 - acc: 0.9429 - val_loss: 1.5239 - val_acc: 0.7362\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 14s 353us/step - loss: 0.1711 - acc: 0.9431 - val_loss: 1.4169 - val_acc: 0.7347\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 14s 354us/step - loss: 0.1571 - acc: 0.9477 - val_loss: 1.5713 - val_acc: 0.7345\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1582 - acc: 0.9475 - val_loss: 1.4649 - val_acc: 0.7345\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 0.1472 - acc: 0.9520 - val_loss: 1.5678 - val_acc: 0.7391\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 14s 354us/step - loss: 0.1373 - acc: 0.9548 - val_loss: 1.4904 - val_acc: 0.7393\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 14s 354us/step - loss: 0.1448 - acc: 0.9532 - val_loss: 1.6740 - val_acc: 0.7411\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 14s 360us/step - loss: 0.1337 - acc: 0.9569 - val_loss: 1.5704 - val_acc: 0.7361\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 14s 353us/step - loss: 0.1279 - acc: 0.9591 - val_loss: 1.6266 - val_acc: 0.7375\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 14s 353us/step - loss: 0.1212 - acc: 0.9608 - val_loss: 1.7180 - val_acc: 0.7363\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 14s 353us/step - loss: 0.1148 - acc: 0.9632 - val_loss: 1.6304 - val_acc: 0.7388\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 14s 355us/step - loss: 0.1197 - acc: 0.9629 - val_loss: 1.6122 - val_acc: 0.7383\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 0.1096 - acc: 0.9648 - val_loss: 1.6815 - val_acc: 0.7336\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 0.1070 - acc: 0.9656 - val_loss: 1.7238 - val_acc: 0.7307\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.1097 - acc: 0.9650 - val_loss: 1.6605 - val_acc: 0.7310\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 17s 424us/step - loss: 0.1075 - acc: 0.9658 - val_loss: 1.7545 - val_acc: 0.7389\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 15s 385us/step - loss: 0.0993 - acc: 0.9687 - val_loss: 1.7865 - val_acc: 0.7349\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 15s 380us/step - loss: 0.1026 - acc: 0.9675 - val_loss: 1.7517 - val_acc: 0.7347\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 16s 408us/step - loss: 0.0987 - acc: 0.9697 - val_loss: 1.6600 - val_acc: 0.7371\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 16s 390us/step - loss: 0.0882 - acc: 0.9728 - val_loss: 1.8906 - val_acc: 0.7321\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 16s 407us/step - loss: 0.1002 - acc: 0.9690 - val_loss: 1.6086 - val_acc: 0.7344\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0917 - acc: 0.9723 - val_loss: 1.8737 - val_acc: 0.7360\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 15s 370us/step - loss: 0.0839 - acc: 0.9743 - val_loss: 1.7649 - val_acc: 0.7306\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 15s 369us/step - loss: 0.0920 - acc: 0.9706 - val_loss: 1.7643 - val_acc: 0.7378\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 16s 396us/step - loss: 0.0875 - acc: 0.9729 - val_loss: 1.6872 - val_acc: 0.7342\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 15s 387us/step - loss: 0.0814 - acc: 0.9751 - val_loss: 1.6344 - val_acc: 0.7407\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.0787 - acc: 0.9755 - val_loss: 1.8083 - val_acc: 0.7411\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 16s 398us/step - loss: 0.0815 - acc: 0.9736 - val_loss: 1.8093 - val_acc: 0.7384\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 16s 397us/step - loss: 0.0760 - acc: 0.9768 - val_loss: 1.7145 - val_acc: 0.7379\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 15s 370us/step - loss: 0.0796 - acc: 0.9761 - val_loss: 1.6996 - val_acc: 0.7361\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 15s 367us/step - loss: 0.0756 - acc: 0.9761 - val_loss: 1.8200 - val_acc: 0.7345\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.0770 - acc: 0.9763 - val_loss: 1.6768 - val_acc: 0.7348\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 15s 372us/step - loss: 0.0734 - acc: 0.9786 - val_loss: 1.8763 - val_acc: 0.7306\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 16s 402us/step - loss: 0.0695 - acc: 0.9777 - val_loss: 1.8404 - val_acc: 0.7418\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 16s 391us/step - loss: 0.0697 - acc: 0.9786 - val_loss: 1.7948 - val_acc: 0.7396\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 17s 423us/step - loss: 0.0736 - acc: 0.9768 - val_loss: 1.8714 - val_acc: 0.7382\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 17s 424us/step - loss: 0.0696 - acc: 0.9788 - val_loss: 1.7322 - val_acc: 0.7432\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 17s 415us/step - loss: 0.0709 - acc: 0.9783 - val_loss: 1.8332 - val_acc: 0.7310\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 15s 378us/step - loss: 0.0678 - acc: 0.9790 - val_loss: 1.8271 - val_acc: 0.7376\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 16s 411us/step - loss: 0.0608 - acc: 0.9809 - val_loss: 1.9126 - val_acc: 0.7353\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 16s 411us/step - loss: 0.0688 - acc: 0.9785 - val_loss: 1.8187 - val_acc: 0.7401\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 17s 414us/step - loss: 0.0656 - acc: 0.9800 - val_loss: 1.8091 - val_acc: 0.7403\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 17s 418us/step - loss: 0.0595 - acc: 0.9817 - val_loss: 2.0053 - val_acc: 0.7333\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 17s 416us/step - loss: 0.0690 - acc: 0.9785 - val_loss: 2.0364 - val_acc: 0.7352\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 16s 388us/step - loss: 0.0597 - acc: 0.9820 - val_loss: 1.8878 - val_acc: 0.7315\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 16s 393us/step - loss: 0.0607 - acc: 0.9811 - val_loss: 1.9810 - val_acc: 0.7355\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 15s 377us/step - loss: 0.0596 - acc: 0.9825 - val_loss: 1.7631 - val_acc: 0.7314\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 16s 393us/step - loss: 0.0615 - acc: 0.9813 - val_loss: 1.8673 - val_acc: 0.7349\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 15s 372us/step - loss: 0.0602 - acc: 0.9808 - val_loss: 1.8495 - val_acc: 0.7400\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 16s 389us/step - loss: 0.0568 - acc: 0.9814 - val_loss: 1.8464 - val_acc: 0.7390\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 16s 391us/step - loss: 0.0581 - acc: 0.9818 - val_loss: 1.8610 - val_acc: 0.7416\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 15s 378us/step - loss: 0.0548 - acc: 0.9818 - val_loss: 1.9726 - val_acc: 0.7415\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 15s 385us/step - loss: 0.0579 - acc: 0.9830 - val_loss: 1.7886 - val_acc: 0.7427\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 15s 369us/step - loss: 0.0611 - acc: 0.9815 - val_loss: 1.9703 - val_acc: 0.7363\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 15s 382us/step - loss: 0.0549 - acc: 0.9835 - val_loss: 1.8731 - val_acc: 0.7441\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0601 - acc: 0.9822 - val_loss: 1.8779 - val_acc: 0.7384\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 15s 371us/step - loss: 0.0555 - acc: 0.9830 - val_loss: 1.9151 - val_acc: 0.7361\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 15s 369us/step - loss: 0.0583 - acc: 0.9825 - val_loss: 1.9996 - val_acc: 0.7409\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 15s 370us/step - loss: 0.0525 - acc: 0.9839 - val_loss: 1.9749 - val_acc: 0.7414\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 15s 367us/step - loss: 0.0537 - acc: 0.9835 - val_loss: 2.0886 - val_acc: 0.7329\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 15s 372us/step - loss: 0.0606 - acc: 0.9818 - val_loss: 1.8769 - val_acc: 0.7379\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 15s 373us/step - loss: 0.0528 - acc: 0.9832 - val_loss: 1.9022 - val_acc: 0.7355\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 16s 400us/step - loss: 0.0468 - acc: 0.9859 - val_loss: 2.0515 - val_acc: 0.7293\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 16s 402us/step - loss: 0.0535 - acc: 0.9834 - val_loss: 1.9968 - val_acc: 0.7356\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 16s 407us/step - loss: 0.0483 - acc: 0.9857 - val_loss: 1.9994 - val_acc: 0.7401\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 16s 407us/step - loss: 0.0532 - acc: 0.9837 - val_loss: 1.7289 - val_acc: 0.7413\n",
      "10000/10000 [==============================] - 3s 273us/step\n",
      "Validation loss: 1.7288700609498087\n",
      "Validation accuracy (NORMALIZED): 0.7412999994754791\n",
      "--- Compiling model\n",
      "================================================\n",
      "Split: 3\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 16s 404us/step - loss: 2.1938 - acc: 0.2234 - val_loss: 1.6820 - val_acc: 0.4462\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 15s 369us/step - loss: 1.6174 - acc: 0.4492 - val_loss: 1.2884 - val_acc: 0.5673\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 1.2821 - acc: 0.5832 - val_loss: 1.0673 - val_acc: 0.6417\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 16s 410us/step - loss: 1.1002 - acc: 0.6421 - val_loss: 1.0747 - val_acc: 0.6366\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 16s 410us/step - loss: 0.9913 - acc: 0.6798 - val_loss: 0.9617 - val_acc: 0.6800\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 16s 392us/step - loss: 0.9040 - acc: 0.7078 - val_loss: 0.9077 - val_acc: 0.6899\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 16s 400us/step - loss: 0.8302 - acc: 0.7299 - val_loss: 0.8585 - val_acc: 0.7141\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.7739 - acc: 0.7482 - val_loss: 0.8709 - val_acc: 0.7124\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.7236 - acc: 0.7637 - val_loss: 0.8604 - val_acc: 0.7201\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.6743 - acc: 0.7803 - val_loss: 0.8621 - val_acc: 0.7196\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.6372 - acc: 0.7910 - val_loss: 0.8479 - val_acc: 0.7269\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.6000 - acc: 0.8027 - val_loss: 0.8617 - val_acc: 0.7280\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 15s 370us/step - loss: 0.5609 - acc: 0.8161 - val_loss: 0.8523 - val_acc: 0.7315\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 15s 380us/step - loss: 0.5277 - acc: 0.8259 - val_loss: 0.8820 - val_acc: 0.7354\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 16s 403us/step - loss: 0.4977 - acc: 0.8350 - val_loss: 0.9079 - val_acc: 0.7308\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 15s 378us/step - loss: 0.4665 - acc: 0.8464 - val_loss: 0.8763 - val_acc: 0.7427\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 16s 388us/step - loss: 0.4410 - acc: 0.8526 - val_loss: 0.9232 - val_acc: 0.7396\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 0.4125 - acc: 0.8612 - val_loss: 0.9863 - val_acc: 0.7398\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.3936 - acc: 0.8689 - val_loss: 1.0014 - val_acc: 0.7376\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 15s 373us/step - loss: 0.3662 - acc: 0.8770 - val_loss: 0.9828 - val_acc: 0.7422\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 15s 379us/step - loss: 0.3482 - acc: 0.8841 - val_loss: 1.0423 - val_acc: 0.7403\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 17s 420us/step - loss: 0.3233 - acc: 0.8918 - val_loss: 1.0333 - val_acc: 0.7404\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 14s 362us/step - loss: 0.3043 - acc: 0.8975 - val_loss: 1.1449 - val_acc: 0.7373\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 14s 362us/step - loss: 0.2870 - acc: 0.9039 - val_loss: 1.1841 - val_acc: 0.7324\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 15s 365us/step - loss: 0.2796 - acc: 0.9069 - val_loss: 1.2321 - val_acc: 0.7358\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 15s 380us/step - loss: 0.2560 - acc: 0.9138 - val_loss: 1.1876 - val_acc: 0.7366\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 16s 398us/step - loss: 0.2531 - acc: 0.9151 - val_loss: 1.2119 - val_acc: 0.7358\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 16s 389us/step - loss: 0.2365 - acc: 0.9207 - val_loss: 1.2667 - val_acc: 0.7387\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 15s 386us/step - loss: 0.2232 - acc: 0.9269 - val_loss: 1.2530 - val_acc: 0.7397\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 15s 370us/step - loss: 0.2073 - acc: 0.9300 - val_loss: 1.3047 - val_acc: 0.7354\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.2085 - acc: 0.9325 - val_loss: 1.3542 - val_acc: 0.7374\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 15s 369us/step - loss: 0.1943 - acc: 0.9350 - val_loss: 1.3122 - val_acc: 0.7401\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 16s 412us/step - loss: 0.1791 - acc: 0.9407 - val_loss: 1.3944 - val_acc: 0.7408\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 17s 426us/step - loss: 0.1859 - acc: 0.9403 - val_loss: 1.3225 - val_acc: 0.7387\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 16s 390us/step - loss: 0.1786 - acc: 0.9417 - val_loss: 1.3825 - val_acc: 0.7388\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 15s 386us/step - loss: 0.1582 - acc: 0.9473 - val_loss: 1.4957 - val_acc: 0.7253\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 16s 390us/step - loss: 0.1642 - acc: 0.9467 - val_loss: 1.4582 - val_acc: 0.7348\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.1543 - acc: 0.9512 - val_loss: 1.4645 - val_acc: 0.7332\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.1441 - acc: 0.9540 - val_loss: 1.4872 - val_acc: 0.7372\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.1432 - acc: 0.9544 - val_loss: 1.5800 - val_acc: 0.7398\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 16s 396us/step - loss: 0.1295 - acc: 0.9583 - val_loss: 1.6024 - val_acc: 0.7377\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.1367 - acc: 0.9559 - val_loss: 1.5506 - val_acc: 0.7352\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 16s 389us/step - loss: 0.1290 - acc: 0.9598 - val_loss: 1.5684 - val_acc: 0.7425\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 15s 377us/step - loss: 0.1235 - acc: 0.9613 - val_loss: 1.6037 - val_acc: 0.7377\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 15s 385us/step - loss: 0.1123 - acc: 0.9634 - val_loss: 1.6305 - val_acc: 0.7404\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 15s 383us/step - loss: 0.1224 - acc: 0.9598 - val_loss: 1.5186 - val_acc: 0.7360\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 15s 377us/step - loss: 0.1103 - acc: 0.9657 - val_loss: 1.6122 - val_acc: 0.7334\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 15s 383us/step - loss: 0.1104 - acc: 0.9652 - val_loss: 1.5436 - val_acc: 0.7382\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 15s 385us/step - loss: 0.1027 - acc: 0.9670 - val_loss: 1.6050 - val_acc: 0.7352\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 15s 385us/step - loss: 0.1080 - acc: 0.9655 - val_loss: 1.6703 - val_acc: 0.7394\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 15s 384us/step - loss: 0.0960 - acc: 0.9691 - val_loss: 1.6243 - val_acc: 0.7347\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 16s 398us/step - loss: 0.1064 - acc: 0.9658 - val_loss: 1.6734 - val_acc: 0.7322\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 16s 392us/step - loss: 0.0994 - acc: 0.9685 - val_loss: 1.7885 - val_acc: 0.7383\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 16s 395us/step - loss: 0.0943 - acc: 0.9700 - val_loss: 1.6602 - val_acc: 0.7388\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 16s 400us/step - loss: 0.0902 - acc: 0.9716 - val_loss: 1.8337 - val_acc: 0.7338\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 16s 403us/step - loss: 0.0895 - acc: 0.9709 - val_loss: 1.7303 - val_acc: 0.7387\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 15s 372us/step - loss: 0.0860 - acc: 0.9732 - val_loss: 1.9126 - val_acc: 0.7340\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 15s 369us/step - loss: 0.0898 - acc: 0.9722 - val_loss: 1.7768 - val_acc: 0.7373\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 15s 370us/step - loss: 0.0851 - acc: 0.9736 - val_loss: 1.8705 - val_acc: 0.7346\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.0865 - acc: 0.9728 - val_loss: 1.6585 - val_acc: 0.7343\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 15s 369us/step - loss: 0.0830 - acc: 0.9739 - val_loss: 1.8218 - val_acc: 0.7389\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 15s 367us/step - loss: 0.0775 - acc: 0.9754 - val_loss: 1.9140 - val_acc: 0.7337\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.0796 - acc: 0.9754 - val_loss: 1.7323 - val_acc: 0.7425\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 15s 367us/step - loss: 0.0762 - acc: 0.9761 - val_loss: 1.8417 - val_acc: 0.7367\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 15s 372us/step - loss: 0.0679 - acc: 0.9789 - val_loss: 1.8104 - val_acc: 0.7303\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 15s 387us/step - loss: 0.0775 - acc: 0.9750 - val_loss: 1.7399 - val_acc: 0.7365\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 16s 388us/step - loss: 0.0746 - acc: 0.9761 - val_loss: 1.6638 - val_acc: 0.7354\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 15s 387us/step - loss: 0.0639 - acc: 0.9795 - val_loss: 1.8838 - val_acc: 0.7423\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 17s 413us/step - loss: 0.0705 - acc: 0.9788 - val_loss: 1.8270 - val_acc: 0.7428\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 16s 393us/step - loss: 0.0718 - acc: 0.9773 - val_loss: 1.8636 - val_acc: 0.7431\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 16s 409us/step - loss: 0.0667 - acc: 0.9793 - val_loss: 1.7902 - val_acc: 0.7491\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 16s 389us/step - loss: 0.0672 - acc: 0.9788 - val_loss: 1.8601 - val_acc: 0.7359\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 17s 423us/step - loss: 0.0691 - acc: 0.9788 - val_loss: 1.8953 - val_acc: 0.7350\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 16s 401us/step - loss: 0.0636 - acc: 0.9795 - val_loss: 1.8983 - val_acc: 0.7381\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 16s 392us/step - loss: 0.0637 - acc: 0.9796 - val_loss: 1.6649 - val_acc: 0.7412\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 15s 379us/step - loss: 0.0661 - acc: 0.9801 - val_loss: 1.6732 - val_acc: 0.7371\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 15s 372us/step - loss: 0.0688 - acc: 0.9792 - val_loss: 1.7658 - val_acc: 0.7326\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 15s 373us/step - loss: 0.0561 - acc: 0.9832 - val_loss: 1.8500 - val_acc: 0.7421\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.0582 - acc: 0.9818 - val_loss: 1.9657 - val_acc: 0.7405\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 15s 385us/step - loss: 0.0616 - acc: 0.9812 - val_loss: 1.9536 - val_acc: 0.7404\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 15s 372us/step - loss: 0.0590 - acc: 0.9812 - val_loss: 1.8855 - val_acc: 0.7409\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 15s 370us/step - loss: 0.0534 - acc: 0.9832 - val_loss: 1.8635 - val_acc: 0.7371\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 15s 372us/step - loss: 0.0608 - acc: 0.9810 - val_loss: 1.9011 - val_acc: 0.7436\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 15s 369us/step - loss: 0.0548 - acc: 0.9839 - val_loss: 1.8520 - val_acc: 0.7378\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.0570 - acc: 0.9827 - val_loss: 2.0194 - val_acc: 0.7405\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 16s 399us/step - loss: 0.0587 - acc: 0.9819 - val_loss: 1.9564 - val_acc: 0.7337\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 16s 389us/step - loss: 0.0567 - acc: 0.9818 - val_loss: 1.9907 - val_acc: 0.7404\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 16s 410us/step - loss: 0.0563 - acc: 0.9827 - val_loss: 1.9995 - val_acc: 0.7306\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 16s 391us/step - loss: 0.0540 - acc: 0.9839 - val_loss: 1.9286 - val_acc: 0.7426\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 15s 373us/step - loss: 0.0511 - acc: 0.9845 - val_loss: 2.0812 - val_acc: 0.7389\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 0.0546 - acc: 0.9836 - val_loss: 2.0538 - val_acc: 0.7381\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 15s 385us/step - loss: 0.0561 - acc: 0.9830 - val_loss: 1.8663 - val_acc: 0.7360\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 16s 391us/step - loss: 0.0515 - acc: 0.9844 - val_loss: 2.1157 - val_acc: 0.7352\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 15s 372us/step - loss: 0.0589 - acc: 0.9832 - val_loss: 1.9638 - val_acc: 0.7340\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 15s 372us/step - loss: 0.0539 - acc: 0.9842 - val_loss: 1.9595 - val_acc: 0.7350\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 15s 373us/step - loss: 0.0491 - acc: 0.9852 - val_loss: 1.9099 - val_acc: 0.7316\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 15s 387us/step - loss: 0.0485 - acc: 0.9849 - val_loss: 1.9836 - val_acc: 0.7442\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 16s 407us/step - loss: 0.0551 - acc: 0.9832 - val_loss: 1.9659 - val_acc: 0.7299\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 16s 397us/step - loss: 0.0510 - acc: 0.9842 - val_loss: 1.9808 - val_acc: 0.7448\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 18s 457us/step - loss: 0.0477 - acc: 0.9861 - val_loss: 1.8295 - val_acc: 0.7322\n",
      "10000/10000 [==============================] - 3s 276us/step\n",
      "Validation loss: 1.829459905605327\n",
      "Validation accuracy (NORMALIZED): 0.7322000008374453\n",
      "--- Compiling model\n",
      "================================================\n",
      "Split: 4\n",
      "Number of Epochs: 100\n",
      "Train shape: (40000, 32, 32, 3)\n",
      "Train batch size: 32\n",
      "Val batch size: 10\n",
      "================================================\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 18s 444us/step - loss: 2.0979 - acc: 0.2618 - val_loss: 1.5905 - val_acc: 0.4482\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 18s 448us/step - loss: 1.5586 - acc: 0.4660 - val_loss: 1.2842 - val_acc: 0.5771\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 17s 437us/step - loss: 1.2731 - acc: 0.5893 - val_loss: 1.0724 - val_acc: 0.6411\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 17s 418us/step - loss: 1.1030 - acc: 0.6539 - val_loss: 0.9992 - val_acc: 0.6759\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 17s 419us/step - loss: 0.9897 - acc: 0.6848 - val_loss: 0.9342 - val_acc: 0.6956\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 17s 430us/step - loss: 0.9042 - acc: 0.7114 - val_loss: 0.9155 - val_acc: 0.7001\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 17s 420us/step - loss: 0.8270 - acc: 0.7364 - val_loss: 0.9418 - val_acc: 0.7015\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 15s 379us/step - loss: 0.7777 - acc: 0.7491 - val_loss: 0.8716 - val_acc: 0.7201\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 0.7262 - acc: 0.7668 - val_loss: 0.8896 - val_acc: 0.7175\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 16s 402us/step - loss: 0.6823 - acc: 0.7806 - val_loss: 0.8673 - val_acc: 0.7252\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 16s 389us/step - loss: 0.6338 - acc: 0.7928 - val_loss: 0.8943 - val_acc: 0.7244\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 16s 409us/step - loss: 0.6005 - acc: 0.8036 - val_loss: 0.8759 - val_acc: 0.7392\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 16s 408us/step - loss: 0.5604 - acc: 0.8159 - val_loss: 0.9122 - val_acc: 0.7240\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 16s 410us/step - loss: 0.5297 - acc: 0.8248 - val_loss: 0.9282 - val_acc: 0.7341\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 17s 425us/step - loss: 0.4978 - acc: 0.8348 - val_loss: 0.9661 - val_acc: 0.7365\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 16s 411us/step - loss: 0.4662 - acc: 0.8467 - val_loss: 0.9370 - val_acc: 0.7377\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 15s 384us/step - loss: 0.4422 - acc: 0.8526 - val_loss: 1.0182 - val_acc: 0.7206\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.4124 - acc: 0.8633 - val_loss: 1.0285 - val_acc: 0.7361\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 15s 382us/step - loss: 0.3855 - acc: 0.8720 - val_loss: 1.0626 - val_acc: 0.7387\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.3698 - acc: 0.8778 - val_loss: 1.0548 - val_acc: 0.7235\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 16s 407us/step - loss: 0.3564 - acc: 0.8801 - val_loss: 1.1059 - val_acc: 0.7380\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 18s 459us/step - loss: 0.3231 - acc: 0.8908 - val_loss: 1.1492 - val_acc: 0.7343\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 18s 441us/step - loss: 0.3069 - acc: 0.8991 - val_loss: 1.1893 - val_acc: 0.7286\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 16s 408us/step - loss: 0.2935 - acc: 0.9019 - val_loss: 1.1998 - val_acc: 0.7257\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 17s 413us/step - loss: 0.2808 - acc: 0.9061 - val_loss: 1.2948 - val_acc: 0.7315\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 15s 382us/step - loss: 0.2692 - acc: 0.9112 - val_loss: 1.3354 - val_acc: 0.7316\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 15s 383us/step - loss: 0.2492 - acc: 0.9180 - val_loss: 1.3008 - val_acc: 0.7240\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 15s 383us/step - loss: 0.2362 - acc: 0.9224 - val_loss: 1.2559 - val_acc: 0.7344\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 16s 398us/step - loss: 0.2335 - acc: 0.9231 - val_loss: 1.2795 - val_acc: 0.7249\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 16s 394us/step - loss: 0.2226 - acc: 0.9263 - val_loss: 1.2960 - val_acc: 0.7345\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 16s 402us/step - loss: 0.2099 - acc: 0.9302 - val_loss: 1.4527 - val_acc: 0.7265\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.1978 - acc: 0.9365 - val_loss: 1.4403 - val_acc: 0.7315\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 16s 402us/step - loss: 0.1954 - acc: 0.9368 - val_loss: 1.3358 - val_acc: 0.7332\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 16s 392us/step - loss: 0.1827 - acc: 0.9402 - val_loss: 1.4264 - val_acc: 0.7290\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 17s 415us/step - loss: 0.1729 - acc: 0.9437 - val_loss: 1.5118 - val_acc: 0.7305\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 16s 411us/step - loss: 0.1717 - acc: 0.9454 - val_loss: 1.4100 - val_acc: 0.7301\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 16s 406us/step - loss: 0.1628 - acc: 0.9473 - val_loss: 1.5310 - val_acc: 0.7349\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 16s 393us/step - loss: 0.1522 - acc: 0.9503 - val_loss: 1.5589 - val_acc: 0.7272\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 16s 401us/step - loss: 0.1538 - acc: 0.9514 - val_loss: 1.5677 - val_acc: 0.7287\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 16s 408us/step - loss: 0.1454 - acc: 0.9551 - val_loss: 1.6486 - val_acc: 0.7314\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 16s 393us/step - loss: 0.1413 - acc: 0.9562 - val_loss: 1.5376 - val_acc: 0.7322\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 0.1337 - acc: 0.9570 - val_loss: 1.7039 - val_acc: 0.7267\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 15s 379us/step - loss: 0.1280 - acc: 0.9587 - val_loss: 1.6498 - val_acc: 0.7287\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 15s 379us/step - loss: 0.1271 - acc: 0.9590 - val_loss: 1.6818 - val_acc: 0.7281\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 15s 378us/step - loss: 0.1280 - acc: 0.9588 - val_loss: 1.6947 - val_acc: 0.7304\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 16s 392us/step - loss: 0.1223 - acc: 0.9619 - val_loss: 1.7036 - val_acc: 0.7284\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 16s 395us/step - loss: 0.1136 - acc: 0.9643 - val_loss: 1.6435 - val_acc: 0.7362\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 18s 441us/step - loss: 0.1152 - acc: 0.9636 - val_loss: 1.7483 - val_acc: 0.7339\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 16s 410us/step - loss: 0.1102 - acc: 0.9657 - val_loss: 1.7005 - val_acc: 0.7274\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 16s 400us/step - loss: 0.1117 - acc: 0.9652 - val_loss: 1.7034 - val_acc: 0.7347\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 16s 400us/step - loss: 0.1001 - acc: 0.9688 - val_loss: 1.7848 - val_acc: 0.7324\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 16s 408us/step - loss: 0.1017 - acc: 0.9682 - val_loss: 1.7588 - val_acc: 0.7226\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 16s 388us/step - loss: 0.1066 - acc: 0.9662 - val_loss: 1.7094 - val_acc: 0.7330\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 16s 398us/step - loss: 0.0978 - acc: 0.9682 - val_loss: 1.8188 - val_acc: 0.7341\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 17s 417us/step - loss: 0.0896 - acc: 0.9713 - val_loss: 1.9340 - val_acc: 0.7257\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 18s 443us/step - loss: 0.1000 - acc: 0.9688 - val_loss: 1.8306 - val_acc: 0.7306\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 17s 413us/step - loss: 0.0899 - acc: 0.9719 - val_loss: 1.9098 - val_acc: 0.7282\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 17s 418us/step - loss: 0.0911 - acc: 0.9714 - val_loss: 1.9259 - val_acc: 0.7279\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 16s 394us/step - loss: 0.0881 - acc: 0.9725 - val_loss: 1.7447 - val_acc: 0.7263\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 15s 382us/step - loss: 0.0830 - acc: 0.9743 - val_loss: 1.8658 - val_acc: 0.7342\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 15s 377us/step - loss: 0.0893 - acc: 0.9713 - val_loss: 1.8782 - val_acc: 0.7343\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.0788 - acc: 0.9760 - val_loss: 1.9378 - val_acc: 0.7289\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0806 - acc: 0.9746 - val_loss: 1.8408 - val_acc: 0.7304\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 0.0783 - acc: 0.9758 - val_loss: 1.8986 - val_acc: 0.7319\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0811 - acc: 0.9753 - val_loss: 1.9858 - val_acc: 0.7354\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0746 - acc: 0.9762 - val_loss: 1.7701 - val_acc: 0.7324\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.0828 - acc: 0.9749 - val_loss: 1.8875 - val_acc: 0.7304\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.0709 - acc: 0.9781 - val_loss: 1.8862 - val_acc: 0.7289\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0756 - acc: 0.9768 - val_loss: 1.7873 - val_acc: 0.7258\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 15s 373us/step - loss: 0.0715 - acc: 0.9789 - val_loss: 1.9329 - val_acc: 0.7275\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.0683 - acc: 0.9793 - val_loss: 2.0307 - val_acc: 0.7307\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0724 - acc: 0.9779 - val_loss: 1.8945 - val_acc: 0.7338\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.0684 - acc: 0.9784 - val_loss: 2.0204 - val_acc: 0.7339\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0705 - acc: 0.9780 - val_loss: 1.9195 - val_acc: 0.7295\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0720 - acc: 0.9778 - val_loss: 1.9500 - val_acc: 0.7354\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.0657 - acc: 0.9805 - val_loss: 2.0678 - val_acc: 0.7338\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0672 - acc: 0.9793 - val_loss: 1.8939 - val_acc: 0.7329\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 0.0647 - acc: 0.9807 - val_loss: 1.8868 - val_acc: 0.7313\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 0.0691 - acc: 0.9796 - val_loss: 1.9945 - val_acc: 0.7349\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 15s 377us/step - loss: 0.0633 - acc: 0.9795 - val_loss: 2.1162 - val_acc: 0.7274\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0596 - acc: 0.9809 - val_loss: 2.0057 - val_acc: 0.7272\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.0664 - acc: 0.9798 - val_loss: 2.0932 - val_acc: 0.7293\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.0656 - acc: 0.9810 - val_loss: 1.9908 - val_acc: 0.7302\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0609 - acc: 0.9810 - val_loss: 1.9623 - val_acc: 0.7329\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 15s 374us/step - loss: 0.0593 - acc: 0.9821 - val_loss: 2.0854 - val_acc: 0.7327\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 15s 373us/step - loss: 0.0584 - acc: 0.9808 - val_loss: 2.0418 - val_acc: 0.7336\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 15s 375us/step - loss: 0.0595 - acc: 0.9812 - val_loss: 2.0263 - val_acc: 0.7314\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 16s 405us/step - loss: 0.0598 - acc: 0.9815 - val_loss: 1.9152 - val_acc: 0.7323\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 16s 406us/step - loss: 0.0578 - acc: 0.9822 - val_loss: 2.0087 - val_acc: 0.7236\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 15s 379us/step - loss: 0.0597 - acc: 0.9817 - val_loss: 1.9318 - val_acc: 0.7330\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 15s 382us/step - loss: 0.0556 - acc: 0.9831 - val_loss: 1.9403 - val_acc: 0.7375\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 15s 380us/step - loss: 0.0591 - acc: 0.9822 - val_loss: 1.9810 - val_acc: 0.7288\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 15s 381us/step - loss: 0.0567 - acc: 0.9823 - val_loss: 1.8998 - val_acc: 0.7331\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 15s 382us/step - loss: 0.0557 - acc: 0.9826 - val_loss: 1.9984 - val_acc: 0.7323\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 16s 408us/step - loss: 0.0522 - acc: 0.9844 - val_loss: 2.1193 - val_acc: 0.7363\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 17s 417us/step - loss: 0.0612 - acc: 0.9821 - val_loss: 2.0847 - val_acc: 0.7335\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 17s 427us/step - loss: 0.0543 - acc: 0.9835 - val_loss: 1.8685 - val_acc: 0.7335\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 17s 415us/step - loss: 0.0526 - acc: 0.9839 - val_loss: 2.1884 - val_acc: 0.7301\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 15s 378us/step - loss: 0.0519 - acc: 0.9841 - val_loss: 2.0705 - val_acc: 0.7337\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 15s 376us/step - loss: 0.0505 - acc: 0.9849 - val_loss: 1.8897 - val_acc: 0.7339\n",
      "10000/10000 [==============================] - 3s 275us/step\n",
      "Validation loss: 1.8897208711421758\n",
      "Validation accuracy (NORMALIZED): 0.7339000012725592\n",
      "[[1.913811366270146, 0.7471999988853931], [2.020084795694748, 0.729600000411272], [1.7288700609498087, 0.7412999994754791], [1.829459905605327, 0.7322000008374453], [1.8897208711421758, 0.7339000012725592]]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "#=====================================\n",
    "# Tensorboard callback\n",
    "#=====================================\n",
    "\n",
    "print(\"--- Preparing tensorboard\")\n",
    "log_dir = \"logs/{}\".format(time())\n",
    "print(\"Log Dir: \", log_dir)\n",
    "tbCallBack = TensorBoard(log_dir=log_dir, write_graph=True)\n",
    "\n",
    "#=====================================\n",
    "# Training model and Evaluation\n",
    "#=====================================\n",
    "print(\"--- Start training\")\n",
    "y_train_categorical = to_categorical(y_train, num_classes=n_classes)\n",
    "y_val_categorical = to_categorical(y_val, num_classes=n_classes)\n",
    "scores = []\n",
    "\n",
    "for i in range(folds):\n",
    "    #--- Loading model\n",
    "    model = get_squeezenet_ft3()\n",
    "    \n",
    "    #--- Evaluating the model for split i\n",
    "    print(\"================================================\")\n",
    "    print(\"Split: \"+str(i))\n",
    "    print(\"Number of Epochs: \"+str(n_epochs))\n",
    "    print(\"Train shape:\",X_train[i].shape)\n",
    "    print(\"Train batch size: \"+str(train_batch_size))\n",
    "    print(\"Val batch size: \"+str(val_batch_size))\n",
    "    #print(\"Optimizer:\", opt)\n",
    "    print(\"================================================\")\n",
    "          \n",
    "    #--- Training without data augmentation\n",
    "    model.fit(X_train[i],y_train_categorical[i], batch_size=train_batch_size,\n",
    "              epochs=n_epochs, \n",
    "              validation_data=(X_val[i],y_val_categorical[i]),              \n",
    "              verbose=1,\n",
    "              callbacks=[tbCallBack])\n",
    "          \n",
    "    #--- Evaluating the model for split i\n",
    "    score = model.evaluate(x=X_val[i], y=y_val_categorical[i], batch_size=val_batch_size, verbose=1)\n",
    "    scores.append(score)\n",
    "    print('Validation loss:', score[0])\n",
    "    print('Validation accuracy (NORMALIZED):', score[1])\n",
    "    \n",
    "#--- Showing scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Validation loss: 1.876389399932441\n",
      "Mean Validation accuracy (NORMALIZED): 0.7368400001764297\n"
     ]
    }
   ],
   "source": [
    "#=====================================\n",
    "# Evaluate on validation\n",
    "#=====================================\n",
    "\n",
    "#--- The evaluation of the model \n",
    "np_aux = np.array(scores).mean(axis=0)\n",
    "\n",
    "print('Mean Validation loss:', np_aux[0])\n",
    "print('Mean Validation accuracy (NORMALIZED):', np_aux[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your best model on test\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "Now that we are working on more complex tasks and our trainings are starting to take more time it is usually a good idea to save the trained model from time to time. [Keras has a lot of ways of saving and loading the model](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model), but in this exercise we will use the simplest of them all: `model.save()`. It saves the architecture, the weights, the choice of loss function/optimizer/metrics and even the current state of the training, so you can resume your training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a model\n",
    "Once we have our model trained, we can load it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.181651924790132\n",
      "Test accuracy (NORMALIZED): 0.729099999576807\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "del model  # Will delete model, only to check if load_model is working\n",
    "\n",
    "# returns a compiled model identical to the previous one\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# evaluate test set again... should give us the same result\n",
    "# ...\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NORMALIZED):', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
